{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiadido95/Haze-Fog-suppression/blob/main/pix2pixHD_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "N-fvIhrKELDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1pF_YMYXAtxV",
        "outputId": "f2ec492b-5cb5-42a0-8f75-0b40048017af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dominate\n",
            "  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: dominate\n",
            "Successfully installed dominate-2.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dominate gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import gdown"
      ],
      "metadata": {
        "id": "DW2OEAxLB1ni"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qMC8BzYnB-Mq",
        "outputId": "6fca0065-a024-4ecb-c64a-66742aa8a905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"drive/MyDrive/Haze-Fog-suppression\"\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "0vFtFOdwCHWs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Apex for Automatic Mixed Precision to speed up training\n"
      ],
      "metadata": {
        "id": "8GO4-qJQlpsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key...\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "cLaTD67wlxW_",
        "outputId": "7321fd9d-5eb9-400f-bb79-a4edf163e797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Processing /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info\n",
            "  writing /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-k24uqvr5/apex-0.1.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/dist-packages (from apex==0.1) (23.1)\n",
            "Building wheels for collected packages: apex\n",
            "  Running command Building wheel for apex (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "\n",
            "  Compiling cuda extensions with\n",
            "  nvcc: NVIDIA (R) Cuda compiler driver\n",
            "  Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "  Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "  Cuda compilation tools, release 11.8, V11.8.89\n",
            "  Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "  from /usr/local/cuda/bin\n",
            "\n",
            "  running bdist_wheel\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-cpython-310\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/__init__.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/_ucc_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  copying apex/contrib/test/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/_transducer_ref.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/test_fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'apex_C' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'amp_C' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'syncbn' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-cpython-310/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/welford.cu -o build/temp.linux-x86_64-cpython-310/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/syncbn.o build/temp.linux-x86_64-cpython-310/csrc/welford.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_layer_norm_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp.cpp -o build/temp.linux-x86_64-cpython-310/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>):\n",
            "  csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     57 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:64:76: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "        |                                                              ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:65:85: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "        |                                                                       ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:67:58: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "        |                                            ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:69:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>):\n",
            "  csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    115 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    120 |   for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                   ~~^~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:121:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "        |                                                    ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:124:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/mlp.o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_dense_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/fused_dense.cpp: In function at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:30:62: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     30 |   auto out = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                    ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:33:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     33 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:64:68: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "        |                                                          ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:68:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     68 |   auto d_bias = at::empty({out_features}, input.type());\n",
            "        |                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:70:65: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     70 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:73:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     73 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:106:69: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:107:69: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:108:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    108 |   auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                        ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:111:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    111 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:149:72: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "        |                                                              ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:150:73: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "        |                                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:151:57: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    151 |   auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "        |                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:152:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    152 |   auto d_bias2 = at::empty({out_features}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:153:65: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    153 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:154:71: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                             ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:157:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    157 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc/megatron\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'generic_scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_weight_gradient_mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing requirements to apex.egg-info/requires.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-d3ttvz1h/.tmp-eld9dset/apex-0.1-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'amp_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_dense_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'syncbn.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/batch_norm.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/group_norm/group_norm.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/channel_swap.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/test/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/test_bottleneck_module.py'\n",
            "  adding 'apex/contrib/test/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/test/clip_grad/test_clip_grad.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py'\n",
            "  adding 'apex/contrib/test/fmha/__init__.py'\n",
            "  adding 'apex/contrib/test/fmha/test_fmha.py'\n",
            "  adding 'apex/contrib/test/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/test/focal_loss/test_focal_loss.py'\n",
            "  adding 'apex/contrib/test/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/group_norm/test_group_norm.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/test_index_mul_2d.py'\n",
            "  adding 'apex/contrib/test/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/layer_norm/test_fast_layer_norm.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_mha_fused_softmax.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_dist_adam.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/test/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py'\n",
            "  adding 'apex/contrib/test/transducer/__init__.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_joint.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_loss.py'\n",
            "  adding 'apex/contrib/test/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/test/xentropy/test_label_smoothing.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/_transducer_ref.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/_ucc_util.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp310-cp310-linux_x86_64.whl size=32236627 sha256=8d0f5d57c52dd93eda408da02959a0dc706e869e8b8c56c93333c8ac6452e593\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ep_u209/wheels/af/00/73/0e6dbf1a46334a3c4c0360bcdaf1085990eaf534b2bb7efefb\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD download\n"
      ],
      "metadata": {
        "id": "6eqPZHOXEAfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YoX7TkTS9MG",
        "outputId": "09120a1d-5280-4fed-f4ba-e3f727363778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pix2pixHD'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 55.68 MiB | 20.18 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n",
            "Updating files: 100% (115/115), done.\n",
            "/content/drive/MyDrive/Haze-suppression/pix2pixHD\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/pix2pixHD.git\n",
        "os.chdir(\"pix2pixHD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKuREuwyAtxd"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"./checkpoints/label2city_1024p/\")\n",
        "os.chdir(\"./checkpoints/label2city_1024p/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download'\n",
        "output = 'latest_net_G.pth'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "5VRByC-oFDWv",
        "outputId": "76edc983-ec8a-4868-c3e7-80a9d0229ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download\n",
            "To: /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/checkpoints/label2city_1024p/latest_net_G.pth\n",
            "100%|| 732M/732M [00:11<00:00, 62.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'latest_net_G.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"../..\")"
      ],
      "metadata": {
        "id": "zpELF8NXHBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD Train\n"
      ],
      "metadata": {
        "id": "CFdeWg-Sqj6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pix2pixHD/"
      ],
      "metadata": {
        "id": "RxD4lD7grQjI",
        "outputId": "b0d0fca6-5cb4-43e8-c5aa-0b89253253f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1000\"\n",
        "\n",
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "cBS7uIxAqn6W",
        "outputId": "e6a0d21b-a46d-4d77-e47a-0e91c937271b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 67 at iteration 2800\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:15<00:00, 36.3MB/s]\n",
            "create web directory ./checkpoints/nebbia/web...\n",
            "(epoch: 67, iters: 2900, time: 2.940) G_GAN: 2.305 G_GAN_Feat: 3.214 G_VGG: 2.003 D_real: 0.025 D_fake: 0.027 \n",
            "(epoch: 67, iters: 3000, time: 0.553) G_GAN: 1.932 G_GAN_Feat: 2.852 G_VGG: 2.272 D_real: 0.007 D_fake: 0.019 \n",
            "(epoch: 67, iters: 3100, time: 0.555) G_GAN: 1.966 G_GAN_Feat: 2.441 G_VGG: 1.691 D_real: 0.379 D_fake: 0.010 \n",
            "(epoch: 67, iters: 3200, time: 0.554) G_GAN: 2.196 G_GAN_Feat: 2.935 G_VGG: 1.913 D_real: 0.106 D_fake: 0.015 \n",
            "(epoch: 67, iters: 3300, time: 0.554) G_GAN: 2.050 G_GAN_Feat: 3.335 G_VGG: 1.641 D_real: 0.012 D_fake: 0.014 \n",
            "(epoch: 67, iters: 3400, time: 0.554) G_GAN: 2.262 G_GAN_Feat: 2.857 G_VGG: 1.547 D_real: 0.094 D_fake: 0.020 \n",
            "(epoch: 67, iters: 3500, time: 0.554) G_GAN: 1.987 G_GAN_Feat: 2.961 G_VGG: 1.912 D_real: 0.040 D_fake: 0.017 \n",
            "(epoch: 67, iters: 3600, time: 0.553) G_GAN: 2.133 G_GAN_Feat: 2.744 G_VGG: 1.574 D_real: 0.062 D_fake: 0.011 \n",
            "(epoch: 67, iters: 3700, time: 0.554) G_GAN: 1.769 G_GAN_Feat: 2.820 G_VGG: 2.405 D_real: 0.012 D_fake: 0.040 \n",
            "(epoch: 67, iters: 3800, time: 0.554) G_GAN: 2.032 G_GAN_Feat: 3.055 G_VGG: 1.852 D_real: 0.056 D_fake: 0.006 \n",
            "saving the latest model (epoch 67, total_steps 281000)\n",
            "(epoch: 67, iters: 3900, time: 0.555) G_GAN: 1.950 G_GAN_Feat: 2.790 G_VGG: 1.945 D_real: 0.007 D_fake: 0.012 \n",
            "(epoch: 67, iters: 4000, time: 0.553) G_GAN: 1.899 G_GAN_Feat: 2.780 G_VGG: 1.716 D_real: 0.013 D_fake: 0.040 \n",
            "(epoch: 67, iters: 4100, time: 0.554) G_GAN: 1.684 G_GAN_Feat: 2.294 G_VGG: 1.570 D_real: 0.011 D_fake: 0.123 \n",
            "(epoch: 67, iters: 4200, time: 0.554) G_GAN: 1.989 G_GAN_Feat: 2.850 G_VGG: 2.432 D_real: 0.110 D_fake: 0.022 \n",
            "End of epoch 67 / 200 \t Time Taken: 1026 sec\n",
            "(epoch: 68, iters: 100, time: 0.554) G_GAN: 1.744 G_GAN_Feat: 2.655 G_VGG: 1.462 D_real: 0.040 D_fake: 0.026 \n",
            "(epoch: 68, iters: 200, time: 0.554) G_GAN: 2.023 G_GAN_Feat: 3.630 G_VGG: 1.807 D_real: 0.015 D_fake: 0.014 \n",
            "(epoch: 68, iters: 300, time: 0.554) G_GAN: 1.624 G_GAN_Feat: 2.602 G_VGG: 2.123 D_real: 0.033 D_fake: 0.077 \n",
            "(epoch: 68, iters: 400, time: 0.553) G_GAN: 1.276 G_GAN_Feat: 2.740 G_VGG: 1.933 D_real: 0.016 D_fake: 0.321 \n",
            "(epoch: 68, iters: 500, time: 0.554) G_GAN: 2.019 G_GAN_Feat: 2.957 G_VGG: 1.970 D_real: 0.110 D_fake: 0.261 \n",
            "(epoch: 68, iters: 600, time: 0.554) G_GAN: 1.774 G_GAN_Feat: 2.558 G_VGG: 2.007 D_real: 0.045 D_fake: 0.024 \n",
            "saving the latest model (epoch 68, total_steps 282000)\n",
            "(epoch: 68, iters: 700, time: 0.556) G_GAN: 2.033 G_GAN_Feat: 2.737 G_VGG: 1.744 D_real: 0.289 D_fake: 0.008 \n",
            "(epoch: 68, iters: 800, time: 0.553) G_GAN: 1.869 G_GAN_Feat: 3.158 G_VGG: 2.298 D_real: 0.009 D_fake: 0.015 \n",
            "(epoch: 68, iters: 900, time: 0.554) G_GAN: 1.851 G_GAN_Feat: 3.483 G_VGG: 1.679 D_real: 0.010 D_fake: 0.019 \n",
            "(epoch: 68, iters: 1000, time: 0.554) G_GAN: 1.970 G_GAN_Feat: 2.638 G_VGG: 1.458 D_real: 0.073 D_fake: 0.033 \n",
            "(epoch: 68, iters: 1100, time: 0.554) G_GAN: 1.771 G_GAN_Feat: 2.604 G_VGG: 1.419 D_real: 0.030 D_fake: 0.094 \n",
            "(epoch: 68, iters: 1200, time: 0.554) G_GAN: 1.925 G_GAN_Feat: 3.169 G_VGG: 2.245 D_real: 0.010 D_fake: 0.008 \n",
            "(epoch: 68, iters: 1300, time: 0.554) G_GAN: 1.752 G_GAN_Feat: 2.456 G_VGG: 1.746 D_real: 0.017 D_fake: 0.038 \n",
            "(epoch: 68, iters: 1400, time: 0.554) G_GAN: 2.055 G_GAN_Feat: 2.607 G_VGG: 1.804 D_real: 0.013 D_fake: 0.024 \n",
            "(epoch: 68, iters: 1500, time: 0.554) G_GAN: 2.204 G_GAN_Feat: 3.021 G_VGG: 1.971 D_real: 0.065 D_fake: 0.053 \n",
            "(epoch: 68, iters: 1600, time: 0.554) G_GAN: 1.916 G_GAN_Feat: 2.611 G_VGG: 1.811 D_real: 0.061 D_fake: 0.025 \n",
            "saving the latest model (epoch 68, total_steps 283000)\n",
            "(epoch: 68, iters: 1700, time: 0.556) G_GAN: 1.751 G_GAN_Feat: 2.895 G_VGG: 2.041 D_real: 0.011 D_fake: 0.049 \n",
            "(epoch: 68, iters: 1800, time: 0.553) G_GAN: 1.823 G_GAN_Feat: 2.723 G_VGG: 1.564 D_real: 0.021 D_fake: 0.076 \n",
            "(epoch: 68, iters: 1900, time: 0.580) G_GAN: 2.064 G_GAN_Feat: 2.773 G_VGG: 1.856 D_real: 0.082 D_fake: 0.022 \n",
            "(epoch: 68, iters: 2000, time: 0.554) G_GAN: 1.796 G_GAN_Feat: 2.872 G_VGG: 1.812 D_real: 0.067 D_fake: 0.021 \n",
            "(epoch: 68, iters: 2100, time: 0.553) G_GAN: 1.365 G_GAN_Feat: 3.259 G_VGG: 1.861 D_real: 0.073 D_fake: 0.093 \n",
            "(epoch: 68, iters: 2200, time: 0.554) G_GAN: 1.981 G_GAN_Feat: 2.726 G_VGG: 2.051 D_real: 0.014 D_fake: 0.007 \n",
            "(epoch: 68, iters: 2300, time: 0.554) G_GAN: 1.880 G_GAN_Feat: 3.640 G_VGG: 2.261 D_real: 0.015 D_fake: 0.047 \n",
            "(epoch: 68, iters: 2400, time: 0.554) G_GAN: 1.949 G_GAN_Feat: 3.050 G_VGG: 1.645 D_real: 0.053 D_fake: 0.093 \n",
            "(epoch: 68, iters: 2500, time: 0.554) G_GAN: 2.038 G_GAN_Feat: 3.103 G_VGG: 1.919 D_real: 0.045 D_fake: 0.017 \n",
            "(epoch: 68, iters: 2600, time: 0.554) G_GAN: 1.924 G_GAN_Feat: 3.062 G_VGG: 1.815 D_real: 0.011 D_fake: 0.013 \n",
            "saving the latest model (epoch 68, total_steps 284000)\n",
            "(epoch: 68, iters: 2700, time: 0.555) G_GAN: 1.679 G_GAN_Feat: 2.607 G_VGG: 1.530 D_real: 0.028 D_fake: 0.042 \n",
            "(epoch: 68, iters: 2800, time: 0.554) G_GAN: 1.426 G_GAN_Feat: 2.789 G_VGG: 2.141 D_real: 0.011 D_fake: 0.268 \n",
            "(epoch: 68, iters: 2900, time: 0.553) G_GAN: 2.116 G_GAN_Feat: 2.752 G_VGG: 1.465 D_real: 0.018 D_fake: 0.024 \n",
            "(epoch: 68, iters: 3000, time: 0.553) G_GAN: 1.902 G_GAN_Feat: 2.717 G_VGG: 1.438 D_real: 0.008 D_fake: 0.020 \n",
            "(epoch: 68, iters: 3100, time: 0.553) G_GAN: 1.955 G_GAN_Feat: 3.124 G_VGG: 2.201 D_real: 0.018 D_fake: 0.006 \n",
            "(epoch: 68, iters: 3200, time: 0.554) G_GAN: 1.613 G_GAN_Feat: 2.922 G_VGG: 1.291 D_real: 0.013 D_fake: 0.057 \n",
            "(epoch: 68, iters: 3300, time: 0.554) G_GAN: 1.623 G_GAN_Feat: 3.339 G_VGG: 2.047 D_real: 0.026 D_fake: 0.116 \n",
            "(epoch: 68, iters: 3400, time: 0.554) G_GAN: 1.959 G_GAN_Feat: 2.635 G_VGG: 1.589 D_real: 0.042 D_fake: 0.011 \n",
            "(epoch: 68, iters: 3500, time: 0.554) G_GAN: 1.659 G_GAN_Feat: 3.055 G_VGG: 2.152 D_real: 0.053 D_fake: 0.099 \n",
            "(epoch: 68, iters: 3600, time: 0.553) G_GAN: 1.766 G_GAN_Feat: 2.941 G_VGG: 1.874 D_real: 0.013 D_fake: 0.076 \n",
            "saving the latest model (epoch 68, total_steps 285000)\n",
            "(epoch: 68, iters: 3700, time: 0.555) G_GAN: 1.821 G_GAN_Feat: 3.096 G_VGG: 1.739 D_real: 0.019 D_fake: 0.159 \n",
            "(epoch: 68, iters: 3800, time: 0.553) G_GAN: 1.756 G_GAN_Feat: 3.344 G_VGG: 2.266 D_real: 0.028 D_fake: 0.100 \n",
            "(epoch: 68, iters: 3900, time: 0.554) G_GAN: 2.051 G_GAN_Feat: 2.523 G_VGG: 1.511 D_real: 0.125 D_fake: 0.006 \n",
            "(epoch: 68, iters: 4000, time: 0.554) G_GAN: 1.779 G_GAN_Feat: 2.782 G_VGG: 1.896 D_real: 0.013 D_fake: 0.033 \n",
            "(epoch: 68, iters: 4100, time: 0.553) G_GAN: 1.890 G_GAN_Feat: 2.918 G_VGG: 1.656 D_real: 0.019 D_fake: 0.023 \n",
            "(epoch: 68, iters: 4200, time: 0.554) G_GAN: 1.630 G_GAN_Feat: 3.008 G_VGG: 2.113 D_real: 0.008 D_fake: 0.053 \n",
            "End of epoch 68 / 200 \t Time Taken: 2355 sec\n",
            "(epoch: 69, iters: 100, time: 0.554) G_GAN: 1.770 G_GAN_Feat: 3.189 G_VGG: 2.097 D_real: 0.052 D_fake: 0.122 \n",
            "(epoch: 69, iters: 200, time: 0.554) G_GAN: 1.528 G_GAN_Feat: 2.541 G_VGG: 1.735 D_real: 0.135 D_fake: 0.147 \n",
            "(epoch: 69, iters: 300, time: 0.554) G_GAN: 1.944 G_GAN_Feat: 2.805 G_VGG: 1.650 D_real: 0.030 D_fake: 0.012 \n",
            "(epoch: 69, iters: 400, time: 0.554) G_GAN: 1.871 G_GAN_Feat: 2.586 G_VGG: 1.858 D_real: 0.012 D_fake: 0.018 \n",
            "saving the latest model (epoch 69, total_steps 286000)\n",
            "(epoch: 69, iters: 500, time: 0.555) G_GAN: 1.525 G_GAN_Feat: 2.752 G_VGG: 1.912 D_real: 0.266 D_fake: 0.068 \n",
            "(epoch: 69, iters: 600, time: 0.553) G_GAN: 2.001 G_GAN_Feat: 3.288 G_VGG: 2.769 D_real: 0.131 D_fake: 0.006 \n",
            "(epoch: 69, iters: 700, time: 0.555) G_GAN: 1.972 G_GAN_Feat: 2.559 G_VGG: 2.019 D_real: 0.094 D_fake: 0.016 \n",
            "(epoch: 69, iters: 800, time: 0.553) G_GAN: 1.938 G_GAN_Feat: 2.901 G_VGG: 2.111 D_real: 0.017 D_fake: 0.020 \n",
            "(epoch: 69, iters: 900, time: 0.554) G_GAN: 1.855 G_GAN_Feat: 2.382 G_VGG: 1.891 D_real: 0.102 D_fake: 0.036 \n",
            "(epoch: 69, iters: 1000, time: 0.554) G_GAN: 1.758 G_GAN_Feat: 2.412 G_VGG: 1.375 D_real: 0.087 D_fake: 0.029 \n",
            "(epoch: 69, iters: 1100, time: 0.554) G_GAN: 2.295 G_GAN_Feat: 2.744 G_VGG: 1.476 D_real: 0.076 D_fake: 0.022 \n",
            "(epoch: 69, iters: 1200, time: 0.554) G_GAN: 1.836 G_GAN_Feat: 3.090 G_VGG: 1.896 D_real: 0.696 D_fake: 0.107 \n",
            "(epoch: 69, iters: 1300, time: 0.555) G_GAN: 1.588 G_GAN_Feat: 2.674 G_VGG: 1.853 D_real: 0.105 D_fake: 0.111 \n",
            "(epoch: 69, iters: 1400, time: 0.555) G_GAN: 1.582 G_GAN_Feat: 2.808 G_VGG: 1.639 D_real: 0.032 D_fake: 0.091 \n",
            "saving the latest model (epoch 69, total_steps 287000)\n",
            "(epoch: 69, iters: 1500, time: 0.555) G_GAN: 1.752 G_GAN_Feat: 2.846 G_VGG: 2.068 D_real: 0.104 D_fake: 0.227 \n",
            "(epoch: 69, iters: 1600, time: 0.553) G_GAN: 1.510 G_GAN_Feat: 2.511 G_VGG: 1.665 D_real: 0.134 D_fake: 0.203 \n",
            "(epoch: 69, iters: 1700, time: 0.553) G_GAN: 1.950 G_GAN_Feat: 2.695 G_VGG: 2.033 D_real: 0.045 D_fake: 0.007 \n",
            "(epoch: 69, iters: 1800, time: 0.554) G_GAN: 1.905 G_GAN_Feat: 2.921 G_VGG: 2.071 D_real: 0.183 D_fake: 0.012 \n",
            "(epoch: 69, iters: 1900, time: 0.554) G_GAN: 1.760 G_GAN_Feat: 2.973 G_VGG: 1.785 D_real: 0.015 D_fake: 0.053 \n",
            "(epoch: 69, iters: 2000, time: 0.555) G_GAN: 2.017 G_GAN_Feat: 3.274 G_VGG: 2.583 D_real: 0.014 D_fake: 0.006 \n",
            "(epoch: 69, iters: 2100, time: 0.554) G_GAN: 1.517 G_GAN_Feat: 2.673 G_VGG: 2.099 D_real: 0.012 D_fake: 0.391 \n",
            "(epoch: 69, iters: 2200, time: 0.554) G_GAN: 1.982 G_GAN_Feat: 2.851 G_VGG: 2.226 D_real: 0.045 D_fake: 0.018 \n",
            "(epoch: 69, iters: 2300, time: 0.554) G_GAN: 1.405 G_GAN_Feat: 3.048 G_VGG: 1.863 D_real: 0.015 D_fake: 0.205 \n",
            "(epoch: 69, iters: 2400, time: 0.554) G_GAN: 1.817 G_GAN_Feat: 3.039 G_VGG: 2.030 D_real: 0.015 D_fake: 0.020 \n",
            "saving the latest model (epoch 69, total_steps 288000)\n",
            "(epoch: 69, iters: 2500, time: 0.556) G_GAN: 1.357 G_GAN_Feat: 2.423 G_VGG: 1.439 D_real: 0.009 D_fake: 0.387 \n",
            "(epoch: 69, iters: 2600, time: 0.553) G_GAN: 1.970 G_GAN_Feat: 2.853 G_VGG: 1.789 D_real: 0.021 D_fake: 0.017 \n",
            "(epoch: 69, iters: 2700, time: 0.555) G_GAN: 1.826 G_GAN_Feat: 2.622 G_VGG: 1.601 D_real: 0.021 D_fake: 0.021 \n",
            "(epoch: 69, iters: 2800, time: 0.553) G_GAN: 1.792 G_GAN_Feat: 2.703 G_VGG: 2.256 D_real: 0.031 D_fake: 0.030 \n",
            "(epoch: 69, iters: 2900, time: 0.554) G_GAN: 2.032 G_GAN_Feat: 3.115 G_VGG: 2.063 D_real: 0.005 D_fake: 0.004 \n",
            "(epoch: 69, iters: 3000, time: 0.554) G_GAN: 2.033 G_GAN_Feat: 3.164 G_VGG: 2.251 D_real: 0.006 D_fake: 0.007 \n",
            "(epoch: 69, iters: 3100, time: 0.554) G_GAN: 1.668 G_GAN_Feat: 2.874 G_VGG: 2.473 D_real: 0.012 D_fake: 0.206 \n",
            "(epoch: 69, iters: 3200, time: 0.554) G_GAN: 1.692 G_GAN_Feat: 2.529 G_VGG: 1.755 D_real: 0.062 D_fake: 0.071 \n",
            "(epoch: 69, iters: 3300, time: 0.554) G_GAN: 1.737 G_GAN_Feat: 2.868 G_VGG: 2.152 D_real: 0.021 D_fake: 0.050 \n",
            "(epoch: 69, iters: 3400, time: 0.553) G_GAN: 1.473 G_GAN_Feat: 2.571 G_VGG: 1.781 D_real: 0.011 D_fake: 0.193 \n",
            "saving the latest model (epoch 69, total_steps 289000)\n",
            "(epoch: 69, iters: 3500, time: 0.556) G_GAN: 1.606 G_GAN_Feat: 2.538 G_VGG: 1.655 D_real: 0.013 D_fake: 0.048 \n",
            "(epoch: 69, iters: 3600, time: 0.553) G_GAN: 1.946 G_GAN_Feat: 2.972 G_VGG: 1.716 D_real: 0.028 D_fake: 0.016 \n",
            "(epoch: 69, iters: 3700, time: 0.554) G_GAN: 1.929 G_GAN_Feat: 2.833 G_VGG: 2.411 D_real: 0.009 D_fake: 0.016 \n",
            "(epoch: 69, iters: 3800, time: 0.554) G_GAN: 2.035 G_GAN_Feat: 3.050 G_VGG: 1.604 D_real: 0.029 D_fake: 0.008 \n",
            "(epoch: 69, iters: 3900, time: 0.554) G_GAN: 1.985 G_GAN_Feat: 2.918 G_VGG: 2.030 D_real: 0.063 D_fake: 0.010 \n",
            "(epoch: 69, iters: 4000, time: 0.554) G_GAN: 2.130 G_GAN_Feat: 3.256 G_VGG: 1.820 D_real: 0.054 D_fake: 0.016 \n",
            "(epoch: 69, iters: 4100, time: 0.554) G_GAN: 1.898 G_GAN_Feat: 2.776 G_VGG: 1.673 D_real: 0.034 D_fake: 0.071 \n",
            "(epoch: 69, iters: 4200, time: 0.554) G_GAN: 1.212 G_GAN_Feat: 2.453 G_VGG: 1.871 D_real: 0.021 D_fake: 0.483 \n",
            "End of epoch 69 / 200 \t Time Taken: 2353 sec\n",
            "(epoch: 70, iters: 100, time: 0.554) G_GAN: 2.110 G_GAN_Feat: 2.878 G_VGG: 1.929 D_real: 0.015 D_fake: 0.009 \n",
            "(epoch: 70, iters: 200, time: 0.554) G_GAN: 1.995 G_GAN_Feat: 2.603 G_VGG: 2.019 D_real: 0.117 D_fake: 0.014 \n",
            "saving the latest model (epoch 70, total_steps 290000)\n",
            "(epoch: 70, iters: 300, time: 0.556) G_GAN: 1.652 G_GAN_Feat: 2.851 G_VGG: 1.563 D_real: 0.029 D_fake: 0.106 \n",
            "(epoch: 70, iters: 400, time: 0.553) G_GAN: 1.886 G_GAN_Feat: 2.810 G_VGG: 1.778 D_real: 0.026 D_fake: 0.017 \n",
            "(epoch: 70, iters: 500, time: 0.554) G_GAN: 2.152 G_GAN_Feat: 2.990 G_VGG: 2.105 D_real: 0.047 D_fake: 0.016 \n",
            "(epoch: 70, iters: 600, time: 0.553) G_GAN: 1.681 G_GAN_Feat: 2.913 G_VGG: 2.286 D_real: 0.012 D_fake: 0.067 \n",
            "(epoch: 70, iters: 700, time: 0.554) G_GAN: 2.039 G_GAN_Feat: 3.080 G_VGG: 1.943 D_real: 0.012 D_fake: 0.008 \n",
            "(epoch: 70, iters: 800, time: 0.555) G_GAN: 1.692 G_GAN_Feat: 2.676 G_VGG: 1.744 D_real: 0.172 D_fake: 0.091 \n",
            "(epoch: 70, iters: 900, time: 0.554) G_GAN: 1.764 G_GAN_Feat: 2.660 G_VGG: 1.900 D_real: 0.009 D_fake: 0.045 \n",
            "(epoch: 70, iters: 1000, time: 0.553) G_GAN: 1.814 G_GAN_Feat: 2.960 G_VGG: 2.019 D_real: 0.018 D_fake: 0.012 \n",
            "(epoch: 70, iters: 1100, time: 0.554) G_GAN: 1.599 G_GAN_Feat: 2.675 G_VGG: 1.815 D_real: 0.011 D_fake: 0.069 \n",
            "(epoch: 70, iters: 1200, time: 0.554) G_GAN: 1.824 G_GAN_Feat: 2.683 G_VGG: 1.421 D_real: 0.084 D_fake: 0.053 \n",
            "saving the latest model (epoch 70, total_steps 291000)\n",
            "(epoch: 70, iters: 1300, time: 0.556) G_GAN: 1.955 G_GAN_Feat: 2.724 G_VGG: 2.139 D_real: 0.019 D_fake: 0.015 \n",
            "(epoch: 70, iters: 1400, time: 0.553) G_GAN: 2.354 G_GAN_Feat: 2.908 G_VGG: 2.281 D_real: 0.049 D_fake: 0.040 \n",
            "(epoch: 70, iters: 1500, time: 0.554) G_GAN: 2.013 G_GAN_Feat: 2.663 G_VGG: 1.329 D_real: 0.402 D_fake: 0.011 \n",
            "(epoch: 70, iters: 1600, time: 0.553) G_GAN: 1.877 G_GAN_Feat: 2.809 G_VGG: 2.040 D_real: 0.036 D_fake: 0.023 \n",
            "(epoch: 70, iters: 1700, time: 0.553) G_GAN: 1.588 G_GAN_Feat: 2.798 G_VGG: 1.772 D_real: 0.031 D_fake: 0.083 \n",
            "(epoch: 70, iters: 1800, time: 0.554) G_GAN: 2.275 G_GAN_Feat: 2.840 G_VGG: 1.961 D_real: 0.053 D_fake: 0.046 \n",
            "(epoch: 70, iters: 1900, time: 0.555) G_GAN: 1.893 G_GAN_Feat: 2.628 G_VGG: 1.372 D_real: 0.048 D_fake: 0.023 \n",
            "(epoch: 70, iters: 2000, time: 0.555) G_GAN: 1.968 G_GAN_Feat: 2.892 G_VGG: 1.778 D_real: 0.081 D_fake: 0.005 \n",
            "(epoch: 70, iters: 2100, time: 0.555) G_GAN: 2.119 G_GAN_Feat: 2.932 G_VGG: 2.104 D_real: 0.013 D_fake: 0.009 \n",
            "(epoch: 70, iters: 2200, time: 0.554) G_GAN: 2.051 G_GAN_Feat: 3.077 G_VGG: 2.251 D_real: 0.015 D_fake: 0.012 \n",
            "saving the latest model (epoch 70, total_steps 292000)\n",
            "(epoch: 70, iters: 2300, time: 0.556) G_GAN: 1.789 G_GAN_Feat: 3.388 G_VGG: 2.963 D_real: 0.011 D_fake: 0.021 \n",
            "(epoch: 70, iters: 2400, time: 0.553) G_GAN: 2.403 G_GAN_Feat: 3.111 G_VGG: 1.720 D_real: 0.030 D_fake: 0.024 \n",
            "(epoch: 70, iters: 2500, time: 0.555) G_GAN: 1.827 G_GAN_Feat: 3.104 G_VGG: 1.840 D_real: 0.120 D_fake: 0.050 \n",
            "(epoch: 70, iters: 2600, time: 0.554) G_GAN: 1.449 G_GAN_Feat: 3.424 G_VGG: 2.148 D_real: 0.036 D_fake: 0.128 \n",
            "(epoch: 70, iters: 2700, time: 0.553) G_GAN: 1.880 G_GAN_Feat: 2.999 G_VGG: 2.218 D_real: 0.009 D_fake: 0.012 \n",
            "(epoch: 70, iters: 2800, time: 0.553) G_GAN: 1.997 G_GAN_Feat: 2.537 G_VGG: 1.664 D_real: 0.342 D_fake: 0.104 \n",
            "(epoch: 70, iters: 2900, time: 0.554) G_GAN: 1.841 G_GAN_Feat: 2.888 G_VGG: 1.823 D_real: 0.016 D_fake: 0.044 \n",
            "(epoch: 70, iters: 3000, time: 0.554) G_GAN: 1.920 G_GAN_Feat: 2.844 G_VGG: 1.875 D_real: 0.018 D_fake: 0.060 \n",
            "(epoch: 70, iters: 3100, time: 0.554) G_GAN: 1.958 G_GAN_Feat: 2.990 G_VGG: 2.164 D_real: 0.021 D_fake: 0.010 \n",
            "(epoch: 70, iters: 3200, time: 0.554) G_GAN: 1.813 G_GAN_Feat: 2.926 G_VGG: 1.891 D_real: 0.012 D_fake: 0.034 \n",
            "saving the latest model (epoch 70, total_steps 293000)\n",
            "(epoch: 70, iters: 3300, time: 0.556) G_GAN: 1.295 G_GAN_Feat: 2.872 G_VGG: 2.222 D_real: 0.201 D_fake: 0.227 \n",
            "(epoch: 70, iters: 3400, time: 0.553) G_GAN: 1.770 G_GAN_Feat: 2.937 G_VGG: 2.007 D_real: 0.076 D_fake: 0.241 \n",
            "(epoch: 70, iters: 3500, time: 0.555) G_GAN: 2.271 G_GAN_Feat: 2.795 G_VGG: 1.989 D_real: 0.020 D_fake: 0.015 \n",
            "(epoch: 70, iters: 3600, time: 0.553) G_GAN: 2.020 G_GAN_Feat: 2.947 G_VGG: 2.152 D_real: 0.140 D_fake: 0.008 \n",
            "(epoch: 70, iters: 3700, time: 0.554) G_GAN: 2.130 G_GAN_Feat: 2.923 G_VGG: 2.011 D_real: 0.086 D_fake: 0.010 \n",
            "(epoch: 70, iters: 3800, time: 0.554) G_GAN: 1.943 G_GAN_Feat: 3.019 G_VGG: 1.511 D_real: 0.015 D_fake: 0.015 \n",
            "(epoch: 70, iters: 3900, time: 0.554) G_GAN: 1.978 G_GAN_Feat: 2.991 G_VGG: 2.312 D_real: 0.022 D_fake: 0.008 \n",
            "(epoch: 70, iters: 4000, time: 0.554) G_GAN: 1.602 G_GAN_Feat: 2.454 G_VGG: 1.525 D_real: 0.124 D_fake: 0.090 \n",
            "(epoch: 70, iters: 4100, time: 0.553) G_GAN: 1.841 G_GAN_Feat: 3.005 G_VGG: 2.179 D_real: 0.101 D_fake: 0.123 \n",
            "(epoch: 70, iters: 4200, time: 0.553) G_GAN: 1.847 G_GAN_Feat: 2.823 G_VGG: 2.002 D_real: 0.010 D_fake: 0.087 \n",
            "saving the latest model (epoch 70, total_steps 294000)\n",
            "End of epoch 70 / 200 \t Time Taken: 2357 sec\n",
            "saving the model at the end of epoch 70, iters 294000\n",
            "(epoch: 71, iters: 100, time: 0.560) G_GAN: 1.960 G_GAN_Feat: 3.136 G_VGG: 2.952 D_real: 0.040 D_fake: 0.044 \n",
            "(epoch: 71, iters: 200, time: 0.551) G_GAN: 2.297 G_GAN_Feat: 2.978 G_VGG: 2.088 D_real: 0.054 D_fake: 0.026 \n",
            "(epoch: 71, iters: 300, time: 0.554) G_GAN: 1.962 G_GAN_Feat: 3.050 G_VGG: 2.197 D_real: 0.009 D_fake: 0.014 \n",
            "(epoch: 71, iters: 400, time: 0.553) G_GAN: 1.706 G_GAN_Feat: 2.510 G_VGG: 1.577 D_real: 0.033 D_fake: 0.271 \n",
            "(epoch: 71, iters: 500, time: 0.554) G_GAN: 1.472 G_GAN_Feat: 2.937 G_VGG: 2.087 D_real: 0.026 D_fake: 0.305 \n",
            "(epoch: 71, iters: 600, time: 0.555) G_GAN: 1.986 G_GAN_Feat: 2.770 G_VGG: 2.010 D_real: 0.072 D_fake: 0.038 \n",
            "(epoch: 71, iters: 700, time: 0.554) G_GAN: 1.702 G_GAN_Feat: 2.612 G_VGG: 1.913 D_real: 0.062 D_fake: 0.120 \n",
            "(epoch: 71, iters: 800, time: 0.554) G_GAN: 1.878 G_GAN_Feat: 3.066 G_VGG: 1.828 D_real: 0.018 D_fake: 0.011 \n",
            "(epoch: 71, iters: 900, time: 0.554) G_GAN: 1.954 G_GAN_Feat: 2.569 G_VGG: 2.300 D_real: 0.084 D_fake: 0.023 \n",
            "(epoch: 71, iters: 1000, time: 0.554) G_GAN: 1.724 G_GAN_Feat: 2.965 G_VGG: 2.229 D_real: 0.026 D_fake: 0.176 \n",
            "saving the latest model (epoch 71, total_steps 295000)\n",
            "(epoch: 71, iters: 1100, time: 0.556) G_GAN: 1.638 G_GAN_Feat: 2.586 G_VGG: 1.481 D_real: 0.095 D_fake: 0.070 \n",
            "(epoch: 71, iters: 1200, time: 0.553) G_GAN: 1.781 G_GAN_Feat: 2.496 G_VGG: 1.330 D_real: 0.096 D_fake: 0.184 \n",
            "(epoch: 71, iters: 1300, time: 0.555) G_GAN: 1.453 G_GAN_Feat: 2.753 G_VGG: 1.511 D_real: 0.030 D_fake: 0.081 \n",
            "(epoch: 71, iters: 1400, time: 0.554) G_GAN: 1.531 G_GAN_Feat: 2.898 G_VGG: 1.859 D_real: 0.159 D_fake: 0.138 \n",
            "(epoch: 71, iters: 1500, time: 0.554) G_GAN: 1.840 G_GAN_Feat: 3.018 G_VGG: 1.737 D_real: 0.277 D_fake: 0.091 \n",
            "(epoch: 71, iters: 1600, time: 0.554) G_GAN: 2.032 G_GAN_Feat: 2.756 G_VGG: 1.692 D_real: 0.018 D_fake: 0.011 \n",
            "(epoch: 71, iters: 1700, time: 0.554) G_GAN: 1.811 G_GAN_Feat: 2.773 G_VGG: 2.087 D_real: 0.014 D_fake: 0.018 \n",
            "(epoch: 71, iters: 1800, time: 0.554) G_GAN: 1.708 G_GAN_Feat: 3.028 G_VGG: 2.179 D_real: 0.056 D_fake: 0.028 \n",
            "(epoch: 71, iters: 1900, time: 0.553) G_GAN: 1.891 G_GAN_Feat: 2.863 G_VGG: 1.823 D_real: 0.009 D_fake: 0.019 \n",
            "(epoch: 71, iters: 2000, time: 0.554) G_GAN: 2.107 G_GAN_Feat: 3.301 G_VGG: 1.841 D_real: 0.047 D_fake: 0.009 \n",
            "saving the latest model (epoch 71, total_steps 296000)\n",
            "(epoch: 71, iters: 2100, time: 0.556) G_GAN: 1.792 G_GAN_Feat: 2.582 G_VGG: 1.692 D_real: 0.021 D_fake: 0.043 \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/train.py\", line 70, in <module>\n",
            "    losses, generated = model(Variable(data['label']), Variable(data['inst']), \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n",
            "    return self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/models/pix2pixHD_model.py\", line 163, in forward\n",
            "    fake_image = self.netG.forward(input_concat)\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/models/networks.py\", line 211, in forward\n",
            "    return self.model(input)             \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/models/networks.py\", line 252, in forward\n",
            "    out = x + self.conv_block(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on high resolution G1 and G2\n",
        "#!python train.py --netG local --load_pretrain checkpoints/nebbia/ --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 1024"
      ],
      "metadata": {
        "id": "3XhQJ5s95ZUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2Pix test"
      ],
      "metadata": {
        "id": "qEuQqQPxGISq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m4nxHHLAtxf",
        "outputId": "d8bb40ca-fe7a-4438-f504-f730e3213a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter.txt  latest_net_D.pth  latest_net_G.pth  loss_log.txt  opt.txt  web\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/test.py\", line 12, in <module>\n",
            "    opt = TestOptions().parse(save=False)\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/options/base_options.py\", line 80, in parse\n",
            "    torch.cuda.set_device(self.opt.gpu_ids[0])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 350, in set_device\n",
            "    torch._C._cuda_setDevice(device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        }
      ],
      "source": [
        "#se non  stato eseguito il download\n",
        "if os.getcwd() != \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD\":\n",
        "  os.chdir(\"pix2pixHD\")\n",
        "\n",
        "#!./scripts/test_1024p.sh\n",
        "!python test.py --name nebbia --netG local --ngf 32 --resize_or_crop none --ntest 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/Haze-Fog-suppression/dataset/SOTS/outdoor/gt/* /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/nebbia/test_img/"
      ],
      "metadata": {
        "id": "o-Uvp5SC9u4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/Haze-Fog-suppression/dataset/SOTS/outdoor/hazy/* /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/nebbia/test_label/"
      ],
      "metadata": {
        "id": "ylzeExck-S7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}