{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiadido95/Haze-Fog-suppression/blob/main/pix2pixHD_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "N-fvIhrKELDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1pF_YMYXAtxV",
        "outputId": "0c609135-80d9-422e-ff7a-0e618b43ce57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dominate\n",
            "  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: dominate\n",
            "Successfully installed dominate-2.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dominate gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import gdown"
      ],
      "metadata": {
        "id": "DW2OEAxLB1ni"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qMC8BzYnB-Mq",
        "outputId": "c5eecc5b-5a3e-4bad-e8d3-3ce4ad6141a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"drive/MyDrive/Haze-Fog-suppression\"\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "0vFtFOdwCHWs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download library for no reference metrics"
      ],
      "metadata": {
        "id": "f2F05uey2amY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/buyizhiyou/NRVQA.git"
      ],
      "metadata": {
        "id": "xgW4xriY17oA",
        "outputId": "424ff3eb-4e2f-4ac7-cc6d-238ae2cfa21e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NRVQA'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 166 (delta 43), reused 131 (delta 22), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (166/166), 9.97 MiB | 11.47 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Apex for Automatic Mixed Precision to speed up training (NOT WORKING)"
      ],
      "metadata": {
        "id": "8GO4-qJQlpsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key...\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "cLaTD67wlxW_",
        "outputId": "7321fd9d-5eb9-400f-bb79-a4edf163e797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Processing /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info\n",
            "  writing /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-k24uqvr5/apex-0.1.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/dist-packages (from apex==0.1) (23.1)\n",
            "Building wheels for collected packages: apex\n",
            "  Running command Building wheel for apex (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "\n",
            "  Compiling cuda extensions with\n",
            "  nvcc: NVIDIA (R) Cuda compiler driver\n",
            "  Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "  Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "  Cuda compilation tools, release 11.8, V11.8.89\n",
            "  Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "  from /usr/local/cuda/bin\n",
            "\n",
            "  running bdist_wheel\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-cpython-310\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/__init__.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/_ucc_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  copying apex/contrib/test/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/_transducer_ref.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/test_fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'apex_C' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'amp_C' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'syncbn' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-cpython-310/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/welford.cu -o build/temp.linux-x86_64-cpython-310/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/syncbn.o build/temp.linux-x86_64-cpython-310/csrc/welford.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_layer_norm_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp.cpp -o build/temp.linux-x86_64-cpython-310/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>):\n",
            "  csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     57 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:64:76: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "        |                                                              ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:65:85: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "        |                                                                       ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:67:58: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "        |                                            ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:69:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>):\n",
            "  csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    115 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    120 |   for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                   ~~^~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:121:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "        |                                                    ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:124:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/mlp.o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_dense_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/fused_dense.cpp: In function at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:30:62: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     30 |   auto out = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                    ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:33:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     33 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:64:68: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "        |                                                          ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:68:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     68 |   auto d_bias = at::empty({out_features}, input.type());\n",
            "        |                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:70:65: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     70 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:73:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     73 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:106:69: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:107:69: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:108:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    108 |   auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                        ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:111:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    111 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:149:72: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "        |                                                              ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:150:73: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "        |                                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:151:57: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    151 |   auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "        |                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:152:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    152 |   auto d_bias2 = at::empty({out_features}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:153:65: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    153 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:154:71: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                             ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:157:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    157 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc/megatron\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'generic_scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_weight_gradient_mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing requirements to apex.egg-info/requires.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-d3ttvz1h/.tmp-eld9dset/apex-0.1-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'amp_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_dense_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'syncbn.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/batch_norm.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/group_norm/group_norm.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/channel_swap.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/test/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/test_bottleneck_module.py'\n",
            "  adding 'apex/contrib/test/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/test/clip_grad/test_clip_grad.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py'\n",
            "  adding 'apex/contrib/test/fmha/__init__.py'\n",
            "  adding 'apex/contrib/test/fmha/test_fmha.py'\n",
            "  adding 'apex/contrib/test/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/test/focal_loss/test_focal_loss.py'\n",
            "  adding 'apex/contrib/test/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/group_norm/test_group_norm.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/test_index_mul_2d.py'\n",
            "  adding 'apex/contrib/test/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/layer_norm/test_fast_layer_norm.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_mha_fused_softmax.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_dist_adam.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/test/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py'\n",
            "  adding 'apex/contrib/test/transducer/__init__.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_joint.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_loss.py'\n",
            "  adding 'apex/contrib/test/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/test/xentropy/test_label_smoothing.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/_transducer_ref.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/_ucc_util.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp310-cp310-linux_x86_64.whl size=32236627 sha256=8d0f5d57c52dd93eda408da02959a0dc706e869e8b8c56c93333c8ac6452e593\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ep_u209/wheels/af/00/73/0e6dbf1a46334a3c4c0360bcdaf1085990eaf534b2bb7efefb\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility"
      ],
      "metadata": {
        "id": "CcWqZ45pecYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "\n",
        "# salva in pix2pixHD/results le metriche\n",
        "def calculate_metrics(synth_folder, gt_folder,type):\n",
        "    # Ottieni la lista dei nomi di file dalle cartelle\n",
        "\n",
        "    image_files = os.listdir(synth_folder)\n",
        "\n",
        "    # Filtra i nomi dei file per rimuovere quelli che contengono \"input_label\"\n",
        "    filtered_image_files = [file for file in image_files if \"input_label\" not in file]\n",
        "\n",
        "    # Inizializza liste per memorizzare i risultati\n",
        "    mse_scores = []\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "\n",
        "    piqe_scores = []\n",
        "    niqe_scores = []\n",
        "\n",
        "    # Itera attraverso le immagini e calcola le metriche\n",
        "    for image_file in filtered_image_files:\n",
        "        # Carica le immagini\n",
        "        synth_image = cv2.imread(os.path.join(synth_folder, image_file))\n",
        "        gt_image = cv2.imread(os.path.join(gt_folder, image_file[:4]+\".png\"))\n",
        "\n",
        "        # Ridimensiona l'immagine sintetizzata alla stessa dimensione dell'immagine originale\n",
        "        synth_image = cv2.resize(synth_image, (gt_image.shape[1], gt_image.shape[0]))\n",
        "\n",
        "        # Calcola le metriche con skimage\n",
        "        mse = np.mean((gt_image - synth_image) ** 2)\n",
        "        ssim_score = ssim(gt_image, synth_image, channel_axis=2)\n",
        "        psnr_score = psnr(gt_image, synth_image)\n",
        "\n",
        "        # Calcola le metriche con piqe e niqe (high score means low quality)\n",
        "        synth_array = synth_image / 255.0\n",
        "\n",
        "        # Esegui il comando e cattura l'output\n",
        "        completed_process = subprocess.run(['python', '/content/drive/MyDrive/Haze-Fog-suppression/NRVQA/test.py', '--mode', 'piqe', f'--path={os.path.join(synth_folder, image_file)}'], stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "        # Ottieni l'output dall'oggetto CompletedProcess\n",
        "        output = completed_process.stdout\n",
        "\n",
        "        # Estrai lo score dalla stringa di output\n",
        "        score_line = [line for line in output.splitlines() if 'score:' in line][0]\n",
        "        piqe_score = float(score_line.split(':')[-1])\n",
        "\n",
        "        # Esegui il comando e cattura l'output\n",
        "        completed_process = subprocess.run(['python', '/content/drive/MyDrive/Haze-Fog-suppression/NRVQA/test.py', '--mode', 'niqe', f'--path={os.path.join(synth_folder, image_file)}'], stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "        # Ottieni l'output dall'oggetto CompletedProcess\n",
        "        output = completed_process.stdout\n",
        "\n",
        "        # Estrai lo score dalla stringa di output\n",
        "        score_line = [line for line in output.splitlines() if 'score:' in line][0]\n",
        "        niqe_score = float(score_line.split(':')[-1])\n",
        "\n",
        "        # Aggiungi i risultati alle liste\n",
        "        mse_scores.append(mse)\n",
        "        ssim_scores.append(ssim_score)\n",
        "        psnr_scores.append(psnr_score)\n",
        "        piqe_scores.append(piqe_score)\n",
        "        niqe_scores.append(niqe_score)\n",
        "\n",
        "\n",
        "    # Creazione del dataframe\n",
        "    data = {\n",
        "        'Image': filtered_image_files,  # Nomi dei file delle immagini generate\n",
        "        'MSE': mse_scores,\n",
        "        'SSIM': ssim_scores,\n",
        "        'PSNR': psnr_scores,\n",
        "        'PIQE': piqe_scores,\n",
        "        'NIQE': niqe_scores\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Calcolo dei valori medi\n",
        "    average_scores = df[['MSE', 'SSIM', 'PSNR', 'PIQE', 'NIQE']].mean()\n",
        "\n",
        "    # Aggiunta della riga con i valori medi al dataframe\n",
        "    average_scores_df = pd.DataFrame([average_scores], columns=['MSE', 'SSIM', 'PSNR', 'PIQE', 'NIQE'])\n",
        "    average_scores_df['Image'] = 'AVG'\n",
        "    # Aggiungi una riga vuota tra i risultati delle immagini e i valori medi\n",
        "    df = pd.concat([df, pd.DataFrame(columns=df.columns)], ignore_index=True)\n",
        "    df = pd.concat([df, average_scores_df], ignore_index=True)\n",
        "\n",
        "    # Salva il dataframe completo in un file CSV\n",
        "    df.to_csv(type + '_result.csv', index=False)\n",
        "\n",
        "def test_indoor_outdoor(indoor_synth_folder, indoor_original_folder, outdoor_synth_folder, outdoor_original_folder, type):\n",
        "  calculate_metrics(indoor_synth_folder, indoor_original_folder,type + \"_indoor\")\n",
        "  calculate_metrics(outdoor_synth_folder, outdoor_original_folder,type + \"_outdoor\")"
      ],
      "metadata": {
        "id": "5OG31gwjegsb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD download\n"
      ],
      "metadata": {
        "id": "6eqPZHOXEAfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YoX7TkTS9MG",
        "outputId": "09120a1d-5280-4fed-f4ba-e3f727363778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pix2pixHD'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 55.68 MiB | 20.18 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n",
            "Updating files: 100% (115/115), done.\n",
            "/content/drive/MyDrive/Haze-suppression/pix2pixHD\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/pix2pixHD.git\n",
        "os.chdir(\"pix2pixHD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKuREuwyAtxd"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"./checkpoints/label2city_1024p/\")\n",
        "os.chdir(\"./checkpoints/label2city_1024p/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download'\n",
        "output = 'latest_net_G.pth'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "5VRByC-oFDWv",
        "outputId": "76edc983-ec8a-4868-c3e7-80a9d0229ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download\n",
            "To: /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/checkpoints/label2city_1024p/latest_net_G.pth\n",
            "100%|| 732M/732M [00:11<00:00, 62.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'latest_net_G.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"../..\")"
      ],
      "metadata": {
        "id": "zpELF8NXHBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD Train\n"
      ],
      "metadata": {
        "id": "CFdeWg-Sqj6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pix2pixHD/"
      ],
      "metadata": {
        "id": "RxD4lD7grQjI",
        "outputId": "d19827ed-a782-4de0-c21d-187df1c9590b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unique model for different level of fog"
      ],
      "metadata": {
        "id": "p9pGBD_A7jkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution(512) only G1 (global net)\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "cBS7uIxAqn6W",
        "outputId": "98797bcc-129e-4794-b804-86e71a2df6eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 97 at iteration 1800\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:07<00:00, 81.8MB/s]\n",
            "create web directory ./checkpoints/nebbia/web...\n",
            "(epoch: 97, iters: 1900, time: 1.948) G_GAN: 2.089 G_GAN_Feat: 2.718 G_VGG: 1.824 D_real: 0.113 D_fake: 0.053 \n",
            "(epoch: 97, iters: 2000, time: 0.559) G_GAN: 1.776 G_GAN_Feat: 2.576 G_VGG: 1.368 D_real: 0.017 D_fake: 0.116 \n",
            "(epoch: 97, iters: 2100, time: 0.551) G_GAN: 1.953 G_GAN_Feat: 2.653 G_VGG: 1.674 D_real: 0.014 D_fake: 0.010 \n",
            "(epoch: 97, iters: 2200, time: 0.553) G_GAN: 1.932 G_GAN_Feat: 2.998 G_VGG: 2.237 D_real: 0.007 D_fake: 0.007 \n",
            "(epoch: 97, iters: 2300, time: 0.554) G_GAN: 1.616 G_GAN_Feat: 2.476 G_VGG: 1.229 D_real: 0.017 D_fake: 0.143 \n",
            "(epoch: 97, iters: 2400, time: 0.552) G_GAN: 1.637 G_GAN_Feat: 2.797 G_VGG: 1.949 D_real: 0.007 D_fake: 0.139 \n",
            "(epoch: 97, iters: 2500, time: 0.554) G_GAN: 1.909 G_GAN_Feat: 2.727 G_VGG: 1.499 D_real: 0.069 D_fake: 0.079 \n",
            "(epoch: 97, iters: 2600, time: 0.552) G_GAN: 2.069 G_GAN_Feat: 2.842 G_VGG: 1.586 D_real: 0.011 D_fake: 0.013 \n",
            "(epoch: 97, iters: 2700, time: 0.552) G_GAN: 1.790 G_GAN_Feat: 2.722 G_VGG: 1.862 D_real: 0.006 D_fake: 0.036 \n",
            "(epoch: 97, iters: 2800, time: 0.553) G_GAN: 1.904 G_GAN_Feat: 2.943 G_VGG: 1.584 D_real: 0.011 D_fake: 0.012 \n",
            "saving the latest model (epoch 97, total_steps 406000)\n",
            "(epoch: 97, iters: 2900, time: 0.556) G_GAN: 2.038 G_GAN_Feat: 2.717 G_VGG: 1.657 D_real: 0.266 D_fake: 0.025 \n",
            "(epoch: 97, iters: 3000, time: 0.553) G_GAN: 1.662 G_GAN_Feat: 2.671 G_VGG: 1.727 D_real: 0.015 D_fake: 0.081 \n",
            "(epoch: 97, iters: 3100, time: 0.553) G_GAN: 1.356 G_GAN_Feat: 2.756 G_VGG: 1.185 D_real: 0.059 D_fake: 0.270 \n",
            "(epoch: 97, iters: 3200, time: 0.553) G_GAN: 1.977 G_GAN_Feat: 3.140 G_VGG: 1.712 D_real: 0.016 D_fake: 0.009 \n",
            "(epoch: 97, iters: 3300, time: 0.554) G_GAN: 1.853 G_GAN_Feat: 2.989 G_VGG: 1.877 D_real: 0.029 D_fake: 0.078 \n",
            "(epoch: 97, iters: 3400, time: 0.554) G_GAN: 1.914 G_GAN_Feat: 2.883 G_VGG: 1.630 D_real: 0.063 D_fake: 0.017 \n",
            "(epoch: 97, iters: 3500, time: 0.554) G_GAN: 1.857 G_GAN_Feat: 2.601 G_VGG: 1.882 D_real: 0.016 D_fake: 0.048 \n",
            "(epoch: 97, iters: 3600, time: 0.553) G_GAN: 1.812 G_GAN_Feat: 2.531 G_VGG: 1.410 D_real: 0.020 D_fake: 0.026 \n",
            "(epoch: 97, iters: 3700, time: 0.553) G_GAN: 1.556 G_GAN_Feat: 2.395 G_VGG: 1.364 D_real: 0.058 D_fake: 0.119 \n",
            "(epoch: 97, iters: 3800, time: 0.554) G_GAN: 1.709 G_GAN_Feat: 2.822 G_VGG: 1.580 D_real: 0.009 D_fake: 0.081 \n",
            "saving the latest model (epoch 97, total_steps 407000)\n",
            "(epoch: 97, iters: 3900, time: 0.556) G_GAN: 1.977 G_GAN_Feat: 3.325 G_VGG: 2.262 D_real: 0.013 D_fake: 0.006 \n",
            "(epoch: 97, iters: 4000, time: 0.553) G_GAN: 2.256 G_GAN_Feat: 3.136 G_VGG: 1.894 D_real: 0.343 D_fake: 0.028 \n",
            "(epoch: 97, iters: 4100, time: 0.554) G_GAN: 1.617 G_GAN_Feat: 2.395 G_VGG: 1.395 D_real: 0.028 D_fake: 0.071 \n",
            "(epoch: 97, iters: 4200, time: 0.554) G_GAN: 1.833 G_GAN_Feat: 3.102 G_VGG: 1.853 D_real: 0.026 D_fake: 0.019 \n",
            "End of epoch 97 / 200 \t Time Taken: 1491 sec\n",
            "(epoch: 98, iters: 100, time: 0.554) G_GAN: 1.927 G_GAN_Feat: 2.838 G_VGG: 2.003 D_real: 0.012 D_fake: 0.011 \n",
            "(epoch: 98, iters: 200, time: 0.552) G_GAN: 1.688 G_GAN_Feat: 3.145 G_VGG: 2.240 D_real: 0.020 D_fake: 0.047 \n",
            "(epoch: 98, iters: 300, time: 0.552) G_GAN: 2.031 G_GAN_Feat: 3.002 G_VGG: 1.869 D_real: 0.027 D_fake: 0.008 \n",
            "(epoch: 98, iters: 400, time: 0.551) G_GAN: 2.198 G_GAN_Feat: 2.770 G_VGG: 1.639 D_real: 0.063 D_fake: 0.022 \n",
            "(epoch: 98, iters: 500, time: 0.550) G_GAN: 2.254 G_GAN_Feat: 2.304 G_VGG: 1.164 D_real: 0.129 D_fake: 0.049 \n",
            "(epoch: 98, iters: 600, time: 0.549) G_GAN: 2.200 G_GAN_Feat: 3.070 G_VGG: 1.983 D_real: 0.030 D_fake: 0.019 \n",
            "saving the latest model (epoch 98, total_steps 408000)\n",
            "(epoch: 98, iters: 700, time: 0.546) G_GAN: 1.991 G_GAN_Feat: 2.614 G_VGG: 1.670 D_real: 0.149 D_fake: 0.010 \n",
            "(epoch: 98, iters: 800, time: 0.547) G_GAN: 1.613 G_GAN_Feat: 2.951 G_VGG: 1.669 D_real: 0.044 D_fake: 0.132 \n",
            "(epoch: 98, iters: 900, time: 0.549) G_GAN: 1.858 G_GAN_Feat: 2.699 G_VGG: 1.811 D_real: 0.011 D_fake: 0.028 \n",
            "(epoch: 98, iters: 1000, time: 0.549) G_GAN: 1.930 G_GAN_Feat: 2.959 G_VGG: 1.917 D_real: 0.062 D_fake: 0.012 \n",
            "(epoch: 98, iters: 1100, time: 0.551) G_GAN: 2.304 G_GAN_Feat: 2.793 G_VGG: 1.849 D_real: 0.288 D_fake: 0.037 \n",
            "(epoch: 98, iters: 1200, time: 0.550) G_GAN: 1.669 G_GAN_Feat: 2.524 G_VGG: 1.611 D_real: 0.012 D_fake: 0.072 \n",
            "(epoch: 98, iters: 1300, time: 0.549) G_GAN: 1.636 G_GAN_Feat: 3.138 G_VGG: 2.353 D_real: 0.021 D_fake: 0.043 \n",
            "(epoch: 98, iters: 1400, time: 0.549) G_GAN: 1.778 G_GAN_Feat: 3.325 G_VGG: 1.740 D_real: 0.015 D_fake: 0.108 \n",
            "(epoch: 98, iters: 1500, time: 0.549) G_GAN: 1.687 G_GAN_Feat: 2.926 G_VGG: 1.550 D_real: 0.091 D_fake: 0.056 \n",
            "(epoch: 98, iters: 1600, time: 0.549) G_GAN: 2.187 G_GAN_Feat: 3.097 G_VGG: 1.893 D_real: 0.008 D_fake: 0.011 \n",
            "saving the latest model (epoch 98, total_steps 409000)\n",
            "(epoch: 98, iters: 1700, time: 0.547) G_GAN: 1.690 G_GAN_Feat: 2.706 G_VGG: 1.697 D_real: 0.030 D_fake: 0.085 \n",
            "(epoch: 98, iters: 1800, time: 0.547) G_GAN: 1.616 G_GAN_Feat: 2.825 G_VGG: 1.713 D_real: 0.013 D_fake: 0.079 \n",
            "(epoch: 98, iters: 1900, time: 0.548) G_GAN: 1.762 G_GAN_Feat: 2.624 G_VGG: 1.745 D_real: 0.164 D_fake: 0.056 \n",
            "(epoch: 98, iters: 2000, time: 0.548) G_GAN: 1.504 G_GAN_Feat: 2.842 G_VGG: 1.521 D_real: 0.449 D_fake: 0.101 \n",
            "(epoch: 98, iters: 2100, time: 0.549) G_GAN: 1.847 G_GAN_Feat: 2.831 G_VGG: 1.847 D_real: 0.032 D_fake: 0.034 \n",
            "(epoch: 98, iters: 2200, time: 0.549) G_GAN: 1.800 G_GAN_Feat: 2.856 G_VGG: 1.645 D_real: 0.042 D_fake: 0.061 \n",
            "(epoch: 98, iters: 2300, time: 0.549) G_GAN: 1.746 G_GAN_Feat: 2.572 G_VGG: 1.538 D_real: 0.010 D_fake: 0.088 \n",
            "(epoch: 98, iters: 2400, time: 0.549) G_GAN: 1.868 G_GAN_Feat: 2.759 G_VGG: 1.814 D_real: 0.365 D_fake: 0.023 \n",
            "(epoch: 98, iters: 2500, time: 0.549) G_GAN: 1.944 G_GAN_Feat: 2.746 G_VGG: 1.666 D_real: 0.019 D_fake: 0.015 \n",
            "(epoch: 98, iters: 2600, time: 0.549) G_GAN: 1.981 G_GAN_Feat: 2.628 G_VGG: 1.677 D_real: 0.016 D_fake: 0.013 \n",
            "saving the latest model (epoch 98, total_steps 410000)\n",
            "(epoch: 98, iters: 2700, time: 0.547) G_GAN: 1.858 G_GAN_Feat: 2.768 G_VGG: 1.671 D_real: 0.039 D_fake: 0.054 \n",
            "(epoch: 98, iters: 2800, time: 0.547) G_GAN: 2.111 G_GAN_Feat: 3.190 G_VGG: 2.182 D_real: 0.059 D_fake: 0.012 \n",
            "(epoch: 98, iters: 2900, time: 0.548) G_GAN: 2.024 G_GAN_Feat: 3.254 G_VGG: 1.935 D_real: 0.094 D_fake: 0.010 \n",
            "(epoch: 98, iters: 3000, time: 0.549) G_GAN: 1.113 G_GAN_Feat: 2.415 G_VGG: 1.216 D_real: 0.052 D_fake: 0.547 \n",
            "(epoch: 98, iters: 3100, time: 0.549) G_GAN: 1.857 G_GAN_Feat: 2.785 G_VGG: 1.481 D_real: 0.018 D_fake: 0.016 \n",
            "(epoch: 98, iters: 3200, time: 0.549) G_GAN: 1.943 G_GAN_Feat: 2.736 G_VGG: 1.410 D_real: 0.011 D_fake: 0.009 \n",
            "(epoch: 98, iters: 3300, time: 0.549) G_GAN: 2.116 G_GAN_Feat: 2.958 G_VGG: 2.145 D_real: 0.016 D_fake: 0.016 \n",
            "(epoch: 98, iters: 3400, time: 0.549) G_GAN: 1.920 G_GAN_Feat: 3.165 G_VGG: 1.807 D_real: 0.021 D_fake: 0.017 \n",
            "(epoch: 98, iters: 3500, time: 0.549) G_GAN: 1.977 G_GAN_Feat: 2.793 G_VGG: 1.292 D_real: 0.022 D_fake: 0.007 \n",
            "(epoch: 98, iters: 3600, time: 0.549) G_GAN: 1.816 G_GAN_Feat: 2.809 G_VGG: 2.106 D_real: 0.023 D_fake: 0.041 \n",
            "saving the latest model (epoch 98, total_steps 411000)\n",
            "(epoch: 98, iters: 3700, time: 0.547) G_GAN: 1.512 G_GAN_Feat: 2.545 G_VGG: 1.556 D_real: 0.010 D_fake: 0.156 \n",
            "(epoch: 98, iters: 3800, time: 0.548) G_GAN: 1.895 G_GAN_Feat: 2.567 G_VGG: 1.677 D_real: 0.060 D_fake: 0.038 \n",
            "(epoch: 98, iters: 3900, time: 0.548) G_GAN: 2.115 G_GAN_Feat: 2.609 G_VGG: 1.530 D_real: 0.025 D_fake: 0.008 \n",
            "(epoch: 98, iters: 4000, time: 0.549) G_GAN: 1.682 G_GAN_Feat: 2.741 G_VGG: 2.013 D_real: 0.055 D_fake: 0.047 \n",
            "(epoch: 98, iters: 4100, time: 0.549) G_GAN: 1.692 G_GAN_Feat: 2.344 G_VGG: 1.172 D_real: 0.050 D_fake: 0.110 \n",
            "(epoch: 98, iters: 4200, time: 0.549) G_GAN: 1.891 G_GAN_Feat: 2.831 G_VGG: 1.953 D_real: 0.011 D_fake: 0.016 \n",
            "End of epoch 98 / 200 \t Time Taken: 2329 sec\n",
            "(epoch: 99, iters: 100, time: 0.549) G_GAN: 2.074 G_GAN_Feat: 2.720 G_VGG: 1.932 D_real: 0.035 D_fake: 0.016 \n",
            "(epoch: 99, iters: 200, time: 0.549) G_GAN: 1.184 G_GAN_Feat: 2.494 G_VGG: 1.661 D_real: 0.026 D_fake: 0.541 \n",
            "(epoch: 99, iters: 300, time: 0.549) G_GAN: 1.913 G_GAN_Feat: 2.700 G_VGG: 1.563 D_real: 0.031 D_fake: 0.025 \n",
            "(epoch: 99, iters: 400, time: 0.549) G_GAN: 1.729 G_GAN_Feat: 2.747 G_VGG: 1.441 D_real: 0.019 D_fake: 0.073 \n",
            "saving the latest model (epoch 99, total_steps 412000)\n",
            "(epoch: 99, iters: 500, time: 0.547) G_GAN: 1.938 G_GAN_Feat: 3.020 G_VGG: 2.087 D_real: 0.010 D_fake: 0.009 \n",
            "(epoch: 99, iters: 600, time: 0.548) G_GAN: 1.817 G_GAN_Feat: 2.844 G_VGG: 2.002 D_real: 0.083 D_fake: 0.053 \n",
            "(epoch: 99, iters: 700, time: 0.549) G_GAN: 1.826 G_GAN_Feat: 2.927 G_VGG: 1.803 D_real: 0.014 D_fake: 0.155 \n",
            "(epoch: 99, iters: 800, time: 0.549) G_GAN: 2.048 G_GAN_Feat: 2.719 G_VGG: 1.958 D_real: 0.018 D_fake: 0.010 \n",
            "(epoch: 99, iters: 900, time: 0.549) G_GAN: 1.648 G_GAN_Feat: 2.854 G_VGG: 2.243 D_real: 0.019 D_fake: 0.070 \n",
            "(epoch: 99, iters: 1000, time: 0.549) G_GAN: 1.992 G_GAN_Feat: 2.778 G_VGG: 1.683 D_real: 0.021 D_fake: 0.005 \n",
            "(epoch: 99, iters: 1100, time: 0.549) G_GAN: 2.081 G_GAN_Feat: 2.619 G_VGG: 1.748 D_real: 0.025 D_fake: 0.008 \n",
            "(epoch: 99, iters: 1200, time: 0.549) G_GAN: 1.109 G_GAN_Feat: 2.362 G_VGG: 1.549 D_real: 0.114 D_fake: 0.336 \n",
            "(epoch: 99, iters: 1300, time: 0.549) G_GAN: 2.043 G_GAN_Feat: 3.186 G_VGG: 1.761 D_real: 0.006 D_fake: 0.003 \n",
            "(epoch: 99, iters: 1400, time: 0.549) G_GAN: 2.011 G_GAN_Feat: 2.506 G_VGG: 1.637 D_real: 0.012 D_fake: 0.007 \n",
            "saving the latest model (epoch 99, total_steps 413000)\n",
            "(epoch: 99, iters: 1500, time: 0.547) G_GAN: 1.754 G_GAN_Feat: 2.620 G_VGG: 1.468 D_real: 0.124 D_fake: 0.153 \n",
            "(epoch: 99, iters: 1600, time: 0.550) G_GAN: 2.073 G_GAN_Feat: 2.351 G_VGG: 1.345 D_real: 0.078 D_fake: 0.022 \n",
            "(epoch: 99, iters: 1700, time: 0.554) G_GAN: 1.755 G_GAN_Feat: 2.612 G_VGG: 1.582 D_real: 0.100 D_fake: 0.046 \n",
            "(epoch: 99, iters: 1800, time: 0.554) G_GAN: 1.714 G_GAN_Feat: 2.459 G_VGG: 1.403 D_real: 0.014 D_fake: 0.062 \n",
            "(epoch: 99, iters: 1900, time: 0.553) G_GAN: 1.947 G_GAN_Feat: 3.306 G_VGG: 1.894 D_real: 0.007 D_fake: 0.006 \n",
            "(epoch: 99, iters: 2000, time: 0.554) G_GAN: 1.665 G_GAN_Feat: 2.794 G_VGG: 1.520 D_real: 0.069 D_fake: 0.048 \n",
            "(epoch: 99, iters: 2100, time: 0.554) G_GAN: 1.914 G_GAN_Feat: 3.322 G_VGG: 1.894 D_real: 0.013 D_fake: 0.019 \n",
            "(epoch: 99, iters: 2200, time: 0.554) G_GAN: 1.889 G_GAN_Feat: 2.654 G_VGG: 1.821 D_real: 0.008 D_fake: 0.018 \n",
            "(epoch: 99, iters: 2300, time: 0.553) G_GAN: 1.745 G_GAN_Feat: 2.808 G_VGG: 2.120 D_real: 0.024 D_fake: 0.043 \n",
            "(epoch: 99, iters: 2400, time: 0.553) G_GAN: 1.740 G_GAN_Feat: 3.011 G_VGG: 2.121 D_real: 0.011 D_fake: 0.025 \n",
            "saving the latest model (epoch 99, total_steps 414000)\n",
            "(epoch: 99, iters: 2500, time: 0.552) G_GAN: 1.617 G_GAN_Feat: 2.687 G_VGG: 1.218 D_real: 0.008 D_fake: 0.114 \n",
            "(epoch: 99, iters: 2600, time: 0.551) G_GAN: 1.897 G_GAN_Feat: 2.901 G_VGG: 1.661 D_real: 0.010 D_fake: 0.013 \n",
            "(epoch: 99, iters: 2700, time: 0.550) G_GAN: 2.195 G_GAN_Feat: 2.783 G_VGG: 2.024 D_real: 0.052 D_fake: 0.023 \n",
            "(epoch: 99, iters: 2800, time: 0.550) G_GAN: 1.758 G_GAN_Feat: 2.996 G_VGG: 2.116 D_real: 0.019 D_fake: 0.025 \n",
            "(epoch: 99, iters: 2900, time: 0.549) G_GAN: 1.612 G_GAN_Feat: 2.586 G_VGG: 1.547 D_real: 0.058 D_fake: 0.109 \n",
            "(epoch: 99, iters: 3000, time: 0.549) G_GAN: 2.024 G_GAN_Feat: 2.702 G_VGG: 1.456 D_real: 0.010 D_fake: 0.009 \n",
            "(epoch: 99, iters: 3100, time: 0.548) G_GAN: 1.887 G_GAN_Feat: 2.752 G_VGG: 1.515 D_real: 0.012 D_fake: 0.017 \n",
            "(epoch: 99, iters: 3200, time: 0.549) G_GAN: 1.722 G_GAN_Feat: 2.869 G_VGG: 1.913 D_real: 0.022 D_fake: 0.096 \n",
            "(epoch: 99, iters: 3300, time: 0.549) G_GAN: 1.989 G_GAN_Feat: 2.893 G_VGG: 1.723 D_real: 0.085 D_fake: 0.007 \n",
            "(epoch: 99, iters: 3400, time: 0.549) G_GAN: 2.044 G_GAN_Feat: 2.686 G_VGG: 1.735 D_real: 0.004 D_fake: 0.008 \n",
            "saving the latest model (epoch 99, total_steps 415000)\n",
            "(epoch: 99, iters: 3500, time: 0.547) G_GAN: 1.912 G_GAN_Feat: 2.797 G_VGG: 1.559 D_real: 0.009 D_fake: 0.025 \n",
            "(epoch: 99, iters: 3600, time: 0.548) G_GAN: 2.063 G_GAN_Feat: 2.656 G_VGG: 1.708 D_real: 0.147 D_fake: 0.012 \n",
            "(epoch: 99, iters: 3700, time: 0.548) G_GAN: 1.399 G_GAN_Feat: 2.756 G_VGG: 1.594 D_real: 0.140 D_fake: 0.114 \n",
            "(epoch: 99, iters: 3800, time: 0.548) G_GAN: 2.039 G_GAN_Feat: 2.849 G_VGG: 1.777 D_real: 0.036 D_fake: 0.007 \n",
            "(epoch: 99, iters: 3900, time: 0.549) G_GAN: 1.833 G_GAN_Feat: 2.657 G_VGG: 1.735 D_real: 0.085 D_fake: 0.053 \n",
            "(epoch: 99, iters: 4000, time: 0.549) G_GAN: 2.017 G_GAN_Feat: 2.816 G_VGG: 2.053 D_real: 0.243 D_fake: 0.014 \n",
            "(epoch: 99, iters: 4100, time: 0.549) G_GAN: 1.900 G_GAN_Feat: 2.901 G_VGG: 1.966 D_real: 0.016 D_fake: 0.009 \n",
            "(epoch: 99, iters: 4200, time: 0.549) G_GAN: 2.084 G_GAN_Feat: 2.667 G_VGG: 1.881 D_real: 0.021 D_fake: 0.015 \n",
            "End of epoch 99 / 200 \t Time Taken: 2332 sec\n",
            "(epoch: 100, iters: 100, time: 0.549) G_GAN: 1.942 G_GAN_Feat: 2.866 G_VGG: 1.855 D_real: 0.046 D_fake: 0.017 \n",
            "(epoch: 100, iters: 200, time: 0.549) G_GAN: 1.877 G_GAN_Feat: 2.907 G_VGG: 2.342 D_real: 0.014 D_fake: 0.023 \n",
            "saving the latest model (epoch 100, total_steps 416000)\n",
            "(epoch: 100, iters: 300, time: 0.547) G_GAN: 1.781 G_GAN_Feat: 2.847 G_VGG: 1.744 D_real: 0.031 D_fake: 0.035 \n",
            "(epoch: 100, iters: 400, time: 0.548) G_GAN: 1.777 G_GAN_Feat: 2.565 G_VGG: 1.514 D_real: 0.040 D_fake: 0.138 \n",
            "(epoch: 100, iters: 500, time: 0.548) G_GAN: 2.203 G_GAN_Feat: 2.456 G_VGG: 1.511 D_real: 0.100 D_fake: 0.090 \n",
            "(epoch: 100, iters: 600, time: 0.549) G_GAN: 1.670 G_GAN_Feat: 3.231 G_VGG: 2.109 D_real: 0.029 D_fake: 0.144 \n",
            "(epoch: 100, iters: 700, time: 0.549) G_GAN: 1.765 G_GAN_Feat: 2.679 G_VGG: 1.638 D_real: 0.032 D_fake: 0.053 \n",
            "(epoch: 100, iters: 800, time: 0.549) G_GAN: 2.044 G_GAN_Feat: 2.652 G_VGG: 1.256 D_real: 0.076 D_fake: 0.015 \n",
            "(epoch: 100, iters: 900, time: 0.549) G_GAN: 1.687 G_GAN_Feat: 2.701 G_VGG: 1.847 D_real: 0.343 D_fake: 0.050 \n",
            "(epoch: 100, iters: 1000, time: 0.549) G_GAN: 1.852 G_GAN_Feat: 2.450 G_VGG: 1.163 D_real: 0.043 D_fake: 0.068 \n",
            "(epoch: 100, iters: 1100, time: 0.550) G_GAN: 1.707 G_GAN_Feat: 2.770 G_VGG: 1.736 D_real: 0.026 D_fake: 0.021 \n",
            "(epoch: 100, iters: 1200, time: 0.549) G_GAN: 1.617 G_GAN_Feat: 2.579 G_VGG: 1.579 D_real: 0.147 D_fake: 0.056 \n",
            "saving the latest model (epoch 100, total_steps 417000)\n",
            "(epoch: 100, iters: 1300, time: 0.547) G_GAN: 2.101 G_GAN_Feat: 3.213 G_VGG: 1.824 D_real: 0.013 D_fake: 0.009 \n",
            "(epoch: 100, iters: 1400, time: 0.548) G_GAN: 2.122 G_GAN_Feat: 2.775 G_VGG: 2.171 D_real: 0.007 D_fake: 0.009 \n",
            "(epoch: 100, iters: 1500, time: 0.548) G_GAN: 1.092 G_GAN_Feat: 2.871 G_VGG: 1.996 D_real: 0.140 D_fake: 0.316 \n",
            "(epoch: 100, iters: 1600, time: 0.549) G_GAN: 1.635 G_GAN_Feat: 2.556 G_VGG: 1.899 D_real: 0.021 D_fake: 0.060 \n",
            "(epoch: 100, iters: 1700, time: 0.548) G_GAN: 1.618 G_GAN_Feat: 3.139 G_VGG: 2.025 D_real: 0.028 D_fake: 0.122 \n",
            "(epoch: 100, iters: 1800, time: 0.550) G_GAN: 2.097 G_GAN_Feat: 2.481 G_VGG: 1.524 D_real: 0.019 D_fake: 0.015 \n",
            "(epoch: 100, iters: 1900, time: 0.549) G_GAN: 1.889 G_GAN_Feat: 2.602 G_VGG: 1.597 D_real: 0.018 D_fake: 0.046 \n",
            "(epoch: 100, iters: 2000, time: 0.549) G_GAN: 1.420 G_GAN_Feat: 2.255 G_VGG: 1.411 D_real: 0.032 D_fake: 0.137 \n",
            "(epoch: 100, iters: 2100, time: 0.549) G_GAN: 1.851 G_GAN_Feat: 3.077 G_VGG: 1.699 D_real: 0.015 D_fake: 0.017 \n",
            "(epoch: 100, iters: 2200, time: 0.549) G_GAN: 2.041 G_GAN_Feat: 2.649 G_VGG: 1.825 D_real: 0.015 D_fake: 0.015 \n",
            "saving the latest model (epoch 100, total_steps 418000)\n",
            "(epoch: 100, iters: 2300, time: 0.548) G_GAN: 2.057 G_GAN_Feat: 2.966 G_VGG: 1.944 D_real: 0.010 D_fake: 0.009 \n",
            "(epoch: 100, iters: 2400, time: 0.549) G_GAN: 1.629 G_GAN_Feat: 3.283 G_VGG: 2.600 D_real: 0.044 D_fake: 0.081 \n",
            "(epoch: 100, iters: 2500, time: 0.549) G_GAN: 1.462 G_GAN_Feat: 2.737 G_VGG: 1.641 D_real: 0.038 D_fake: 0.109 \n",
            "(epoch: 100, iters: 2600, time: 0.549) G_GAN: 1.799 G_GAN_Feat: 2.397 G_VGG: 1.305 D_real: 0.027 D_fake: 0.025 \n",
            "(epoch: 100, iters: 2700, time: 0.550) G_GAN: 2.069 G_GAN_Feat: 2.537 G_VGG: 1.461 D_real: 0.110 D_fake: 0.008 \n",
            "(epoch: 100, iters: 2800, time: 0.550) G_GAN: 2.045 G_GAN_Feat: 2.336 G_VGG: 1.625 D_real: 0.268 D_fake: 0.013 \n",
            "(epoch: 100, iters: 2900, time: 0.550) G_GAN: 1.720 G_GAN_Feat: 2.933 G_VGG: 2.184 D_real: 0.020 D_fake: 0.051 \n",
            "(epoch: 100, iters: 3000, time: 0.550) G_GAN: 1.845 G_GAN_Feat: 2.979 G_VGG: 2.273 D_real: 0.012 D_fake: 0.017 \n",
            "(epoch: 100, iters: 3100, time: 0.550) G_GAN: 1.864 G_GAN_Feat: 2.574 G_VGG: 1.765 D_real: 0.009 D_fake: 0.012 \n",
            "(epoch: 100, iters: 3200, time: 0.550) G_GAN: 1.623 G_GAN_Feat: 3.072 G_VGG: 1.968 D_real: 0.074 D_fake: 0.080 \n",
            "saving the latest model (epoch 100, total_steps 419000)\n",
            "(epoch: 100, iters: 3300, time: 0.548) G_GAN: 1.736 G_GAN_Feat: 2.945 G_VGG: 1.864 D_real: 0.027 D_fake: 0.023 \n",
            "(epoch: 100, iters: 3400, time: 0.549) G_GAN: 1.836 G_GAN_Feat: 2.391 G_VGG: 1.475 D_real: 0.039 D_fake: 0.033 \n",
            "(epoch: 100, iters: 3500, time: 0.549) G_GAN: 1.739 G_GAN_Feat: 2.657 G_VGG: 1.495 D_real: 0.197 D_fake: 0.041 \n",
            "(epoch: 100, iters: 3600, time: 0.549) G_GAN: 1.593 G_GAN_Feat: 2.596 G_VGG: 1.564 D_real: 0.039 D_fake: 0.154 \n",
            "(epoch: 100, iters: 3700, time: 0.550) G_GAN: 2.056 G_GAN_Feat: 2.805 G_VGG: 1.510 D_real: 0.038 D_fake: 0.006 \n",
            "(epoch: 100, iters: 3800, time: 0.550) G_GAN: 1.737 G_GAN_Feat: 2.571 G_VGG: 1.838 D_real: 0.223 D_fake: 0.068 \n",
            "(epoch: 100, iters: 3900, time: 0.550) G_GAN: 1.847 G_GAN_Feat: 2.914 G_VGG: 2.210 D_real: 0.010 D_fake: 0.013 \n",
            "(epoch: 100, iters: 4000, time: 0.550) G_GAN: 1.812 G_GAN_Feat: 3.255 G_VGG: 1.485 D_real: 0.011 D_fake: 0.012 \n",
            "(epoch: 100, iters: 4100, time: 0.550) G_GAN: 2.081 G_GAN_Feat: 2.711 G_VGG: 1.778 D_real: 0.254 D_fake: 0.027 \n",
            "(epoch: 100, iters: 4200, time: 0.550) G_GAN: 1.938 G_GAN_Feat: 2.901 G_VGG: 1.430 D_real: 0.065 D_fake: 0.006 \n",
            "saving the latest model (epoch 100, total_steps 420000)\n",
            "End of epoch 100 / 200 \t Time Taken: 2333 sec\n",
            "saving the model at the end of epoch 100, iters 420000\n",
            "(epoch: 101, iters: 100, time: 0.541) G_GAN: 1.836 G_GAN_Feat: 2.893 G_VGG: 2.319 D_real: 0.153 D_fake: 0.013 \n",
            "(epoch: 101, iters: 200, time: 0.545) G_GAN: 1.966 G_GAN_Feat: 2.941 G_VGG: 1.396 D_real: 0.284 D_fake: 0.012 \n",
            "(epoch: 101, iters: 300, time: 0.547) G_GAN: 1.727 G_GAN_Feat: 2.774 G_VGG: 1.855 D_real: 0.031 D_fake: 0.081 \n",
            "(epoch: 101, iters: 400, time: 0.548) G_GAN: 1.618 G_GAN_Feat: 2.941 G_VGG: 1.917 D_real: 0.013 D_fake: 0.137 \n",
            "(epoch: 101, iters: 500, time: 0.550) G_GAN: 1.926 G_GAN_Feat: 2.785 G_VGG: 1.905 D_real: 0.230 D_fake: 0.027 \n",
            "(epoch: 101, iters: 600, time: 0.550) G_GAN: 1.983 G_GAN_Feat: 2.726 G_VGG: 1.910 D_real: 0.042 D_fake: 0.007 \n",
            "(epoch: 101, iters: 700, time: 0.550) G_GAN: 1.453 G_GAN_Feat: 2.512 G_VGG: 1.446 D_real: 0.235 D_fake: 0.169 \n",
            "(epoch: 101, iters: 800, time: 0.550) G_GAN: 1.771 G_GAN_Feat: 2.562 G_VGG: 1.153 D_real: 0.009 D_fake: 0.024 \n",
            "(epoch: 101, iters: 900, time: 0.550) G_GAN: 2.028 G_GAN_Feat: 2.823 G_VGG: 1.667 D_real: 0.043 D_fake: 0.031 \n",
            "(epoch: 101, iters: 1000, time: 0.550) G_GAN: 2.037 G_GAN_Feat: 2.565 G_VGG: 1.734 D_real: 0.036 D_fake: 0.004 \n",
            "saving the latest model (epoch 101, total_steps 421000)\n",
            "(epoch: 101, iters: 1100, time: 0.548) G_GAN: 1.987 G_GAN_Feat: 2.842 G_VGG: 1.824 D_real: 0.009 D_fake: 0.013 \n",
            "(epoch: 101, iters: 1200, time: 0.549) G_GAN: 1.533 G_GAN_Feat: 2.895 G_VGG: 2.024 D_real: 0.055 D_fake: 0.060 \n",
            "(epoch: 101, iters: 1300, time: 0.550) G_GAN: 1.947 G_GAN_Feat: 2.882 G_VGG: 1.722 D_real: 0.071 D_fake: 0.055 \n",
            "(epoch: 101, iters: 1400, time: 0.550) G_GAN: 1.957 G_GAN_Feat: 2.480 G_VGG: 1.662 D_real: 0.014 D_fake: 0.009 \n",
            "(epoch: 101, iters: 1500, time: 0.550) G_GAN: 2.067 G_GAN_Feat: 2.951 G_VGG: 1.828 D_real: 0.012 D_fake: 0.005 \n",
            "(epoch: 101, iters: 1600, time: 0.550) G_GAN: 1.997 G_GAN_Feat: 2.685 G_VGG: 1.591 D_real: 0.038 D_fake: 0.028 \n",
            "(epoch: 101, iters: 1700, time: 0.550) G_GAN: 1.881 G_GAN_Feat: 2.525 G_VGG: 1.584 D_real: 0.075 D_fake: 0.051 \n",
            "(epoch: 101, iters: 1800, time: 0.550) G_GAN: 1.963 G_GAN_Feat: 2.731 G_VGG: 1.428 D_real: 0.067 D_fake: 0.023 \n",
            "(epoch: 101, iters: 1900, time: 0.550) G_GAN: 1.634 G_GAN_Feat: 2.614 G_VGG: 1.824 D_real: 0.006 D_fake: 0.078 \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/train.py\", line 95, in <module>\n",
            "    loss_D.backward()        \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on high resolution (1024) G1 and G2 (local+global) starting from epoch 100 of G1\n",
        "\n",
        "#import torch\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "!python train.py --netG local --continue_train --load_pretrain checkpoints/nebbia/ --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 1024 --batchSize 2"
      ],
      "metadata": {
        "id": "3XhQJ5s95ZUj",
        "outputId": "e66a7b05-5166-430b-a269-d099aa36ab74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 2\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 1024\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: checkpoints/nebbia/\n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: local\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 101 at iteration 2000\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "LocalEnhancer(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 128, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "  )\n",
            "  (model1_1): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "  )\n",
            "  (model1_2): Sequential(\n",
            "    (0): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (1): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (2): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "create web directory ./checkpoints/nebbia/web...\n",
            "(epoch: 101, iters: 2100, time: 0.383) G_GAN: 1.592 G_GAN_Feat: 5.207 G_VGG: 2.289 D_real: 0.071 D_fake: 0.065 \n",
            "(epoch: 101, iters: 2200, time: 0.290) G_GAN: 1.348 G_GAN_Feat: 3.637 G_VGG: 1.688 D_real: 0.248 D_fake: 0.256 \n",
            "(epoch: 101, iters: 2300, time: 0.304) G_GAN: 1.714 G_GAN_Feat: 4.760 G_VGG: 1.955 D_real: 0.022 D_fake: 0.057 \n",
            "(epoch: 101, iters: 2400, time: 0.294) G_GAN: 1.613 G_GAN_Feat: 4.804 G_VGG: 2.445 D_real: 0.036 D_fake: 0.061 \n",
            "(epoch: 101, iters: 2500, time: 0.292) G_GAN: 1.564 G_GAN_Feat: 3.755 G_VGG: 2.038 D_real: 0.039 D_fake: 0.076 \n",
            "(epoch: 101, iters: 2600, time: 0.312) G_GAN: 1.336 G_GAN_Feat: 5.686 G_VGG: 2.232 D_real: 0.021 D_fake: 0.215 \n",
            "(epoch: 101, iters: 2700, time: 0.304) G_GAN: 2.030 G_GAN_Feat: 5.321 G_VGG: 1.513 D_real: 0.029 D_fake: 0.010 \n",
            "(epoch: 101, iters: 2800, time: 0.308) G_GAN: 2.152 G_GAN_Feat: 3.912 G_VGG: 1.695 D_real: 0.118 D_fake: 0.034 \n",
            "(epoch: 101, iters: 2900, time: 0.269) G_GAN: 2.145 G_GAN_Feat: 4.968 G_VGG: 2.125 D_real: 0.174 D_fake: 0.025 \n",
            "(epoch: 101, iters: 3000, time: 0.311) G_GAN: 1.503 G_GAN_Feat: 4.705 G_VGG: 1.572 D_real: 0.048 D_fake: 0.063 \n",
            "saving the latest model (epoch 101, total_steps 423000)\n",
            "(epoch: 101, iters: 3100, time: 0.299) G_GAN: 2.045 G_GAN_Feat: 5.750 G_VGG: 1.668 D_real: 0.019 D_fake: 0.011 \n",
            "(epoch: 101, iters: 3200, time: 0.293) G_GAN: 0.541 G_GAN_Feat: 3.046 G_VGG: 1.551 D_real: 0.045 D_fake: 1.347 \n",
            "(epoch: 101, iters: 3300, time: 0.315) G_GAN: 1.358 G_GAN_Feat: 4.192 G_VGG: 1.528 D_real: 0.041 D_fake: 0.202 \n",
            "(epoch: 101, iters: 3400, time: 0.299) G_GAN: 1.480 G_GAN_Feat: 5.785 G_VGG: 1.794 D_real: 0.013 D_fake: 0.129 \n",
            "(epoch: 101, iters: 3500, time: 0.311) G_GAN: 2.077 G_GAN_Feat: 4.840 G_VGG: 1.624 D_real: 0.625 D_fake: 0.026 \n",
            "(epoch: 101, iters: 3600, time: 0.285) G_GAN: 1.586 G_GAN_Feat: 4.124 G_VGG: 1.671 D_real: 0.018 D_fake: 0.192 \n",
            "(epoch: 101, iters: 3700, time: 0.308) G_GAN: 1.115 G_GAN_Feat: 4.132 G_VGG: 1.491 D_real: 0.024 D_fake: 0.549 \n",
            "(epoch: 101, iters: 3800, time: 0.334) G_GAN: 1.803 G_GAN_Feat: 4.478 G_VGG: 1.400 D_real: 0.154 D_fake: 0.031 \n",
            "(epoch: 101, iters: 3900, time: 0.269) G_GAN: 1.747 G_GAN_Feat: 4.181 G_VGG: 1.496 D_real: 0.024 D_fake: 0.041 \n",
            "(epoch: 101, iters: 4000, time: 0.293) G_GAN: 1.533 G_GAN_Feat: 4.542 G_VGG: 1.911 D_real: 0.060 D_fake: 0.120 \n",
            "saving the latest model (epoch 101, total_steps 424000)\n",
            "(epoch: 101, iters: 4100, time: 0.284) G_GAN: 1.530 G_GAN_Feat: 3.843 G_VGG: 1.794 D_real: 0.097 D_fake: 0.150 \n",
            "(epoch: 101, iters: 4200, time: 0.296) G_GAN: 1.383 G_GAN_Feat: 3.406 G_VGG: 1.339 D_real: 0.080 D_fake: 0.160 \n",
            "End of epoch 101 / 200 \t Time Taken: 705 sec\n",
            "(epoch: 102, iters: 100, time: 0.193) G_GAN: 1.809 G_GAN_Feat: 5.276 G_VGG: 1.367 D_real: 0.022 D_fake: 0.026 \n",
            "(epoch: 102, iters: 200, time: 0.193) G_GAN: 1.881 G_GAN_Feat: 5.282 G_VGG: 1.957 D_real: 0.066 D_fake: 0.037 \n",
            "(epoch: 102, iters: 300, time: 0.240) G_GAN: 2.049 G_GAN_Feat: 5.034 G_VGG: 2.019 D_real: 0.153 D_fake: 0.033 \n",
            "(epoch: 102, iters: 400, time: 0.225) G_GAN: 1.551 G_GAN_Feat: 4.351 G_VGG: 1.384 D_real: 0.030 D_fake: 0.094 \n",
            "(epoch: 102, iters: 500, time: 0.194) G_GAN: 1.973 G_GAN_Feat: 3.040 G_VGG: 1.674 D_real: 0.275 D_fake: 0.032 \n",
            "(epoch: 102, iters: 600, time: 0.220) G_GAN: 1.418 G_GAN_Feat: 3.723 G_VGG: 1.072 D_real: 0.071 D_fake: 0.139 \n",
            "(epoch: 102, iters: 700, time: 0.248) G_GAN: 1.485 G_GAN_Feat: 3.348 G_VGG: 1.570 D_real: 0.163 D_fake: 0.201 \n",
            "(epoch: 102, iters: 800, time: 0.204) G_GAN: 1.731 G_GAN_Feat: 5.882 G_VGG: 1.240 D_real: 0.042 D_fake: 0.045 \n",
            "saving the latest model (epoch 102, total_steps 425000)\n",
            "(epoch: 102, iters: 900, time: 0.209) G_GAN: 1.952 G_GAN_Feat: 5.027 G_VGG: 1.580 D_real: 0.033 D_fake: 0.040 \n",
            "(epoch: 102, iters: 1000, time: 0.216) G_GAN: 1.686 G_GAN_Feat: 3.432 G_VGG: 1.308 D_real: 0.188 D_fake: 0.141 \n",
            "(epoch: 102, iters: 1100, time: 0.217) G_GAN: 1.913 G_GAN_Feat: 3.372 G_VGG: 1.887 D_real: 0.127 D_fake: 0.018 \n",
            "(epoch: 102, iters: 1200, time: 0.200) G_GAN: 1.832 G_GAN_Feat: 4.231 G_VGG: 1.639 D_real: 0.148 D_fake: 0.024 \n",
            "(epoch: 102, iters: 1300, time: 0.218) G_GAN: 1.617 G_GAN_Feat: 6.255 G_VGG: 2.043 D_real: 0.030 D_fake: 0.158 \n",
            "(epoch: 102, iters: 1400, time: 0.201) G_GAN: 2.163 G_GAN_Feat: 3.985 G_VGG: 0.980 D_real: 0.106 D_fake: 0.032 \n",
            "(epoch: 102, iters: 1500, time: 0.214) G_GAN: 1.905 G_GAN_Feat: 4.820 G_VGG: 1.758 D_real: 0.090 D_fake: 0.082 \n",
            "(epoch: 102, iters: 1600, time: 0.197) G_GAN: 2.397 G_GAN_Feat: 4.035 G_VGG: 1.659 D_real: 0.073 D_fake: 0.054 \n",
            "(epoch: 102, iters: 1700, time: 0.208) G_GAN: 2.221 G_GAN_Feat: 4.375 G_VGG: 1.831 D_real: 0.173 D_fake: 0.023 \n",
            "(epoch: 102, iters: 1800, time: 0.220) G_GAN: 1.602 G_GAN_Feat: 3.855 G_VGG: 1.703 D_real: 0.240 D_fake: 0.085 \n",
            "saving the latest model (epoch 102, total_steps 426000)\n",
            "(epoch: 102, iters: 1900, time: 0.205) G_GAN: 1.885 G_GAN_Feat: 3.782 G_VGG: 1.588 D_real: 0.132 D_fake: 0.049 \n",
            "(epoch: 102, iters: 2000, time: 0.203) G_GAN: 1.280 G_GAN_Feat: 2.738 G_VGG: 1.377 D_real: 0.099 D_fake: 0.202 \n",
            "(epoch: 102, iters: 2100, time: 0.207) G_GAN: 2.328 G_GAN_Feat: 4.256 G_VGG: 1.466 D_real: 0.079 D_fake: 0.053 \n",
            "(epoch: 102, iters: 2200, time: 0.212) G_GAN: 2.064 G_GAN_Feat: 3.140 G_VGG: 1.668 D_real: 0.092 D_fake: 0.097 \n",
            "(epoch: 102, iters: 2300, time: 0.208) G_GAN: 1.909 G_GAN_Feat: 4.211 G_VGG: 1.639 D_real: 0.036 D_fake: 0.018 \n",
            "(epoch: 102, iters: 2400, time: 0.218) G_GAN: 1.719 G_GAN_Feat: 4.589 G_VGG: 1.199 D_real: 0.028 D_fake: 0.041 \n",
            "(epoch: 102, iters: 2500, time: 0.216) G_GAN: 1.245 G_GAN_Feat: 3.168 G_VGG: 1.250 D_real: 0.062 D_fake: 0.309 \n",
            "(epoch: 102, iters: 2600, time: 0.224) G_GAN: 2.016 G_GAN_Feat: 5.258 G_VGG: 1.573 D_real: 0.079 D_fake: 0.021 \n",
            "(epoch: 102, iters: 2700, time: 0.215) G_GAN: 1.723 G_GAN_Feat: 3.594 G_VGG: 1.930 D_real: 0.109 D_fake: 0.035 \n",
            "(epoch: 102, iters: 2800, time: 0.206) G_GAN: 1.905 G_GAN_Feat: 4.851 G_VGG: 1.430 D_real: 0.045 D_fake: 0.019 \n",
            "saving the latest model (epoch 102, total_steps 427000)\n",
            "(epoch: 102, iters: 2900, time: 0.210) G_GAN: 1.590 G_GAN_Feat: 4.270 G_VGG: 1.720 D_real: 0.099 D_fake: 0.097 \n",
            "(epoch: 102, iters: 3000, time: 0.229) G_GAN: 1.855 G_GAN_Feat: 3.692 G_VGG: 1.359 D_real: 0.127 D_fake: 0.072 \n",
            "(epoch: 102, iters: 3100, time: 0.199) G_GAN: 1.405 G_GAN_Feat: 2.432 G_VGG: 1.289 D_real: 0.078 D_fake: 0.202 \n",
            "(epoch: 102, iters: 3200, time: 0.203) G_GAN: 1.751 G_GAN_Feat: 3.857 G_VGG: 1.655 D_real: 0.357 D_fake: 0.052 \n",
            "(epoch: 102, iters: 3300, time: 0.214) G_GAN: 1.776 G_GAN_Feat: 3.613 G_VGG: 1.359 D_real: 0.078 D_fake: 0.056 \n",
            "(epoch: 102, iters: 3400, time: 0.207) G_GAN: 1.515 G_GAN_Feat: 3.666 G_VGG: 1.364 D_real: 0.018 D_fake: 0.106 \n",
            "(epoch: 102, iters: 3500, time: 0.209) G_GAN: 1.637 G_GAN_Feat: 3.128 G_VGG: 1.298 D_real: 0.089 D_fake: 0.128 \n",
            "(epoch: 102, iters: 3600, time: 0.205) G_GAN: 1.823 G_GAN_Feat: 4.308 G_VGG: 1.379 D_real: 0.031 D_fake: 0.042 \n",
            "(epoch: 102, iters: 3700, time: 0.204) G_GAN: 1.257 G_GAN_Feat: 4.143 G_VGG: 1.703 D_real: 0.032 D_fake: 0.359 \n",
            "(epoch: 102, iters: 3800, time: 0.204) G_GAN: 1.123 G_GAN_Feat: 2.697 G_VGG: 1.408 D_real: 0.040 D_fake: 0.466 \n",
            "saving the latest model (epoch 102, total_steps 428000)\n",
            "(epoch: 102, iters: 3900, time: 0.202) G_GAN: 1.609 G_GAN_Feat: 3.775 G_VGG: 1.498 D_real: 0.099 D_fake: 0.068 \n",
            "(epoch: 102, iters: 4000, time: 0.217) G_GAN: 1.587 G_GAN_Feat: 4.152 G_VGG: 1.512 D_real: 0.045 D_fake: 0.204 \n",
            "(epoch: 102, iters: 4100, time: 0.196) G_GAN: 1.695 G_GAN_Feat: 4.618 G_VGG: 1.564 D_real: 0.070 D_fake: 0.122 \n",
            "(epoch: 102, iters: 4200, time: 0.205) G_GAN: 1.692 G_GAN_Feat: 4.393 G_VGG: 1.560 D_real: 0.037 D_fake: 0.089 \n",
            "End of epoch 102 / 200 \t Time Taken: 948 sec\n",
            "(epoch: 103, iters: 100, time: 0.193) G_GAN: 1.639 G_GAN_Feat: 2.899 G_VGG: 1.368 D_real: 0.101 D_fake: 0.079 \n",
            "(epoch: 103, iters: 200, time: 0.193) G_GAN: 1.572 G_GAN_Feat: 4.093 G_VGG: 1.450 D_real: 0.113 D_fake: 0.099 \n",
            "(epoch: 103, iters: 300, time: 0.193) G_GAN: 1.177 G_GAN_Feat: 3.475 G_VGG: 1.643 D_real: 0.159 D_fake: 0.247 \n",
            "(epoch: 103, iters: 400, time: 0.193) G_GAN: 1.978 G_GAN_Feat: 3.779 G_VGG: 1.755 D_real: 0.130 D_fake: 0.014 \n",
            "(epoch: 103, iters: 500, time: 0.193) G_GAN: 1.702 G_GAN_Feat: 3.636 G_VGG: 1.725 D_real: 0.706 D_fake: 0.125 \n",
            "(epoch: 103, iters: 600, time: 0.193) G_GAN: 1.192 G_GAN_Feat: 3.279 G_VGG: 1.642 D_real: 0.147 D_fake: 0.260 \n",
            "saving the latest model (epoch 103, total_steps 429000)\n",
            "(epoch: 103, iters: 700, time: 0.193) G_GAN: 1.490 G_GAN_Feat: 4.806 G_VGG: 1.482 D_real: 0.192 D_fake: 0.216 \n",
            "(epoch: 103, iters: 800, time: 0.193) G_GAN: 0.935 G_GAN_Feat: 2.812 G_VGG: 1.286 D_real: 0.193 D_fake: 0.353 \n",
            "(epoch: 103, iters: 900, time: 0.193) G_GAN: 1.075 G_GAN_Feat: 4.440 G_VGG: 1.064 D_real: 0.025 D_fake: 0.864 \n",
            "(epoch: 103, iters: 1000, time: 0.193) G_GAN: 0.997 G_GAN_Feat: 3.581 G_VGG: 1.111 D_real: 0.018 D_fake: 0.596 \n",
            "(epoch: 103, iters: 1100, time: 0.193) G_GAN: 1.793 G_GAN_Feat: 3.373 G_VGG: 1.621 D_real: 0.101 D_fake: 0.029 \n",
            "(epoch: 103, iters: 1200, time: 0.193) G_GAN: 1.906 G_GAN_Feat: 5.966 G_VGG: 1.758 D_real: 0.026 D_fake: 0.010 \n",
            "(epoch: 103, iters: 1300, time: 0.193) G_GAN: 1.307 G_GAN_Feat: 3.808 G_VGG: 1.271 D_real: 0.100 D_fake: 0.282 \n",
            "(epoch: 103, iters: 1400, time: 0.193) G_GAN: 1.053 G_GAN_Feat: 2.815 G_VGG: 1.240 D_real: 0.050 D_fake: 0.463 \n",
            "(epoch: 103, iters: 1500, time: 0.193) G_GAN: 2.492 G_GAN_Feat: 5.499 G_VGG: 1.970 D_real: 0.082 D_fake: 0.056 \n",
            "(epoch: 103, iters: 1600, time: 0.193) G_GAN: 1.839 G_GAN_Feat: 5.348 G_VGG: 1.963 D_real: 0.218 D_fake: 0.053 \n",
            "saving the latest model (epoch 103, total_steps 430000)\n",
            "(epoch: 103, iters: 1700, time: 0.193) G_GAN: 1.607 G_GAN_Feat: 3.982 G_VGG: 0.968 D_real: 0.036 D_fake: 0.182 \n",
            "(epoch: 103, iters: 1800, time: 0.193) G_GAN: 1.570 G_GAN_Feat: 3.996 G_VGG: 1.583 D_real: 0.036 D_fake: 0.258 \n",
            "(epoch: 103, iters: 1900, time: 0.193) G_GAN: 1.280 G_GAN_Feat: 3.106 G_VGG: 1.073 D_real: 0.070 D_fake: 0.196 \n",
            "(epoch: 103, iters: 2000, time: 0.193) G_GAN: 0.892 G_GAN_Feat: 2.748 G_VGG: 1.073 D_real: 0.033 D_fake: 0.552 \n",
            "(epoch: 103, iters: 2100, time: 0.193) G_GAN: 1.183 G_GAN_Feat: 3.004 G_VGG: 1.435 D_real: 0.029 D_fake: 0.391 \n",
            "(epoch: 103, iters: 2200, time: 0.193) G_GAN: 2.265 G_GAN_Feat: 4.246 G_VGG: 1.565 D_real: 0.358 D_fake: 0.059 \n",
            "(epoch: 103, iters: 2300, time: 0.193) G_GAN: 1.504 G_GAN_Feat: 5.130 G_VGG: 1.624 D_real: 0.058 D_fake: 0.205 \n",
            "(epoch: 103, iters: 2400, time: 0.193) G_GAN: 1.271 G_GAN_Feat: 4.015 G_VGG: 1.289 D_real: 0.148 D_fake: 0.347 \n",
            "(epoch: 103, iters: 2500, time: 0.193) G_GAN: 1.305 G_GAN_Feat: 2.469 G_VGG: 1.146 D_real: 0.173 D_fake: 0.231 \n",
            "(epoch: 103, iters: 2600, time: 0.193) G_GAN: 2.151 G_GAN_Feat: 4.570 G_VGG: 1.286 D_real: 0.064 D_fake: 0.020 \n",
            "saving the latest model (epoch 103, total_steps 431000)\n",
            "(epoch: 103, iters: 2700, time: 0.193) G_GAN: 2.003 G_GAN_Feat: 4.484 G_VGG: 1.547 D_real: 0.048 D_fake: 0.016 \n",
            "(epoch: 103, iters: 2800, time: 0.193) G_GAN: 1.660 G_GAN_Feat: 4.073 G_VGG: 1.493 D_real: 0.336 D_fake: 0.094 \n",
            "(epoch: 103, iters: 2900, time: 0.193) G_GAN: 0.939 G_GAN_Feat: 3.046 G_VGG: 1.062 D_real: 0.049 D_fake: 0.403 \n",
            "(epoch: 103, iters: 3000, time: 0.193) G_GAN: 1.898 G_GAN_Feat: 4.998 G_VGG: 1.690 D_real: 0.152 D_fake: 0.033 \n",
            "(epoch: 103, iters: 3100, time: 0.193) G_GAN: 1.735 G_GAN_Feat: 3.648 G_VGG: 1.303 D_real: 0.022 D_fake: 0.031 \n",
            "(epoch: 103, iters: 3200, time: 0.193) G_GAN: 1.335 G_GAN_Feat: 3.491 G_VGG: 1.289 D_real: 0.099 D_fake: 0.328 \n",
            "(epoch: 103, iters: 3300, time: 0.193) G_GAN: 1.188 G_GAN_Feat: 3.237 G_VGG: 1.051 D_real: 0.204 D_fake: 0.311 \n",
            "(epoch: 103, iters: 3400, time: 0.193) G_GAN: 1.272 G_GAN_Feat: 3.194 G_VGG: 1.408 D_real: 0.027 D_fake: 0.563 \n",
            "(epoch: 103, iters: 3500, time: 0.193) G_GAN: 1.507 G_GAN_Feat: 4.371 G_VGG: 1.547 D_real: 0.015 D_fake: 0.403 \n",
            "(epoch: 103, iters: 3600, time: 0.193) G_GAN: 1.141 G_GAN_Feat: 3.243 G_VGG: 1.286 D_real: 0.026 D_fake: 0.497 \n",
            "saving the latest model (epoch 103, total_steps 432000)\n",
            "(epoch: 103, iters: 3700, time: 0.193) G_GAN: 1.429 G_GAN_Feat: 3.782 G_VGG: 1.178 D_real: 0.100 D_fake: 0.282 \n",
            "(epoch: 103, iters: 3800, time: 0.193) G_GAN: 1.625 G_GAN_Feat: 3.691 G_VGG: 1.574 D_real: 0.125 D_fake: 0.144 \n",
            "(epoch: 103, iters: 3900, time: 0.193) G_GAN: 1.923 G_GAN_Feat: 3.171 G_VGG: 1.225 D_real: 0.189 D_fake: 0.031 \n",
            "(epoch: 103, iters: 4000, time: 0.193) G_GAN: 1.019 G_GAN_Feat: 3.948 G_VGG: 1.221 D_real: 0.026 D_fake: 0.539 \n",
            "(epoch: 103, iters: 4100, time: 0.193) G_GAN: 1.277 G_GAN_Feat: 2.966 G_VGG: 1.350 D_real: 0.255 D_fake: 0.293 \n",
            "(epoch: 103, iters: 4200, time: 0.193) G_GAN: 0.961 G_GAN_Feat: 2.682 G_VGG: 1.225 D_real: 0.079 D_fake: 0.282 \n",
            "End of epoch 103 / 200 \t Time Taken: 873 sec\n",
            "(epoch: 104, iters: 100, time: 0.193) G_GAN: 2.021 G_GAN_Feat: 4.588 G_VGG: 1.365 D_real: 0.051 D_fake: 0.022 \n",
            "(epoch: 104, iters: 200, time: 0.193) G_GAN: 1.538 G_GAN_Feat: 2.938 G_VGG: 1.150 D_real: 0.321 D_fake: 0.191 \n",
            "(epoch: 104, iters: 300, time: 0.193) G_GAN: 1.854 G_GAN_Feat: 4.202 G_VGG: 1.512 D_real: 0.131 D_fake: 0.015 \n",
            "(epoch: 104, iters: 400, time: 0.193) G_GAN: 1.487 G_GAN_Feat: 3.293 G_VGG: 1.266 D_real: 0.217 D_fake: 0.091 \n",
            "saving the latest model (epoch 104, total_steps 433000)\n",
            "(epoch: 104, iters: 500, time: 0.193) G_GAN: 1.103 G_GAN_Feat: 3.011 G_VGG: 1.255 D_real: 0.045 D_fake: 0.520 \n",
            "(epoch: 104, iters: 600, time: 0.193) G_GAN: 1.804 G_GAN_Feat: 4.250 G_VGG: 1.730 D_real: 0.090 D_fake: 0.070 \n",
            "(epoch: 104, iters: 700, time: 0.193) G_GAN: 1.290 G_GAN_Feat: 3.006 G_VGG: 1.305 D_real: 0.085 D_fake: 0.224 \n",
            "(epoch: 104, iters: 800, time: 0.193) G_GAN: 1.406 G_GAN_Feat: 4.003 G_VGG: 1.571 D_real: 0.016 D_fake: 0.136 \n",
            "(epoch: 104, iters: 900, time: 0.193) G_GAN: 0.816 G_GAN_Feat: 2.802 G_VGG: 1.370 D_real: 0.150 D_fake: 0.409 \n",
            "(epoch: 104, iters: 1000, time: 0.193) G_GAN: 2.004 G_GAN_Feat: 4.587 G_VGG: 1.726 D_real: 0.502 D_fake: 0.028 \n",
            "(epoch: 104, iters: 1100, time: 0.193) G_GAN: 1.653 G_GAN_Feat: 3.056 G_VGG: 1.209 D_real: 0.066 D_fake: 0.065 \n",
            "(epoch: 104, iters: 1200, time: 0.193) G_GAN: 1.788 G_GAN_Feat: 3.851 G_VGG: 1.346 D_real: 0.131 D_fake: 0.027 \n",
            "(epoch: 104, iters: 1300, time: 0.193) G_GAN: 1.541 G_GAN_Feat: 3.870 G_VGG: 1.333 D_real: 0.013 D_fake: 0.219 \n",
            "(epoch: 104, iters: 1400, time: 0.193) G_GAN: 1.069 G_GAN_Feat: 2.478 G_VGG: 1.414 D_real: 0.053 D_fake: 0.469 \n",
            "saving the latest model (epoch 104, total_steps 434000)\n",
            "(epoch: 104, iters: 1500, time: 0.193) G_GAN: 2.312 G_GAN_Feat: 3.936 G_VGG: 1.301 D_real: 0.275 D_fake: 0.035 \n",
            "(epoch: 104, iters: 1600, time: 0.193) G_GAN: 1.855 G_GAN_Feat: 3.645 G_VGG: 1.205 D_real: 0.056 D_fake: 0.017 \n",
            "(epoch: 104, iters: 1700, time: 0.193) G_GAN: 1.367 G_GAN_Feat: 3.026 G_VGG: 1.122 D_real: 0.220 D_fake: 0.120 \n",
            "(epoch: 104, iters: 1800, time: 0.193) G_GAN: 1.691 G_GAN_Feat: 3.406 G_VGG: 1.164 D_real: 0.107 D_fake: 0.064 \n",
            "(epoch: 104, iters: 1900, time: 0.193) G_GAN: 1.689 G_GAN_Feat: 3.112 G_VGG: 1.459 D_real: 0.395 D_fake: 0.087 \n",
            "(epoch: 104, iters: 2000, time: 0.193) G_GAN: 2.033 G_GAN_Feat: 3.560 G_VGG: 1.493 D_real: 0.094 D_fake: 0.012 \n",
            "(epoch: 104, iters: 2100, time: 0.193) G_GAN: 1.295 G_GAN_Feat: 2.696 G_VGG: 1.008 D_real: 0.098 D_fake: 0.254 \n",
            "(epoch: 104, iters: 2200, time: 0.193) G_GAN: 1.331 G_GAN_Feat: 3.685 G_VGG: 1.244 D_real: 0.120 D_fake: 0.200 \n",
            "(epoch: 104, iters: 2300, time: 0.193) G_GAN: 1.686 G_GAN_Feat: 3.929 G_VGG: 1.675 D_real: 0.347 D_fake: 0.105 \n",
            "(epoch: 104, iters: 2400, time: 0.193) G_GAN: 1.394 G_GAN_Feat: 4.158 G_VGG: 1.527 D_real: 0.088 D_fake: 0.212 \n",
            "saving the latest model (epoch 104, total_steps 435000)\n",
            "(epoch: 104, iters: 2500, time: 0.193) G_GAN: 2.126 G_GAN_Feat: 4.004 G_VGG: 1.752 D_real: 0.338 D_fake: 0.029 \n",
            "(epoch: 104, iters: 2600, time: 0.193) G_GAN: 1.480 G_GAN_Feat: 3.402 G_VGG: 1.343 D_real: 0.016 D_fake: 0.212 \n",
            "(epoch: 104, iters: 2700, time: 0.193) G_GAN: 1.725 G_GAN_Feat: 3.622 G_VGG: 1.192 D_real: 0.353 D_fake: 0.037 \n",
            "(epoch: 104, iters: 2800, time: 0.193) G_GAN: 1.299 G_GAN_Feat: 2.807 G_VGG: 1.517 D_real: 0.084 D_fake: 0.255 \n",
            "(epoch: 104, iters: 2900, time: 0.193) G_GAN: 0.745 G_GAN_Feat: 2.747 G_VGG: 1.019 D_real: 0.405 D_fake: 0.375 \n",
            "(epoch: 104, iters: 3000, time: 0.193) G_GAN: 2.042 G_GAN_Feat: 5.011 G_VGG: 1.871 D_real: 0.018 D_fake: 0.009 \n",
            "(epoch: 104, iters: 3100, time: 0.193) G_GAN: 1.956 G_GAN_Feat: 3.758 G_VGG: 1.839 D_real: 0.109 D_fake: 0.022 \n",
            "(epoch: 104, iters: 3200, time: 0.193) G_GAN: 1.831 G_GAN_Feat: 3.394 G_VGG: 1.532 D_real: 0.072 D_fake: 0.030 \n",
            "(epoch: 104, iters: 3300, time: 0.193) G_GAN: 1.477 G_GAN_Feat: 4.431 G_VGG: 1.558 D_real: 0.114 D_fake: 0.216 \n",
            "(epoch: 104, iters: 3400, time: 0.193) G_GAN: 1.535 G_GAN_Feat: 4.432 G_VGG: 1.544 D_real: 0.054 D_fake: 0.131 \n",
            "saving the latest model (epoch 104, total_steps 436000)\n",
            "(epoch: 104, iters: 3500, time: 0.193) G_GAN: 1.841 G_GAN_Feat: 4.086 G_VGG: 1.235 D_real: 0.098 D_fake: 0.031 \n",
            "(epoch: 104, iters: 3600, time: 0.193) G_GAN: 1.695 G_GAN_Feat: 3.649 G_VGG: 1.370 D_real: 0.156 D_fake: 0.168 \n",
            "(epoch: 104, iters: 3700, time: 0.193) G_GAN: 2.365 G_GAN_Feat: 4.636 G_VGG: 1.513 D_real: 0.327 D_fake: 0.052 \n",
            "(epoch: 104, iters: 3800, time: 0.193) G_GAN: 1.867 G_GAN_Feat: 3.595 G_VGG: 1.292 D_real: 0.062 D_fake: 0.046 \n",
            "(epoch: 104, iters: 3900, time: 0.193) G_GAN: 1.515 G_GAN_Feat: 3.402 G_VGG: 1.116 D_real: 0.064 D_fake: 0.183 \n",
            "(epoch: 104, iters: 4000, time: 0.193) G_GAN: 1.651 G_GAN_Feat: 3.295 G_VGG: 1.293 D_real: 0.092 D_fake: 0.044 \n",
            "(epoch: 104, iters: 4100, time: 0.193) G_GAN: 1.865 G_GAN_Feat: 3.394 G_VGG: 1.678 D_real: 0.091 D_fake: 0.037 \n",
            "(epoch: 104, iters: 4200, time: 0.193) G_GAN: 1.281 G_GAN_Feat: 2.590 G_VGG: 1.138 D_real: 0.147 D_fake: 0.222 \n",
            "End of epoch 104 / 200 \t Time Taken: 871 sec\n",
            "(epoch: 105, iters: 100, time: 0.193) G_GAN: 1.343 G_GAN_Feat: 3.312 G_VGG: 1.409 D_real: 0.035 D_fake: 0.235 \n",
            "(epoch: 105, iters: 200, time: 0.193) G_GAN: 1.568 G_GAN_Feat: 5.548 G_VGG: 1.539 D_real: 0.026 D_fake: 0.088 \n",
            "saving the latest model (epoch 105, total_steps 437000)\n",
            "(epoch: 105, iters: 300, time: 0.194) G_GAN: 1.992 G_GAN_Feat: 5.161 G_VGG: 1.542 D_real: 0.045 D_fake: 0.042 \n",
            "(epoch: 105, iters: 400, time: 0.193) G_GAN: 1.115 G_GAN_Feat: 3.877 G_VGG: 1.485 D_real: 0.037 D_fake: 0.338 \n",
            "(epoch: 105, iters: 500, time: 0.193) G_GAN: 1.998 G_GAN_Feat: 4.202 G_VGG: 1.587 D_real: 0.026 D_fake: 0.012 \n",
            "(epoch: 105, iters: 600, time: 0.193) G_GAN: 1.992 G_GAN_Feat: 2.760 G_VGG: 0.965 D_real: 0.474 D_fake: 0.095 \n",
            "(epoch: 105, iters: 700, time: 0.193) G_GAN: 1.591 G_GAN_Feat: 2.756 G_VGG: 1.200 D_real: 0.201 D_fake: 0.133 \n",
            "(epoch: 105, iters: 800, time: 0.193) G_GAN: 2.284 G_GAN_Feat: 5.785 G_VGG: 1.255 D_real: 0.030 D_fake: 0.030 \n",
            "(epoch: 105, iters: 900, time: 0.193) G_GAN: 1.619 G_GAN_Feat: 3.435 G_VGG: 1.431 D_real: 0.092 D_fake: 0.071 \n",
            "(epoch: 105, iters: 1000, time: 0.193) G_GAN: 1.397 G_GAN_Feat: 3.097 G_VGG: 1.206 D_real: 0.283 D_fake: 0.141 \n",
            "(epoch: 105, iters: 1100, time: 0.193) G_GAN: 1.024 G_GAN_Feat: 5.497 G_VGG: 1.580 D_real: 0.205 D_fake: 0.220 \n",
            "(epoch: 105, iters: 1200, time: 0.193) G_GAN: 1.522 G_GAN_Feat: 3.032 G_VGG: 0.957 D_real: 0.432 D_fake: 0.097 \n",
            "saving the latest model (epoch 105, total_steps 438000)\n",
            "(epoch: 105, iters: 1300, time: 0.193) G_GAN: 1.424 G_GAN_Feat: 2.569 G_VGG: 1.107 D_real: 0.158 D_fake: 0.149 \n",
            "(epoch: 105, iters: 1400, time: 0.193) G_GAN: 1.822 G_GAN_Feat: 2.519 G_VGG: 1.010 D_real: 0.387 D_fake: 0.050 \n",
            "(epoch: 105, iters: 1500, time: 0.193) G_GAN: 1.025 G_GAN_Feat: 3.111 G_VGG: 1.229 D_real: 0.046 D_fake: 0.348 \n",
            "(epoch: 105, iters: 1600, time: 0.193) G_GAN: 1.199 G_GAN_Feat: 3.523 G_VGG: 1.280 D_real: 0.025 D_fake: 0.265 \n",
            "(epoch: 105, iters: 1700, time: 0.193) G_GAN: 1.711 G_GAN_Feat: 3.762 G_VGG: 1.356 D_real: 0.095 D_fake: 0.074 \n",
            "(epoch: 105, iters: 1800, time: 0.193) G_GAN: 1.418 G_GAN_Feat: 3.514 G_VGG: 1.283 D_real: 0.039 D_fake: 0.138 \n",
            "(epoch: 105, iters: 1900, time: 0.193) G_GAN: 1.169 G_GAN_Feat: 3.025 G_VGG: 1.184 D_real: 0.154 D_fake: 0.357 \n",
            "(epoch: 105, iters: 2000, time: 0.193) G_GAN: 1.619 G_GAN_Feat: 3.062 G_VGG: 1.346 D_real: 0.224 D_fake: 0.112 \n",
            "(epoch: 105, iters: 2100, time: 0.193) G_GAN: 1.440 G_GAN_Feat: 3.658 G_VGG: 1.741 D_real: 0.149 D_fake: 0.161 \n",
            "(epoch: 105, iters: 2200, time: 0.193) G_GAN: 1.589 G_GAN_Feat: 3.099 G_VGG: 1.147 D_real: 0.046 D_fake: 0.113 \n",
            "saving the latest model (epoch 105, total_steps 439000)\n",
            "(epoch: 105, iters: 2300, time: 0.193) G_GAN: 1.147 G_GAN_Feat: 3.566 G_VGG: 1.432 D_real: 0.076 D_fake: 0.415 \n",
            "(epoch: 105, iters: 2400, time: 0.193) G_GAN: 1.085 G_GAN_Feat: 2.980 G_VGG: 1.279 D_real: 0.097 D_fake: 0.258 \n",
            "(epoch: 105, iters: 2500, time: 0.193) G_GAN: 1.349 G_GAN_Feat: 2.489 G_VGG: 1.054 D_real: 0.038 D_fake: 0.188 \n",
            "(epoch: 105, iters: 2600, time: 0.193) G_GAN: 1.363 G_GAN_Feat: 2.707 G_VGG: 1.131 D_real: 0.105 D_fake: 0.199 \n",
            "(epoch: 105, iters: 2700, time: 0.193) G_GAN: 1.916 G_GAN_Feat: 3.011 G_VGG: 1.129 D_real: 0.073 D_fake: 0.038 \n",
            "(epoch: 105, iters: 2800, time: 0.193) G_GAN: 1.716 G_GAN_Feat: 4.444 G_VGG: 1.477 D_real: 0.030 D_fake: 0.175 \n",
            "(epoch: 105, iters: 2900, time: 0.193) G_GAN: 1.790 G_GAN_Feat: 4.489 G_VGG: 1.367 D_real: 0.021 D_fake: 0.041 \n",
            "(epoch: 105, iters: 3000, time: 0.193) G_GAN: 0.622 G_GAN_Feat: 2.739 G_VGG: 1.194 D_real: 0.020 D_fake: 0.996 \n",
            "(epoch: 105, iters: 3100, time: 0.193) G_GAN: 1.354 G_GAN_Feat: 3.227 G_VGG: 1.391 D_real: 0.094 D_fake: 0.277 \n",
            "(epoch: 105, iters: 3200, time: 0.193) G_GAN: 1.543 G_GAN_Feat: 2.480 G_VGG: 1.414 D_real: 0.029 D_fake: 0.219 \n",
            "saving the latest model (epoch 105, total_steps 440000)\n",
            "(epoch: 105, iters: 3300, time: 0.194) G_GAN: 1.405 G_GAN_Feat: 3.056 G_VGG: 1.073 D_real: 0.195 D_fake: 0.170 \n",
            "(epoch: 105, iters: 3400, time: 0.193) G_GAN: 1.435 G_GAN_Feat: 3.208 G_VGG: 1.414 D_real: 0.075 D_fake: 0.458 \n",
            "(epoch: 105, iters: 3500, time: 0.193) G_GAN: 1.433 G_GAN_Feat: 3.564 G_VGG: 1.366 D_real: 0.096 D_fake: 0.120 \n",
            "(epoch: 105, iters: 3600, time: 0.193) G_GAN: 1.975 G_GAN_Feat: 4.197 G_VGG: 1.548 D_real: 0.227 D_fake: 0.068 \n",
            "(epoch: 105, iters: 3700, time: 0.193) G_GAN: 1.578 G_GAN_Feat: 3.085 G_VGG: 1.397 D_real: 0.048 D_fake: 0.249 \n",
            "(epoch: 105, iters: 3800, time: 0.193) G_GAN: 1.109 G_GAN_Feat: 3.083 G_VGG: 1.247 D_real: 0.026 D_fake: 0.422 \n",
            "(epoch: 105, iters: 3900, time: 0.193) G_GAN: 1.998 G_GAN_Feat: 4.049 G_VGG: 1.419 D_real: 0.151 D_fake: 0.143 \n",
            "(epoch: 105, iters: 4000, time: 0.193) G_GAN: 2.532 G_GAN_Feat: 3.664 G_VGG: 1.720 D_real: 0.333 D_fake: 0.061 \n",
            "(epoch: 105, iters: 4100, time: 0.193) G_GAN: 1.940 G_GAN_Feat: 4.629 G_VGG: 1.280 D_real: 0.160 D_fake: 0.037 \n",
            "(epoch: 105, iters: 4200, time: 0.193) G_GAN: 1.351 G_GAN_Feat: 4.840 G_VGG: 1.855 D_real: 0.085 D_fake: 0.348 \n",
            "saving the latest model (epoch 105, total_steps 441000)\n",
            "End of epoch 105 / 200 \t Time Taken: 885 sec\n",
            "(epoch: 106, iters: 100, time: 0.193) G_GAN: 1.861 G_GAN_Feat: 4.108 G_VGG: 1.210 D_real: 0.063 D_fake: 0.033 \n",
            "(epoch: 106, iters: 200, time: 0.193) G_GAN: 1.682 G_GAN_Feat: 2.811 G_VGG: 1.093 D_real: 0.403 D_fake: 0.041 \n",
            "(epoch: 106, iters: 300, time: 0.193) G_GAN: 1.264 G_GAN_Feat: 2.748 G_VGG: 1.238 D_real: 0.113 D_fake: 0.229 \n",
            "(epoch: 106, iters: 400, time: 0.193) G_GAN: 2.183 G_GAN_Feat: 3.256 G_VGG: 1.324 D_real: 0.215 D_fake: 0.039 \n",
            "(epoch: 106, iters: 500, time: 0.193) G_GAN: 1.037 G_GAN_Feat: 3.334 G_VGG: 1.073 D_real: 0.170 D_fake: 0.257 \n",
            "(epoch: 106, iters: 600, time: 0.193) G_GAN: 1.703 G_GAN_Feat: 4.369 G_VGG: 1.284 D_real: 0.205 D_fake: 0.091 \n",
            "(epoch: 106, iters: 700, time: 0.193) G_GAN: 1.792 G_GAN_Feat: 4.377 G_VGG: 1.844 D_real: 0.374 D_fake: 0.095 \n",
            "(epoch: 106, iters: 800, time: 0.193) G_GAN: 1.462 G_GAN_Feat: 2.988 G_VGG: 1.320 D_real: 0.123 D_fake: 0.093 \n",
            "(epoch: 106, iters: 900, time: 0.193) G_GAN: 1.767 G_GAN_Feat: 3.434 G_VGG: 1.379 D_real: 0.045 D_fake: 0.155 \n",
            "(epoch: 106, iters: 1000, time: 0.193) G_GAN: 1.618 G_GAN_Feat: 3.257 G_VGG: 0.983 D_real: 0.022 D_fake: 0.073 \n",
            "saving the latest model (epoch 106, total_steps 442000)\n",
            "(epoch: 106, iters: 1100, time: 0.193) G_GAN: 1.559 G_GAN_Feat: 3.202 G_VGG: 1.135 D_real: 0.081 D_fake: 0.173 \n",
            "(epoch: 106, iters: 1200, time: 0.193) G_GAN: 2.071 G_GAN_Feat: 3.444 G_VGG: 1.274 D_real: 0.335 D_fake: 0.073 \n",
            "(epoch: 106, iters: 1300, time: 0.193) G_GAN: 1.802 G_GAN_Feat: 2.314 G_VGG: 1.070 D_real: 0.268 D_fake: 0.061 \n",
            "(epoch: 106, iters: 1400, time: 0.193) G_GAN: 1.715 G_GAN_Feat: 4.480 G_VGG: 1.521 D_real: 0.070 D_fake: 0.065 \n",
            "(epoch: 106, iters: 1500, time: 0.193) G_GAN: 2.024 G_GAN_Feat: 4.105 G_VGG: 1.482 D_real: 0.016 D_fake: 0.014 \n",
            "(epoch: 106, iters: 1600, time: 0.193) G_GAN: 1.944 G_GAN_Feat: 4.118 G_VGG: 1.295 D_real: 0.187 D_fake: 0.023 \n",
            "(epoch: 106, iters: 1700, time: 0.193) G_GAN: 1.595 G_GAN_Feat: 3.281 G_VGG: 1.428 D_real: 0.052 D_fake: 0.123 \n",
            "(epoch: 106, iters: 1800, time: 0.193) G_GAN: 1.806 G_GAN_Feat: 3.112 G_VGG: 1.274 D_real: 0.218 D_fake: 0.051 \n",
            "(epoch: 106, iters: 1900, time: 0.193) G_GAN: 1.941 G_GAN_Feat: 4.498 G_VGG: 1.565 D_real: 0.052 D_fake: 0.014 \n",
            "(epoch: 106, iters: 2000, time: 0.193) G_GAN: 1.794 G_GAN_Feat: 3.172 G_VGG: 1.117 D_real: 0.283 D_fake: 0.114 \n",
            "saving the latest model (epoch 106, total_steps 443000)\n",
            "(epoch: 106, iters: 2100, time: 0.194) G_GAN: 1.423 G_GAN_Feat: 3.069 G_VGG: 1.021 D_real: 0.030 D_fake: 0.149 \n",
            "(epoch: 106, iters: 2200, time: 0.193) G_GAN: 1.102 G_GAN_Feat: 3.324 G_VGG: 1.309 D_real: 0.251 D_fake: 0.179 \n",
            "(epoch: 106, iters: 2300, time: 0.193) G_GAN: 1.947 G_GAN_Feat: 3.706 G_VGG: 1.375 D_real: 0.064 D_fake: 0.040 \n",
            "(epoch: 106, iters: 2400, time: 0.193) G_GAN: 1.770 G_GAN_Feat: 4.021 G_VGG: 1.332 D_real: 0.119 D_fake: 0.046 \n",
            "(epoch: 106, iters: 2500, time: 0.193) G_GAN: 2.529 G_GAN_Feat: 3.784 G_VGG: 1.103 D_real: 0.510 D_fake: 0.046 \n",
            "(epoch: 106, iters: 2600, time: 0.193) G_GAN: 1.575 G_GAN_Feat: 3.184 G_VGG: 1.226 D_real: 0.124 D_fake: 0.059 \n",
            "(epoch: 106, iters: 2700, time: 0.193) G_GAN: 1.620 G_GAN_Feat: 3.280 G_VGG: 1.508 D_real: 0.065 D_fake: 0.079 \n",
            "(epoch: 106, iters: 2800, time: 0.193) G_GAN: 1.716 G_GAN_Feat: 2.798 G_VGG: 1.508 D_real: 0.101 D_fake: 0.082 \n",
            "(epoch: 106, iters: 2900, time: 0.193) G_GAN: 1.324 G_GAN_Feat: 3.654 G_VGG: 1.305 D_real: 0.139 D_fake: 0.119 \n",
            "(epoch: 106, iters: 3000, time: 0.193) G_GAN: 2.040 G_GAN_Feat: 3.120 G_VGG: 1.058 D_real: 0.153 D_fake: 0.025 \n",
            "saving the latest model (epoch 106, total_steps 444000)\n",
            "(epoch: 106, iters: 3100, time: 0.193) G_GAN: 1.854 G_GAN_Feat: 3.257 G_VGG: 1.436 D_real: 0.230 D_fake: 0.018 \n",
            "(epoch: 106, iters: 3200, time: 0.193) G_GAN: 1.352 G_GAN_Feat: 3.222 G_VGG: 1.322 D_real: 0.029 D_fake: 0.313 \n",
            "(epoch: 106, iters: 3300, time: 0.193) G_GAN: 1.780 G_GAN_Feat: 2.622 G_VGG: 0.923 D_real: 0.068 D_fake: 0.074 \n",
            "(epoch: 106, iters: 3400, time: 0.193) G_GAN: 1.766 G_GAN_Feat: 3.542 G_VGG: 1.357 D_real: 0.075 D_fake: 0.058 \n",
            "(epoch: 106, iters: 3500, time: 0.193) G_GAN: 2.236 G_GAN_Feat: 3.443 G_VGG: 1.330 D_real: 0.099 D_fake: 0.038 \n",
            "(epoch: 106, iters: 3600, time: 0.193) G_GAN: 1.656 G_GAN_Feat: 3.720 G_VGG: 1.370 D_real: 0.018 D_fake: 0.042 \n",
            "(epoch: 106, iters: 3700, time: 0.193) G_GAN: 1.721 G_GAN_Feat: 3.830 G_VGG: 1.504 D_real: 0.085 D_fake: 0.090 \n",
            "(epoch: 106, iters: 3800, time: 0.193) G_GAN: 1.065 G_GAN_Feat: 2.774 G_VGG: 1.276 D_real: 0.037 D_fake: 0.626 \n",
            "(epoch: 106, iters: 3900, time: 0.193) G_GAN: 1.541 G_GAN_Feat: 4.091 G_VGG: 1.414 D_real: 0.015 D_fake: 0.107 \n",
            "(epoch: 106, iters: 4000, time: 0.193) G_GAN: 1.155 G_GAN_Feat: 3.575 G_VGG: 1.200 D_real: 0.100 D_fake: 0.540 \n",
            "saving the latest model (epoch 106, total_steps 445000)\n",
            "(epoch: 106, iters: 4100, time: 0.194) G_GAN: 1.556 G_GAN_Feat: 3.092 G_VGG: 1.333 D_real: 0.011 D_fake: 0.159 \n",
            "(epoch: 106, iters: 4200, time: 0.193) G_GAN: 1.526 G_GAN_Feat: 2.934 G_VGG: 0.909 D_real: 0.019 D_fake: 0.219 \n",
            "End of epoch 106 / 200 \t Time Taken: 875 sec\n",
            "(epoch: 107, iters: 100, time: 0.193) G_GAN: 2.170 G_GAN_Feat: 4.194 G_VGG: 1.422 D_real: 0.498 D_fake: 0.022 \n",
            "(epoch: 107, iters: 200, time: 0.193) G_GAN: 2.352 G_GAN_Feat: 2.980 G_VGG: 1.313 D_real: 0.122 D_fake: 0.042 \n",
            "(epoch: 107, iters: 300, time: 0.193) G_GAN: 2.083 G_GAN_Feat: 4.039 G_VGG: 1.317 D_real: 0.335 D_fake: 0.033 \n",
            "(epoch: 107, iters: 400, time: 0.193) G_GAN: 2.061 G_GAN_Feat: 3.260 G_VGG: 0.976 D_real: 0.225 D_fake: 0.048 \n",
            "(epoch: 107, iters: 500, time: 0.193) G_GAN: 1.487 G_GAN_Feat: 3.131 G_VGG: 1.329 D_real: 0.248 D_fake: 0.125 \n",
            "(epoch: 107, iters: 600, time: 0.193) G_GAN: 1.363 G_GAN_Feat: 2.421 G_VGG: 0.932 D_real: 0.024 D_fake: 0.234 \n",
            "(epoch: 107, iters: 700, time: 0.193) G_GAN: 1.872 G_GAN_Feat: 4.298 G_VGG: 1.311 D_real: 0.043 D_fake: 0.024 \n",
            "(epoch: 107, iters: 800, time: 0.193) G_GAN: 1.420 G_GAN_Feat: 3.234 G_VGG: 1.083 D_real: 0.040 D_fake: 0.160 \n",
            "saving the latest model (epoch 107, total_steps 446000)\n",
            "(epoch: 107, iters: 900, time: 0.193) G_GAN: 1.192 G_GAN_Feat: 2.550 G_VGG: 1.157 D_real: 0.119 D_fake: 0.285 \n",
            "(epoch: 107, iters: 1000, time: 0.193) G_GAN: 1.175 G_GAN_Feat: 3.595 G_VGG: 1.533 D_real: 0.030 D_fake: 0.570 \n",
            "(epoch: 107, iters: 1100, time: 0.193) G_GAN: 1.963 G_GAN_Feat: 3.487 G_VGG: 1.297 D_real: 0.016 D_fake: 0.013 \n",
            "(epoch: 107, iters: 1200, time: 0.193) G_GAN: 1.935 G_GAN_Feat: 3.066 G_VGG: 1.529 D_real: 0.180 D_fake: 0.031 \n",
            "(epoch: 107, iters: 1300, time: 0.193) G_GAN: 1.598 G_GAN_Feat: 2.300 G_VGG: 1.233 D_real: 0.160 D_fake: 0.089 \n",
            "(epoch: 107, iters: 1400, time: 0.193) G_GAN: 1.364 G_GAN_Feat: 3.260 G_VGG: 1.460 D_real: 0.079 D_fake: 0.178 \n",
            "(epoch: 107, iters: 1500, time: 0.193) G_GAN: 1.809 G_GAN_Feat: 2.507 G_VGG: 1.063 D_real: 0.220 D_fake: 0.087 \n",
            "(epoch: 107, iters: 1600, time: 0.193) G_GAN: 2.156 G_GAN_Feat: 3.001 G_VGG: 1.051 D_real: 0.115 D_fake: 0.305 \n",
            "(epoch: 107, iters: 1700, time: 0.193) G_GAN: 0.675 G_GAN_Feat: 2.748 G_VGG: 1.479 D_real: 0.172 D_fake: 0.945 \n",
            "(epoch: 107, iters: 1800, time: 0.193) G_GAN: 1.332 G_GAN_Feat: 2.794 G_VGG: 1.197 D_real: 0.039 D_fake: 0.140 \n",
            "saving the latest model (epoch 107, total_steps 447000)\n",
            "(epoch: 107, iters: 1900, time: 0.193) G_GAN: 1.673 G_GAN_Feat: 2.144 G_VGG: 1.173 D_real: 0.307 D_fake: 0.092 \n",
            "(epoch: 107, iters: 2000, time: 0.193) G_GAN: 1.395 G_GAN_Feat: 3.007 G_VGG: 0.990 D_real: 0.021 D_fake: 0.260 \n",
            "(epoch: 107, iters: 2100, time: 0.193) G_GAN: 1.569 G_GAN_Feat: 3.425 G_VGG: 1.246 D_real: 0.210 D_fake: 0.086 \n",
            "(epoch: 107, iters: 2200, time: 0.193) G_GAN: 1.460 G_GAN_Feat: 3.396 G_VGG: 1.612 D_real: 0.066 D_fake: 0.091 \n",
            "(epoch: 107, iters: 2300, time: 0.193) G_GAN: 1.608 G_GAN_Feat: 2.879 G_VGG: 1.203 D_real: 0.095 D_fake: 0.114 \n",
            "(epoch: 107, iters: 2400, time: 0.193) G_GAN: 2.141 G_GAN_Feat: 4.068 G_VGG: 0.945 D_real: 0.483 D_fake: 0.017 \n",
            "(epoch: 107, iters: 2500, time: 0.193) G_GAN: 1.641 G_GAN_Feat: 3.246 G_VGG: 1.059 D_real: 0.040 D_fake: 0.064 \n",
            "(epoch: 107, iters: 2600, time: 0.193) G_GAN: 1.264 G_GAN_Feat: 2.624 G_VGG: 1.032 D_real: 0.143 D_fake: 0.373 \n",
            "(epoch: 107, iters: 2700, time: 0.193) G_GAN: 1.076 G_GAN_Feat: 2.558 G_VGG: 0.985 D_real: 0.019 D_fake: 0.256 \n",
            "(epoch: 107, iters: 2800, time: 0.193) G_GAN: 1.765 G_GAN_Feat: 2.282 G_VGG: 0.835 D_real: 0.262 D_fake: 0.124 \n",
            "saving the latest model (epoch 107, total_steps 448000)\n",
            "(epoch: 107, iters: 2900, time: 0.193) G_GAN: 1.668 G_GAN_Feat: 3.326 G_VGG: 1.607 D_real: 0.312 D_fake: 0.052 \n",
            "(epoch: 107, iters: 3000, time: 0.193) G_GAN: 1.491 G_GAN_Feat: 2.879 G_VGG: 1.692 D_real: 0.731 D_fake: 0.090 \n",
            "(epoch: 107, iters: 3100, time: 0.193) G_GAN: 1.763 G_GAN_Feat: 3.348 G_VGG: 1.408 D_real: 0.500 D_fake: 0.034 \n",
            "(epoch: 107, iters: 3200, time: 0.193) G_GAN: 1.578 G_GAN_Feat: 3.295 G_VGG: 1.393 D_real: 0.079 D_fake: 0.243 \n",
            "(epoch: 107, iters: 3300, time: 0.193) G_GAN: 1.877 G_GAN_Feat: 3.877 G_VGG: 1.152 D_real: 0.245 D_fake: 0.040 \n",
            "(epoch: 107, iters: 3400, time: 0.193) G_GAN: 1.108 G_GAN_Feat: 3.370 G_VGG: 1.049 D_real: 0.030 D_fake: 0.521 \n",
            "(epoch: 107, iters: 3500, time: 0.193) G_GAN: 2.182 G_GAN_Feat: 3.492 G_VGG: 1.463 D_real: 0.505 D_fake: 0.025 \n",
            "(epoch: 107, iters: 3600, time: 0.193) G_GAN: 0.618 G_GAN_Feat: 2.926 G_VGG: 1.203 D_real: 0.036 D_fake: 0.624 \n",
            "(epoch: 107, iters: 3700, time: 0.193) G_GAN: 1.825 G_GAN_Feat: 4.292 G_VGG: 1.447 D_real: 0.081 D_fake: 0.322 \n",
            "(epoch: 107, iters: 3800, time: 0.193) G_GAN: 0.917 G_GAN_Feat: 2.351 G_VGG: 0.954 D_real: 0.079 D_fake: 0.316 \n",
            "saving the latest model (epoch 107, total_steps 449000)\n",
            "(epoch: 107, iters: 3900, time: 0.193) G_GAN: 1.442 G_GAN_Feat: 3.079 G_VGG: 1.016 D_real: 0.085 D_fake: 0.255 \n",
            "(epoch: 107, iters: 4000, time: 0.193) G_GAN: 1.379 G_GAN_Feat: 2.729 G_VGG: 1.139 D_real: 0.064 D_fake: 0.237 \n",
            "(epoch: 107, iters: 4100, time: 0.193) G_GAN: 1.696 G_GAN_Feat: 3.489 G_VGG: 1.578 D_real: 0.364 D_fake: 0.031 \n",
            "(epoch: 107, iters: 4200, time: 0.193) G_GAN: 1.499 G_GAN_Feat: 2.745 G_VGG: 1.216 D_real: 0.203 D_fake: 0.060 \n",
            "End of epoch 107 / 200 \t Time Taken: 874 sec\n",
            "(epoch: 108, iters: 100, time: 0.193) G_GAN: 1.036 G_GAN_Feat: 2.868 G_VGG: 1.111 D_real: 0.052 D_fake: 0.275 \n",
            "(epoch: 108, iters: 200, time: 0.193) G_GAN: 2.007 G_GAN_Feat: 2.767 G_VGG: 1.187 D_real: 0.423 D_fake: 0.018 \n",
            "(epoch: 108, iters: 300, time: 0.193) G_GAN: 0.905 G_GAN_Feat: 2.306 G_VGG: 1.155 D_real: 0.122 D_fake: 0.330 \n",
            "(epoch: 108, iters: 400, time: 0.193) G_GAN: 1.598 G_GAN_Feat: 3.456 G_VGG: 2.329 D_real: 0.704 D_fake: 0.092 \n",
            "(epoch: 108, iters: 500, time: 0.193) G_GAN: 1.178 G_GAN_Feat: 2.719 G_VGG: 1.319 D_real: 0.047 D_fake: 0.158 \n",
            "(epoch: 108, iters: 600, time: 0.193) G_GAN: 1.673 G_GAN_Feat: 3.257 G_VGG: 1.283 D_real: 0.057 D_fake: 0.090 \n",
            "saving the latest model (epoch 108, total_steps 450000)\n",
            "(epoch: 108, iters: 700, time: 0.193) G_GAN: 1.406 G_GAN_Feat: 4.442 G_VGG: 0.852 D_real: 0.015 D_fake: 0.149 \n",
            "(epoch: 108, iters: 800, time: 0.193) G_GAN: 1.523 G_GAN_Feat: 2.930 G_VGG: 0.996 D_real: 0.113 D_fake: 0.107 \n",
            "(epoch: 108, iters: 900, time: 0.193) G_GAN: 2.191 G_GAN_Feat: 3.939 G_VGG: 1.319 D_real: 0.146 D_fake: 0.015 \n",
            "(epoch: 108, iters: 1000, time: 0.193) G_GAN: 1.871 G_GAN_Feat: 3.327 G_VGG: 1.229 D_real: 0.045 D_fake: 0.069 \n",
            "(epoch: 108, iters: 1100, time: 0.193) G_GAN: 1.586 G_GAN_Feat: 3.727 G_VGG: 1.021 D_real: 0.018 D_fake: 0.076 \n",
            "(epoch: 108, iters: 1200, time: 0.193) G_GAN: 1.221 G_GAN_Feat: 2.719 G_VGG: 1.012 D_real: 0.048 D_fake: 0.251 \n",
            "(epoch: 108, iters: 1300, time: 0.193) G_GAN: 1.422 G_GAN_Feat: 2.918 G_VGG: 1.334 D_real: 0.018 D_fake: 0.226 \n",
            "(epoch: 108, iters: 1400, time: 0.193) G_GAN: 1.231 G_GAN_Feat: 2.316 G_VGG: 1.021 D_real: 0.028 D_fake: 0.485 \n",
            "(epoch: 108, iters: 1500, time: 0.193) G_GAN: 2.398 G_GAN_Feat: 3.162 G_VGG: 1.378 D_real: 0.854 D_fake: 0.076 \n",
            "(epoch: 108, iters: 1600, time: 0.193) G_GAN: 1.479 G_GAN_Feat: 3.691 G_VGG: 0.991 D_real: 0.040 D_fake: 0.228 \n",
            "saving the latest model (epoch 108, total_steps 451000)\n",
            "(epoch: 108, iters: 1700, time: 0.193) G_GAN: 1.553 G_GAN_Feat: 2.844 G_VGG: 0.947 D_real: 0.031 D_fake: 0.128 \n",
            "(epoch: 108, iters: 1800, time: 0.193) G_GAN: 1.615 G_GAN_Feat: 2.395 G_VGG: 1.104 D_real: 0.129 D_fake: 0.124 \n",
            "(epoch: 108, iters: 1900, time: 0.193) G_GAN: 1.302 G_GAN_Feat: 3.299 G_VGG: 1.235 D_real: 0.657 D_fake: 0.200 \n",
            "(epoch: 108, iters: 2000, time: 0.193) G_GAN: 2.059 G_GAN_Feat: 4.055 G_VGG: 1.116 D_real: 0.058 D_fake: 0.041 \n",
            "(epoch: 108, iters: 2100, time: 0.193) G_GAN: 1.704 G_GAN_Feat: 2.694 G_VGG: 1.249 D_real: 0.208 D_fake: 0.044 \n",
            "(epoch: 108, iters: 2200, time: 0.193) G_GAN: 1.380 G_GAN_Feat: 2.858 G_VGG: 1.246 D_real: 0.088 D_fake: 0.267 \n",
            "(epoch: 108, iters: 2300, time: 0.193) G_GAN: 1.920 G_GAN_Feat: 3.520 G_VGG: 1.236 D_real: 0.074 D_fake: 0.050 \n",
            "(epoch: 108, iters: 2400, time: 0.193) G_GAN: 1.565 G_GAN_Feat: 3.239 G_VGG: 1.368 D_real: 0.145 D_fake: 0.071 \n",
            "(epoch: 108, iters: 2500, time: 0.193) G_GAN: 2.118 G_GAN_Feat: 3.767 G_VGG: 1.510 D_real: 0.114 D_fake: 0.072 \n",
            "(epoch: 108, iters: 2600, time: 0.193) G_GAN: 2.077 G_GAN_Feat: 3.570 G_VGG: 1.149 D_real: 0.069 D_fake: 0.036 \n",
            "saving the latest model (epoch 108, total_steps 452000)\n",
            "(epoch: 108, iters: 2700, time: 0.193) G_GAN: 1.757 G_GAN_Feat: 4.152 G_VGG: 1.423 D_real: 0.051 D_fake: 0.074 \n",
            "(epoch: 108, iters: 2800, time: 0.193) G_GAN: 2.098 G_GAN_Feat: 2.577 G_VGG: 1.112 D_real: 0.293 D_fake: 0.035 \n",
            "(epoch: 108, iters: 2900, time: 0.193) G_GAN: 1.954 G_GAN_Feat: 3.622 G_VGG: 1.456 D_real: 0.071 D_fake: 0.014 \n",
            "(epoch: 108, iters: 3000, time: 0.193) G_GAN: 1.846 G_GAN_Feat: 3.308 G_VGG: 1.046 D_real: 0.224 D_fake: 0.029 \n",
            "(epoch: 108, iters: 3100, time: 0.193) G_GAN: 1.424 G_GAN_Feat: 2.739 G_VGG: 1.185 D_real: 0.058 D_fake: 0.189 \n",
            "(epoch: 108, iters: 3200, time: 0.193) G_GAN: 1.896 G_GAN_Feat: 2.743 G_VGG: 1.056 D_real: 0.476 D_fake: 0.018 \n",
            "(epoch: 108, iters: 3300, time: 0.193) G_GAN: 1.595 G_GAN_Feat: 3.785 G_VGG: 1.062 D_real: 0.097 D_fake: 0.173 \n",
            "(epoch: 108, iters: 3400, time: 0.193) G_GAN: 1.853 G_GAN_Feat: 2.436 G_VGG: 1.105 D_real: 0.158 D_fake: 0.027 \n",
            "(epoch: 108, iters: 3500, time: 0.193) G_GAN: 1.460 G_GAN_Feat: 4.029 G_VGG: 1.312 D_real: 0.018 D_fake: 0.148 \n",
            "(epoch: 108, iters: 3600, time: 0.193) G_GAN: 1.859 G_GAN_Feat: 3.107 G_VGG: 0.939 D_real: 0.300 D_fake: 0.176 \n",
            "saving the latest model (epoch 108, total_steps 453000)\n",
            "(epoch: 108, iters: 3700, time: 0.193) G_GAN: 1.572 G_GAN_Feat: 2.399 G_VGG: 1.107 D_real: 0.116 D_fake: 0.077 \n",
            "(epoch: 108, iters: 3800, time: 0.193) G_GAN: 1.217 G_GAN_Feat: 2.565 G_VGG: 1.074 D_real: 0.025 D_fake: 0.330 \n",
            "(epoch: 108, iters: 3900, time: 0.193) G_GAN: 1.788 G_GAN_Feat: 3.439 G_VGG: 1.317 D_real: 0.180 D_fake: 0.072 \n",
            "(epoch: 108, iters: 4000, time: 0.193) G_GAN: 2.177 G_GAN_Feat: 3.312 G_VGG: 1.058 D_real: 0.091 D_fake: 0.021 \n",
            "(epoch: 108, iters: 4100, time: 0.193) G_GAN: 1.523 G_GAN_Feat: 2.208 G_VGG: 1.088 D_real: 0.203 D_fake: 0.142 \n",
            "(epoch: 108, iters: 4200, time: 0.193) G_GAN: 1.929 G_GAN_Feat: 3.017 G_VGG: 1.143 D_real: 0.090 D_fake: 0.023 \n",
            "End of epoch 108 / 200 \t Time Taken: 878 sec\n",
            "(epoch: 109, iters: 100, time: 0.193) G_GAN: 1.281 G_GAN_Feat: 2.425 G_VGG: 1.092 D_real: 0.259 D_fake: 0.334 \n",
            "(epoch: 109, iters: 200, time: 0.193) G_GAN: 1.391 G_GAN_Feat: 3.120 G_VGG: 0.902 D_real: 0.038 D_fake: 0.218 \n",
            "(epoch: 109, iters: 300, time: 0.193) G_GAN: 2.020 G_GAN_Feat: 4.608 G_VGG: 1.590 D_real: 0.042 D_fake: 0.031 \n",
            "(epoch: 109, iters: 400, time: 0.193) G_GAN: 1.975 G_GAN_Feat: 2.669 G_VGG: 1.225 D_real: 0.671 D_fake: 0.052 \n",
            "saving the latest model (epoch 109, total_steps 454000)\n",
            "(epoch: 109, iters: 500, time: 0.193) G_GAN: 1.734 G_GAN_Feat: 2.129 G_VGG: 1.271 D_real: 0.368 D_fake: 0.212 \n",
            "(epoch: 109, iters: 600, time: 0.193) G_GAN: 1.000 G_GAN_Feat: 2.886 G_VGG: 1.419 D_real: 0.042 D_fake: 0.779 \n",
            "(epoch: 109, iters: 700, time: 0.193) G_GAN: 1.617 G_GAN_Feat: 2.717 G_VGG: 1.307 D_real: 0.433 D_fake: 0.091 \n",
            "(epoch: 109, iters: 800, time: 0.193) G_GAN: 2.212 G_GAN_Feat: 3.682 G_VGG: 1.582 D_real: 0.023 D_fake: 0.025 \n",
            "(epoch: 109, iters: 900, time: 0.193) G_GAN: 1.548 G_GAN_Feat: 2.547 G_VGG: 1.127 D_real: 0.076 D_fake: 0.124 \n",
            "(epoch: 109, iters: 1000, time: 0.193) G_GAN: 2.009 G_GAN_Feat: 2.697 G_VGG: 1.127 D_real: 0.464 D_fake: 0.204 \n",
            "(epoch: 109, iters: 1100, time: 0.193) G_GAN: 1.669 G_GAN_Feat: 3.396 G_VGG: 1.207 D_real: 0.046 D_fake: 0.075 \n",
            "(epoch: 109, iters: 1200, time: 0.193) G_GAN: 1.440 G_GAN_Feat: 3.189 G_VGG: 1.184 D_real: 0.020 D_fake: 0.100 \n",
            "(epoch: 109, iters: 1300, time: 0.193) G_GAN: 2.036 G_GAN_Feat: 3.736 G_VGG: 1.194 D_real: 0.058 D_fake: 0.015 \n",
            "(epoch: 109, iters: 1400, time: 0.193) G_GAN: 1.415 G_GAN_Feat: 4.489 G_VGG: 1.568 D_real: 0.067 D_fake: 0.282 \n",
            "saving the latest model (epoch 109, total_steps 455000)\n",
            "(epoch: 109, iters: 1500, time: 0.193) G_GAN: 1.302 G_GAN_Feat: 3.583 G_VGG: 1.400 D_real: 0.122 D_fake: 0.138 \n",
            "(epoch: 109, iters: 1600, time: 0.193) G_GAN: 1.690 G_GAN_Feat: 2.896 G_VGG: 1.479 D_real: 0.139 D_fake: 0.060 \n",
            "(epoch: 109, iters: 1700, time: 0.193) G_GAN: 1.445 G_GAN_Feat: 3.441 G_VGG: 1.148 D_real: 0.131 D_fake: 0.137 \n",
            "(epoch: 109, iters: 1800, time: 0.193) G_GAN: 1.415 G_GAN_Feat: 2.589 G_VGG: 1.275 D_real: 0.133 D_fake: 0.142 \n",
            "(epoch: 109, iters: 1900, time: 0.193) G_GAN: 1.309 G_GAN_Feat: 2.559 G_VGG: 0.871 D_real: 0.061 D_fake: 0.265 \n",
            "(epoch: 109, iters: 2000, time: 0.193) G_GAN: 1.715 G_GAN_Feat: 3.737 G_VGG: 1.259 D_real: 0.133 D_fake: 0.028 \n",
            "(epoch: 109, iters: 2100, time: 0.193) G_GAN: 1.446 G_GAN_Feat: 3.069 G_VGG: 1.228 D_real: 0.161 D_fake: 0.148 \n",
            "(epoch: 109, iters: 2200, time: 0.193) G_GAN: 1.693 G_GAN_Feat: 3.170 G_VGG: 1.343 D_real: 0.302 D_fake: 0.043 \n",
            "(epoch: 109, iters: 2300, time: 0.193) G_GAN: 1.472 G_GAN_Feat: 3.834 G_VGG: 1.066 D_real: 0.136 D_fake: 0.208 \n",
            "(epoch: 109, iters: 2400, time: 0.193) G_GAN: 2.040 G_GAN_Feat: 3.206 G_VGG: 1.147 D_real: 0.239 D_fake: 0.032 \n",
            "saving the latest model (epoch 109, total_steps 456000)\n",
            "(epoch: 109, iters: 2500, time: 0.193) G_GAN: 1.104 G_GAN_Feat: 2.666 G_VGG: 1.211 D_real: 0.025 D_fake: 0.787 \n",
            "(epoch: 109, iters: 2600, time: 0.193) G_GAN: 1.906 G_GAN_Feat: 3.263 G_VGG: 1.420 D_real: 0.221 D_fake: 0.029 \n",
            "(epoch: 109, iters: 2700, time: 0.193) G_GAN: 1.843 G_GAN_Feat: 2.428 G_VGG: 1.119 D_real: 0.206 D_fake: 0.116 \n",
            "(epoch: 109, iters: 2800, time: 0.193) G_GAN: 2.286 G_GAN_Feat: 3.020 G_VGG: 1.294 D_real: 0.172 D_fake: 0.034 \n",
            "(epoch: 109, iters: 2900, time: 0.193) G_GAN: 1.672 G_GAN_Feat: 3.232 G_VGG: 1.154 D_real: 0.069 D_fake: 0.069 \n",
            "(epoch: 109, iters: 3000, time: 0.193) G_GAN: 1.353 G_GAN_Feat: 2.710 G_VGG: 1.143 D_real: 0.053 D_fake: 0.182 \n",
            "(epoch: 109, iters: 3100, time: 0.193) G_GAN: 1.003 G_GAN_Feat: 3.049 G_VGG: 1.225 D_real: 0.098 D_fake: 0.324 \n",
            "(epoch: 109, iters: 3200, time: 0.193) G_GAN: 1.667 G_GAN_Feat: 2.276 G_VGG: 1.049 D_real: 0.342 D_fake: 0.043 \n",
            "(epoch: 109, iters: 3300, time: 0.193) G_GAN: 1.495 G_GAN_Feat: 2.561 G_VGG: 1.033 D_real: 0.076 D_fake: 0.189 \n",
            "(epoch: 109, iters: 3400, time: 0.193) G_GAN: 1.802 G_GAN_Feat: 4.244 G_VGG: 1.498 D_real: 0.154 D_fake: 0.032 \n",
            "saving the latest model (epoch 109, total_steps 457000)\n",
            "(epoch: 109, iters: 3500, time: 0.193) G_GAN: 2.218 G_GAN_Feat: 4.148 G_VGG: 1.279 D_real: 0.109 D_fake: 0.033 \n",
            "(epoch: 109, iters: 3600, time: 0.193) G_GAN: 2.036 G_GAN_Feat: 3.530 G_VGG: 1.216 D_real: 0.017 D_fake: 0.018 \n",
            "(epoch: 109, iters: 3700, time: 0.193) G_GAN: 1.865 G_GAN_Feat: 2.726 G_VGG: 1.050 D_real: 0.129 D_fake: 0.022 \n",
            "(epoch: 109, iters: 3800, time: 0.193) G_GAN: 2.050 G_GAN_Feat: 4.156 G_VGG: 1.283 D_real: 0.166 D_fake: 0.026 \n",
            "(epoch: 109, iters: 3900, time: 0.193) G_GAN: 1.620 G_GAN_Feat: 3.610 G_VGG: 0.977 D_real: 0.217 D_fake: 0.052 \n",
            "(epoch: 109, iters: 4000, time: 0.193) G_GAN: 1.820 G_GAN_Feat: 3.093 G_VGG: 1.189 D_real: 0.498 D_fake: 0.022 \n",
            "(epoch: 109, iters: 4100, time: 0.193) G_GAN: 1.327 G_GAN_Feat: 3.019 G_VGG: 1.294 D_real: 0.235 D_fake: 0.142 \n",
            "(epoch: 109, iters: 4200, time: 0.193) G_GAN: 1.228 G_GAN_Feat: 2.801 G_VGG: 1.392 D_real: 0.092 D_fake: 0.170 \n",
            "End of epoch 109 / 200 \t Time Taken: 874 sec\n",
            "(epoch: 110, iters: 100, time: 0.193) G_GAN: 1.725 G_GAN_Feat: 2.491 G_VGG: 0.981 D_real: 0.175 D_fake: 0.079 \n",
            "(epoch: 110, iters: 200, time: 0.193) G_GAN: 1.901 G_GAN_Feat: 3.235 G_VGG: 0.845 D_real: 0.146 D_fake: 0.021 \n",
            "saving the latest model (epoch 110, total_steps 458000)\n",
            "(epoch: 110, iters: 300, time: 0.194) G_GAN: 0.223 G_GAN_Feat: 1.855 G_VGG: 0.852 D_real: 0.110 D_fake: 1.138 \n",
            "(epoch: 110, iters: 400, time: 0.193) G_GAN: 1.377 G_GAN_Feat: 2.973 G_VGG: 1.319 D_real: 0.200 D_fake: 0.132 \n",
            "(epoch: 110, iters: 500, time: 0.193) G_GAN: 2.190 G_GAN_Feat: 3.960 G_VGG: 1.176 D_real: 0.127 D_fake: 0.022 \n",
            "(epoch: 110, iters: 600, time: 0.193) G_GAN: 1.979 G_GAN_Feat: 3.636 G_VGG: 1.211 D_real: 0.096 D_fake: 0.013 \n",
            "(epoch: 110, iters: 700, time: 0.193) G_GAN: 1.326 G_GAN_Feat: 2.444 G_VGG: 1.027 D_real: 0.377 D_fake: 0.206 \n",
            "(epoch: 110, iters: 800, time: 0.193) G_GAN: 0.942 G_GAN_Feat: 2.227 G_VGG: 0.877 D_real: 0.029 D_fake: 0.694 \n",
            "(epoch: 110, iters: 900, time: 0.193) G_GAN: 2.188 G_GAN_Feat: 2.854 G_VGG: 0.919 D_real: 0.066 D_fake: 0.018 \n",
            "(epoch: 110, iters: 1000, time: 0.193) G_GAN: 2.118 G_GAN_Feat: 2.873 G_VGG: 0.961 D_real: 0.173 D_fake: 0.020 \n",
            "(epoch: 110, iters: 1100, time: 0.193) G_GAN: 1.312 G_GAN_Feat: 3.209 G_VGG: 1.207 D_real: 0.042 D_fake: 0.199 \n",
            "(epoch: 110, iters: 1200, time: 0.193) G_GAN: 1.469 G_GAN_Feat: 3.407 G_VGG: 1.104 D_real: 0.046 D_fake: 0.176 \n",
            "saving the latest model (epoch 110, total_steps 459000)\n",
            "(epoch: 110, iters: 1300, time: 0.193) G_GAN: 1.730 G_GAN_Feat: 3.009 G_VGG: 0.955 D_real: 0.158 D_fake: 0.073 \n",
            "(epoch: 110, iters: 1400, time: 0.193) G_GAN: 1.842 G_GAN_Feat: 5.440 G_VGG: 1.412 D_real: 0.018 D_fake: 0.026 \n",
            "(epoch: 110, iters: 1500, time: 0.193) G_GAN: 2.006 G_GAN_Feat: 2.996 G_VGG: 0.878 D_real: 0.108 D_fake: 0.012 \n",
            "(epoch: 110, iters: 1600, time: 0.193) G_GAN: 0.607 G_GAN_Feat: 2.076 G_VGG: 1.060 D_real: 0.174 D_fake: 1.014 \n",
            "(epoch: 110, iters: 1700, time: 0.193) G_GAN: 1.470 G_GAN_Feat: 3.274 G_VGG: 1.282 D_real: 0.092 D_fake: 0.091 \n",
            "(epoch: 110, iters: 1800, time: 0.193) G_GAN: 1.774 G_GAN_Feat: 3.326 G_VGG: 1.345 D_real: 0.141 D_fake: 0.111 \n",
            "(epoch: 110, iters: 1900, time: 0.193) G_GAN: 1.802 G_GAN_Feat: 3.120 G_VGG: 1.106 D_real: 0.241 D_fake: 0.029 \n",
            "(epoch: 110, iters: 2000, time: 0.193) G_GAN: 1.630 G_GAN_Feat: 3.497 G_VGG: 1.264 D_real: 0.044 D_fake: 0.084 \n",
            "(epoch: 110, iters: 2100, time: 0.193) G_GAN: 1.583 G_GAN_Feat: 3.259 G_VGG: 1.076 D_real: 0.025 D_fake: 0.173 \n",
            "(epoch: 110, iters: 2200, time: 0.193) G_GAN: 1.544 G_GAN_Feat: 2.991 G_VGG: 1.080 D_real: 0.467 D_fake: 0.083 \n",
            "saving the latest model (epoch 110, total_steps 460000)\n",
            "(epoch: 110, iters: 2300, time: 0.193) G_GAN: 1.230 G_GAN_Feat: 2.652 G_VGG: 1.145 D_real: 0.052 D_fake: 0.219 \n",
            "(epoch: 110, iters: 2400, time: 0.193) G_GAN: 1.510 G_GAN_Feat: 4.590 G_VGG: 1.204 D_real: 0.090 D_fake: 0.195 \n",
            "(epoch: 110, iters: 2500, time: 0.193) G_GAN: 1.403 G_GAN_Feat: 2.576 G_VGG: 0.925 D_real: 0.051 D_fake: 0.272 \n",
            "(epoch: 110, iters: 2600, time: 0.193) G_GAN: 1.741 G_GAN_Feat: 4.192 G_VGG: 1.478 D_real: 0.075 D_fake: 0.028 \n",
            "(epoch: 110, iters: 2700, time: 0.193) G_GAN: 1.925 G_GAN_Feat: 5.206 G_VGG: 1.399 D_real: 0.030 D_fake: 0.014 \n",
            "(epoch: 110, iters: 2800, time: 0.193) G_GAN: 2.103 G_GAN_Feat: 3.607 G_VGG: 1.134 D_real: 0.033 D_fake: 0.025 \n",
            "(epoch: 110, iters: 2900, time: 0.193) G_GAN: 1.899 G_GAN_Feat: 3.542 G_VGG: 1.147 D_real: 0.262 D_fake: 0.210 \n",
            "(epoch: 110, iters: 3000, time: 0.193) G_GAN: 0.853 G_GAN_Feat: 2.154 G_VGG: 1.023 D_real: 0.069 D_fake: 0.562 \n",
            "(epoch: 110, iters: 3100, time: 0.193) G_GAN: 1.680 G_GAN_Feat: 4.464 G_VGG: 1.209 D_real: 0.370 D_fake: 0.052 \n",
            "(epoch: 110, iters: 3200, time: 0.193) G_GAN: 2.211 G_GAN_Feat: 4.066 G_VGG: 1.046 D_real: 0.070 D_fake: 0.056 \n",
            "saving the latest model (epoch 110, total_steps 461000)\n",
            "(epoch: 110, iters: 3300, time: 0.193) G_GAN: 2.499 G_GAN_Feat: 4.601 G_VGG: 1.220 D_real: 0.121 D_fake: 0.047 \n",
            "(epoch: 110, iters: 3400, time: 0.193) G_GAN: 2.538 G_GAN_Feat: 3.267 G_VGG: 1.150 D_real: 0.197 D_fake: 0.054 \n",
            "(epoch: 110, iters: 3500, time: 0.193) G_GAN: 1.720 G_GAN_Feat: 3.072 G_VGG: 1.499 D_real: 0.034 D_fake: 0.113 \n",
            "(epoch: 110, iters: 3600, time: 0.193) G_GAN: 1.588 G_GAN_Feat: 2.894 G_VGG: 1.027 D_real: 0.066 D_fake: 0.139 \n",
            "(epoch: 110, iters: 3700, time: 0.193) G_GAN: 0.886 G_GAN_Feat: 2.259 G_VGG: 0.983 D_real: 0.021 D_fake: 0.520 \n",
            "(epoch: 110, iters: 3800, time: 0.193) G_GAN: 1.733 G_GAN_Feat: 4.263 G_VGG: 1.146 D_real: 0.573 D_fake: 0.163 \n",
            "(epoch: 110, iters: 3900, time: 0.193) G_GAN: 2.141 G_GAN_Feat: 4.098 G_VGG: 1.329 D_real: 0.160 D_fake: 0.020 \n",
            "(epoch: 110, iters: 4000, time: 0.193) G_GAN: 1.757 G_GAN_Feat: 3.572 G_VGG: 1.044 D_real: 0.174 D_fake: 0.048 \n",
            "(epoch: 110, iters: 4100, time: 0.193) G_GAN: 1.267 G_GAN_Feat: 2.915 G_VGG: 1.034 D_real: 0.145 D_fake: 0.232 \n",
            "(epoch: 110, iters: 4200, time: 0.193) G_GAN: 1.716 G_GAN_Feat: 3.331 G_VGG: 1.173 D_real: 0.208 D_fake: 0.091 \n",
            "saving the latest model (epoch 110, total_steps 462000)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 441, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 668, in _save\n",
            "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
            "RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/36: file write failed\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/train.py\", line 117, in <module>\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/pix2pixHD_model.py\", line 274, in save\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/base_model.py\", line 45, in save_network\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 440, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 291, in __exit__\n",
            "    self.file_like.write_end_of_file()\n",
            "RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 1964315264 vs 1964315152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 GANS for each fog level"
      ],
      "metadata": {
        "id": "JlZUpp-N9Yni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia_low --dataroot ./datasets/nebbia_low --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "zhRBwxm3Cxxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1ba6eb-6eb5-4f18-f47b-649f05450d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia_low\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_low\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 46 at iteration 2000\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:07<00:00, 79.7MB/s]\n",
            "create web directory ./checkpoints/nebbia_low/web...\n",
            "(epoch: 46, iters: 2100, time: 2.033) G_GAN: 1.831 G_GAN_Feat: 3.535 G_VGG: 1.215 D_real: 0.078 D_fake: 0.020 \n",
            "(epoch: 46, iters: 2200, time: 0.541) G_GAN: 1.994 G_GAN_Feat: 3.654 G_VGG: 1.360 D_real: 0.010 D_fake: 0.010 \n",
            "(epoch: 46, iters: 2300, time: 0.565) G_GAN: 1.787 G_GAN_Feat: 3.365 G_VGG: 1.581 D_real: 0.013 D_fake: 0.171 \n",
            "(epoch: 46, iters: 2400, time: 0.565) G_GAN: 1.894 G_GAN_Feat: 3.882 G_VGG: 1.889 D_real: 0.013 D_fake: 0.010 \n",
            "(epoch: 46, iters: 2500, time: 0.566) G_GAN: 1.832 G_GAN_Feat: 3.075 G_VGG: 1.415 D_real: 0.004 D_fake: 0.076 \n",
            "(epoch: 46, iters: 2600, time: 0.566) G_GAN: 2.027 G_GAN_Feat: 3.414 G_VGG: 1.470 D_real: 0.034 D_fake: 0.016 \n",
            "(epoch: 46, iters: 2700, time: 0.566) G_GAN: 2.481 G_GAN_Feat: 3.493 G_VGG: 1.408 D_real: 0.052 D_fake: 0.038 \n",
            "(epoch: 46, iters: 2800, time: 0.564) G_GAN: 1.439 G_GAN_Feat: 3.772 G_VGG: 1.472 D_real: 0.039 D_fake: 0.077 \n",
            "(epoch: 46, iters: 2900, time: 0.565) G_GAN: 2.277 G_GAN_Feat: 4.214 G_VGG: 1.679 D_real: 0.018 D_fake: 0.016 \n",
            "(epoch: 46, iters: 3000, time: 0.564) G_GAN: 1.819 G_GAN_Feat: 3.599 G_VGG: 2.208 D_real: 0.030 D_fake: 0.019 \n",
            "saving the latest model (epoch 46, total_steps 192000)\n",
            "(epoch: 46, iters: 3100, time: 0.567) G_GAN: 1.911 G_GAN_Feat: 3.672 G_VGG: 1.599 D_real: 0.013 D_fake: 0.011 \n",
            "(epoch: 46, iters: 3200, time: 0.564) G_GAN: 2.082 G_GAN_Feat: 4.140 G_VGG: 1.608 D_real: 0.007 D_fake: 0.004 \n",
            "(epoch: 46, iters: 3300, time: 0.566) G_GAN: 2.045 G_GAN_Feat: 3.007 G_VGG: 1.459 D_real: 0.017 D_fake: 0.006 \n",
            "(epoch: 46, iters: 3400, time: 0.566) G_GAN: 1.902 G_GAN_Feat: 3.477 G_VGG: 1.229 D_real: 0.036 D_fake: 0.029 \n",
            "(epoch: 46, iters: 3500, time: 0.566) G_GAN: 1.923 G_GAN_Feat: 3.444 G_VGG: 1.594 D_real: 0.068 D_fake: 0.051 \n",
            "(epoch: 46, iters: 3600, time: 0.566) G_GAN: 2.039 G_GAN_Feat: 3.629 G_VGG: 1.929 D_real: 0.038 D_fake: 0.007 \n",
            "(epoch: 46, iters: 3700, time: 0.566) G_GAN: 1.995 G_GAN_Feat: 3.599 G_VGG: 1.336 D_real: 0.004 D_fake: 0.003 \n",
            "(epoch: 46, iters: 3800, time: 0.566) G_GAN: 1.965 G_GAN_Feat: 3.367 G_VGG: 1.607 D_real: 0.010 D_fake: 0.006 \n",
            "(epoch: 46, iters: 3900, time: 0.566) G_GAN: 1.855 G_GAN_Feat: 3.266 G_VGG: 1.643 D_real: 0.008 D_fake: 0.019 \n",
            "(epoch: 46, iters: 4000, time: 0.566) G_GAN: 1.732 G_GAN_Feat: 3.909 G_VGG: 1.314 D_real: 0.028 D_fake: 0.024 \n",
            "saving the latest model (epoch 46, total_steps 193000)\n",
            "(epoch: 46, iters: 4100, time: 0.567) G_GAN: 2.145 G_GAN_Feat: 3.465 G_VGG: 1.233 D_real: 0.026 D_fake: 0.016 \n",
            "(epoch: 46, iters: 4200, time: 0.564) G_GAN: 2.077 G_GAN_Feat: 4.345 G_VGG: 1.768 D_real: 0.024 D_fake: 0.006 \n",
            "End of epoch 46 / 200 \t Time Taken: 1411 sec\n",
            "(epoch: 47, iters: 100, time: 0.567) G_GAN: 1.318 G_GAN_Feat: 3.398 G_VGG: 1.403 D_real: 0.037 D_fake: 0.175 \n",
            "(epoch: 47, iters: 200, time: 0.564) G_GAN: 1.906 G_GAN_Feat: 3.411 G_VGG: 1.754 D_real: 0.056 D_fake: 0.014 \n",
            "(epoch: 47, iters: 300, time: 0.566) G_GAN: 1.671 G_GAN_Feat: 3.748 G_VGG: 1.622 D_real: 0.024 D_fake: 0.039 \n",
            "(epoch: 47, iters: 400, time: 0.566) G_GAN: 1.940 G_GAN_Feat: 3.126 G_VGG: 1.432 D_real: 0.020 D_fake: 0.032 \n",
            "(epoch: 47, iters: 500, time: 0.566) G_GAN: 1.766 G_GAN_Feat: 3.586 G_VGG: 1.408 D_real: 0.105 D_fake: 0.033 \n",
            "(epoch: 47, iters: 600, time: 0.566) G_GAN: 2.179 G_GAN_Feat: 4.642 G_VGG: 1.503 D_real: 0.031 D_fake: 0.023 \n",
            "(epoch: 47, iters: 700, time: 0.564) G_GAN: 1.881 G_GAN_Feat: 3.317 G_VGG: 1.191 D_real: 0.021 D_fake: 0.015 \n",
            "(epoch: 47, iters: 800, time: 0.565) G_GAN: 2.079 G_GAN_Feat: 3.483 G_VGG: 1.387 D_real: 0.011 D_fake: 0.004 \n",
            "saving the latest model (epoch 47, total_steps 194000)\n",
            "(epoch: 47, iters: 900, time: 0.567) G_GAN: 2.089 G_GAN_Feat: 3.598 G_VGG: 2.025 D_real: 0.017 D_fake: 0.014 \n",
            "(epoch: 47, iters: 1000, time: 0.563) G_GAN: 1.952 G_GAN_Feat: 4.098 G_VGG: 1.383 D_real: 0.011 D_fake: 0.014 \n",
            "(epoch: 47, iters: 1100, time: 0.565) G_GAN: 2.044 G_GAN_Feat: 3.484 G_VGG: 1.292 D_real: 0.004 D_fake: 0.004 \n",
            "(epoch: 47, iters: 1200, time: 0.566) G_GAN: 2.024 G_GAN_Feat: 3.588 G_VGG: 1.479 D_real: 0.008 D_fake: 0.003 \n",
            "(epoch: 47, iters: 1300, time: 0.566) G_GAN: 2.008 G_GAN_Feat: 3.376 G_VGG: 1.562 D_real: 0.010 D_fake: 0.009 \n",
            "(epoch: 47, iters: 1400, time: 0.566) G_GAN: 2.220 G_GAN_Feat: 3.720 G_VGG: 1.556 D_real: 0.260 D_fake: 0.021 \n",
            "(epoch: 47, iters: 1500, time: 0.566) G_GAN: 2.251 G_GAN_Feat: 3.569 G_VGG: 1.620 D_real: 0.057 D_fake: 0.040 \n",
            "(epoch: 47, iters: 1600, time: 0.566) G_GAN: 2.033 G_GAN_Feat: 3.678 G_VGG: 1.441 D_real: 0.006 D_fake: 0.004 \n",
            "(epoch: 47, iters: 1700, time: 0.567) G_GAN: 1.978 G_GAN_Feat: 3.644 G_VGG: 1.290 D_real: 0.044 D_fake: 0.017 \n",
            "(epoch: 47, iters: 1800, time: 0.565) G_GAN: 1.577 G_GAN_Feat: 3.933 G_VGG: 1.908 D_real: 0.021 D_fake: 0.109 \n",
            "saving the latest model (epoch 47, total_steps 195000)\n",
            "(epoch: 47, iters: 1900, time: 0.567) G_GAN: 1.786 G_GAN_Feat: 3.150 G_VGG: 1.658 D_real: 0.022 D_fake: 0.036 \n",
            "(epoch: 47, iters: 2000, time: 0.564) G_GAN: 1.946 G_GAN_Feat: 3.285 G_VGG: 1.282 D_real: 0.008 D_fake: 0.009 \n",
            "(epoch: 47, iters: 2100, time: 0.566) G_GAN: 1.865 G_GAN_Feat: 3.526 G_VGG: 1.308 D_real: 0.012 D_fake: 0.019 \n",
            "(epoch: 47, iters: 2200, time: 0.564) G_GAN: 2.029 G_GAN_Feat: 3.107 G_VGG: 1.517 D_real: 0.007 D_fake: 0.006 \n",
            "(epoch: 47, iters: 2300, time: 0.566) G_GAN: 2.112 G_GAN_Feat: 3.596 G_VGG: 1.956 D_real: 0.012 D_fake: 0.008 \n",
            "(epoch: 47, iters: 2400, time: 0.565) G_GAN: 1.880 G_GAN_Feat: 3.284 G_VGG: 1.690 D_real: 0.111 D_fake: 0.052 \n",
            "(epoch: 47, iters: 2500, time: 0.566) G_GAN: 1.878 G_GAN_Feat: 3.615 G_VGG: 1.552 D_real: 0.016 D_fake: 0.009 \n",
            "(epoch: 47, iters: 2600, time: 0.565) G_GAN: 1.997 G_GAN_Feat: 3.495 G_VGG: 1.438 D_real: 0.040 D_fake: 0.011 \n",
            "(epoch: 47, iters: 2700, time: 0.566) G_GAN: 1.952 G_GAN_Feat: 3.405 G_VGG: 1.522 D_real: 0.035 D_fake: 0.016 \n",
            "(epoch: 47, iters: 2800, time: 0.565) G_GAN: 1.891 G_GAN_Feat: 4.202 G_VGG: 2.159 D_real: 0.048 D_fake: 0.009 \n",
            "saving the latest model (epoch 47, total_steps 196000)\n",
            "(epoch: 47, iters: 2900, time: 0.566) G_GAN: 2.017 G_GAN_Feat: 3.775 G_VGG: 2.439 D_real: 0.009 D_fake: 0.007 \n",
            "(epoch: 47, iters: 3000, time: 0.564) G_GAN: 2.264 G_GAN_Feat: 3.276 G_VGG: 1.598 D_real: 0.042 D_fake: 0.015 \n",
            "(epoch: 47, iters: 3100, time: 0.565) G_GAN: 1.886 G_GAN_Feat: 3.744 G_VGG: 1.556 D_real: 0.014 D_fake: 0.010 \n",
            "(epoch: 47, iters: 3200, time: 0.566) G_GAN: 2.026 G_GAN_Feat: 3.631 G_VGG: 1.580 D_real: 0.005 D_fake: 0.003 \n",
            "(epoch: 47, iters: 3300, time: 0.566) G_GAN: 3.737 G_GAN_Feat: 3.761 G_VGG: 1.587 D_real: 1.429 D_fake: 1.178 \n",
            "(epoch: 47, iters: 3400, time: 0.566) G_GAN: 1.929 G_GAN_Feat: 3.117 G_VGG: 1.687 D_real: 0.056 D_fake: 0.025 \n",
            "(epoch: 47, iters: 3500, time: 0.564) G_GAN: 1.500 G_GAN_Feat: 2.859 G_VGG: 1.664 D_real: 0.013 D_fake: 0.213 \n",
            "(epoch: 47, iters: 3600, time: 0.566) G_GAN: 1.831 G_GAN_Feat: 3.641 G_VGG: 1.666 D_real: 0.009 D_fake: 0.046 \n",
            "(epoch: 47, iters: 3700, time: 0.565) G_GAN: 2.114 G_GAN_Feat: 4.187 G_VGG: 1.524 D_real: 0.193 D_fake: 0.015 \n",
            "(epoch: 47, iters: 3800, time: 0.566) G_GAN: 2.007 G_GAN_Feat: 3.823 G_VGG: 1.821 D_real: 0.018 D_fake: 0.005 \n",
            "saving the latest model (epoch 47, total_steps 197000)\n",
            "(epoch: 47, iters: 3900, time: 0.566) G_GAN: 2.148 G_GAN_Feat: 3.888 G_VGG: 1.685 D_real: 0.047 D_fake: 0.012 \n",
            "(epoch: 47, iters: 4000, time: 0.564) G_GAN: 1.946 G_GAN_Feat: 3.858 G_VGG: 1.809 D_real: 0.015 D_fake: 0.006 \n",
            "(epoch: 47, iters: 4100, time: 0.567) G_GAN: 1.689 G_GAN_Feat: 3.653 G_VGG: 1.342 D_real: 0.043 D_fake: 0.036 \n",
            "(epoch: 47, iters: 4200, time: 0.564) G_GAN: 1.868 G_GAN_Feat: 3.820 G_VGG: 1.498 D_real: 0.029 D_fake: 0.046 \n",
            "End of epoch 47 / 200 \t Time Taken: 2394 sec\n",
            "(epoch: 48, iters: 100, time: 0.565) G_GAN: 1.510 G_GAN_Feat: 3.535 G_VGG: 1.507 D_real: 0.008 D_fake: 0.145 \n",
            "(epoch: 48, iters: 200, time: 0.566) G_GAN: 1.690 G_GAN_Feat: 3.136 G_VGG: 1.672 D_real: 0.016 D_fake: 0.162 \n",
            "(epoch: 48, iters: 300, time: 0.566) G_GAN: 2.064 G_GAN_Feat: 3.868 G_VGG: 1.761 D_real: 0.115 D_fake: 0.005 \n",
            "(epoch: 48, iters: 400, time: 0.566) G_GAN: 1.823 G_GAN_Feat: 3.496 G_VGG: 1.691 D_real: 0.052 D_fake: 0.028 \n",
            "(epoch: 48, iters: 500, time: 0.566) G_GAN: 1.509 G_GAN_Feat: 3.180 G_VGG: 1.397 D_real: 0.019 D_fake: 0.182 \n",
            "(epoch: 48, iters: 600, time: 0.565) G_GAN: 2.163 G_GAN_Feat: 3.608 G_VGG: 1.540 D_real: 0.006 D_fake: 0.008 \n",
            "saving the latest model (epoch 48, total_steps 198000)\n",
            "(epoch: 48, iters: 700, time: 0.566) G_GAN: 2.050 G_GAN_Feat: 3.851 G_VGG: 1.922 D_real: 0.003 D_fake: 0.003 \n",
            "(epoch: 48, iters: 800, time: 0.563) G_GAN: 1.693 G_GAN_Feat: 3.151 G_VGG: 1.331 D_real: 0.105 D_fake: 0.040 \n",
            "(epoch: 48, iters: 900, time: 0.566) G_GAN: 1.658 G_GAN_Feat: 3.238 G_VGG: 1.524 D_real: 0.021 D_fake: 0.062 \n",
            "(epoch: 48, iters: 1000, time: 0.563) G_GAN: 1.601 G_GAN_Feat: 4.518 G_VGG: 1.836 D_real: 0.026 D_fake: 0.056 \n",
            "(epoch: 48, iters: 1100, time: 0.564) G_GAN: 1.970 G_GAN_Feat: 3.087 G_VGG: 1.556 D_real: 0.004 D_fake: 0.005 \n",
            "(epoch: 48, iters: 1200, time: 0.566) G_GAN: 1.803 G_GAN_Feat: 3.915 G_VGG: 1.582 D_real: 0.011 D_fake: 0.014 \n",
            "(epoch: 48, iters: 1300, time: 0.564) G_GAN: 1.840 G_GAN_Feat: 3.434 G_VGG: 1.753 D_real: 0.007 D_fake: 0.056 \n",
            "(epoch: 48, iters: 1400, time: 0.566) G_GAN: 1.856 G_GAN_Feat: 2.937 G_VGG: 1.366 D_real: 0.014 D_fake: 0.205 \n",
            "(epoch: 48, iters: 1500, time: 0.565) G_GAN: 1.811 G_GAN_Feat: 3.178 G_VGG: 1.696 D_real: 0.013 D_fake: 0.058 \n",
            "(epoch: 48, iters: 1600, time: 0.566) G_GAN: 2.038 G_GAN_Feat: 3.379 G_VGG: 1.337 D_real: 0.047 D_fake: 0.006 \n",
            "saving the latest model (epoch 48, total_steps 199000)\n",
            "(epoch: 48, iters: 1700, time: 0.566) G_GAN: 1.967 G_GAN_Feat: 3.504 G_VGG: 1.700 D_real: 0.007 D_fake: 0.028 \n",
            "(epoch: 48, iters: 1800, time: 0.564) G_GAN: 2.133 G_GAN_Feat: 3.374 G_VGG: 1.815 D_real: 0.129 D_fake: 0.010 \n",
            "(epoch: 48, iters: 1900, time: 0.566) G_GAN: 2.008 G_GAN_Feat: 4.039 G_VGG: 1.726 D_real: 0.005 D_fake: 0.002 \n",
            "(epoch: 48, iters: 2000, time: 0.564) G_GAN: 1.909 G_GAN_Feat: 3.822 G_VGG: 1.224 D_real: 0.007 D_fake: 0.005 \n",
            "(epoch: 48, iters: 2100, time: 0.566) G_GAN: 2.104 G_GAN_Feat: 3.618 G_VGG: 1.637 D_real: 0.052 D_fake: 0.015 \n",
            "(epoch: 48, iters: 2200, time: 0.564) G_GAN: 1.886 G_GAN_Feat: 3.016 G_VGG: 1.210 D_real: 0.019 D_fake: 0.078 \n",
            "(epoch: 48, iters: 2300, time: 0.564) G_GAN: 2.094 G_GAN_Feat: 3.824 G_VGG: 1.393 D_real: 0.007 D_fake: 0.005 \n",
            "(epoch: 48, iters: 2400, time: 0.566) G_GAN: 1.771 G_GAN_Feat: 3.637 G_VGG: 1.575 D_real: 0.016 D_fake: 0.016 \n",
            "(epoch: 48, iters: 2500, time: 0.564) G_GAN: 2.121 G_GAN_Feat: 4.086 G_VGG: 1.604 D_real: 0.130 D_fake: 0.007 \n",
            "(epoch: 48, iters: 2600, time: 0.566) G_GAN: 1.843 G_GAN_Feat: 3.098 G_VGG: 1.512 D_real: 0.004 D_fake: 0.076 \n",
            "saving the latest model (epoch 48, total_steps 200000)\n",
            "(epoch: 48, iters: 2700, time: 0.566) G_GAN: 2.399 G_GAN_Feat: 3.472 G_VGG: 1.693 D_real: 0.095 D_fake: 0.033 \n",
            "(epoch: 48, iters: 2800, time: 0.566) G_GAN: 1.682 G_GAN_Feat: 3.672 G_VGG: 1.221 D_real: 0.040 D_fake: 0.038 \n",
            "(epoch: 48, iters: 2900, time: 0.564) G_GAN: 1.771 G_GAN_Feat: 3.266 G_VGG: 1.529 D_real: 0.073 D_fake: 0.030 \n",
            "(epoch: 48, iters: 3000, time: 0.565) G_GAN: 1.788 G_GAN_Feat: 3.631 G_VGG: 1.480 D_real: 0.008 D_fake: 0.016 \n",
            "(epoch: 48, iters: 3100, time: 0.565) G_GAN: 1.978 G_GAN_Feat: 3.477 G_VGG: 1.619 D_real: 0.020 D_fake: 0.005 \n",
            "(epoch: 48, iters: 3200, time: 0.567) G_GAN: 2.306 G_GAN_Feat: 3.714 G_VGG: 1.695 D_real: 0.071 D_fake: 0.019 \n",
            "(epoch: 48, iters: 3300, time: 0.565) G_GAN: 1.988 G_GAN_Feat: 3.415 G_VGG: 1.615 D_real: 0.012 D_fake: 0.026 \n",
            "(epoch: 48, iters: 3400, time: 0.565) G_GAN: 1.947 G_GAN_Feat: 3.326 G_VGG: 1.374 D_real: 0.044 D_fake: 0.004 \n",
            "(epoch: 48, iters: 3500, time: 0.566) G_GAN: 2.050 G_GAN_Feat: 3.328 G_VGG: 1.447 D_real: 0.005 D_fake: 0.005 \n",
            "(epoch: 48, iters: 3600, time: 0.565) G_GAN: 2.060 G_GAN_Feat: 4.175 G_VGG: 1.484 D_real: 0.003 D_fake: 0.003 \n",
            "saving the latest model (epoch 48, total_steps 201000)\n",
            "(epoch: 48, iters: 3700, time: 0.567) G_GAN: 2.105 G_GAN_Feat: 3.204 G_VGG: 1.622 D_real: 0.009 D_fake: 0.007 \n",
            "(epoch: 48, iters: 3800, time: 0.564) G_GAN: 1.782 G_GAN_Feat: 3.510 G_VGG: 1.549 D_real: 0.036 D_fake: 0.056 \n",
            "(epoch: 48, iters: 3900, time: 0.564) G_GAN: 1.891 G_GAN_Feat: 3.860 G_VGG: 1.592 D_real: 0.014 D_fake: 0.009 \n",
            "(epoch: 48, iters: 4000, time: 0.565) G_GAN: 1.908 G_GAN_Feat: 3.141 G_VGG: 1.567 D_real: 0.009 D_fake: 0.029 \n",
            "(epoch: 48, iters: 4100, time: 0.566) G_GAN: 2.013 G_GAN_Feat: 3.811 G_VGG: 1.627 D_real: 0.003 D_fake: 0.008 \n",
            "(epoch: 48, iters: 4200, time: 0.566) G_GAN: 1.804 G_GAN_Feat: 3.061 G_VGG: 1.385 D_real: 0.008 D_fake: 0.110 \n",
            "End of epoch 48 / 200 \t Time Taken: 2391 sec\n",
            "(epoch: 49, iters: 100, time: 0.566) G_GAN: 2.053 G_GAN_Feat: 4.104 G_VGG: 1.520 D_real: 0.006 D_fake: 0.004 \n",
            "(epoch: 49, iters: 200, time: 0.566) G_GAN: 1.826 G_GAN_Feat: 3.041 G_VGG: 1.603 D_real: 0.008 D_fake: 0.028 \n",
            "(epoch: 49, iters: 300, time: 0.566) G_GAN: 2.039 G_GAN_Feat: 4.251 G_VGG: 1.343 D_real: 0.005 D_fake: 0.004 \n",
            "(epoch: 49, iters: 400, time: 0.566) G_GAN: 2.055 G_GAN_Feat: 3.834 G_VGG: 1.576 D_real: 0.009 D_fake: 0.006 \n",
            "saving the latest model (epoch 49, total_steps 202000)\n",
            "(epoch: 49, iters: 500, time: 0.566) G_GAN: 2.035 G_GAN_Feat: 3.855 G_VGG: 1.706 D_real: 0.037 D_fake: 0.016 \n",
            "(epoch: 49, iters: 600, time: 0.564) G_GAN: 1.572 G_GAN_Feat: 3.612 G_VGG: 1.627 D_real: 0.028 D_fake: 0.077 \n",
            "(epoch: 49, iters: 700, time: 0.565) G_GAN: 1.792 G_GAN_Feat: 3.170 G_VGG: 1.286 D_real: 0.007 D_fake: 0.143 \n",
            "(epoch: 49, iters: 800, time: 0.566) G_GAN: 2.322 G_GAN_Feat: 3.774 G_VGG: 1.728 D_real: 0.104 D_fake: 0.023 \n",
            "(epoch: 49, iters: 900, time: 0.565) G_GAN: 2.003 G_GAN_Feat: 3.539 G_VGG: 1.567 D_real: 0.013 D_fake: 0.008 \n",
            "(epoch: 49, iters: 1000, time: 0.566) G_GAN: 2.270 G_GAN_Feat: 4.460 G_VGG: 1.794 D_real: 0.034 D_fake: 0.027 \n",
            "(epoch: 49, iters: 1100, time: 0.565) G_GAN: 2.102 G_GAN_Feat: 4.086 G_VGG: 1.240 D_real: 0.008 D_fake: 0.006 \n",
            "(epoch: 49, iters: 1200, time: 0.566) G_GAN: 2.046 G_GAN_Feat: 4.353 G_VGG: 1.444 D_real: 0.031 D_fake: 0.003 \n",
            "(epoch: 49, iters: 1300, time: 0.564) G_GAN: 2.260 G_GAN_Feat: 3.821 G_VGG: 1.810 D_real: 0.039 D_fake: 0.025 \n",
            "(epoch: 49, iters: 1400, time: 0.566) G_GAN: 1.921 G_GAN_Feat: 3.081 G_VGG: 1.625 D_real: 0.007 D_fake: 0.006 \n",
            "saving the latest model (epoch 49, total_steps 203000)\n",
            "(epoch: 49, iters: 1500, time: 0.567) G_GAN: 2.005 G_GAN_Feat: 3.360 G_VGG: 1.317 D_real: 0.059 D_fake: 0.014 \n",
            "(epoch: 49, iters: 1600, time: 0.565) G_GAN: 2.168 G_GAN_Feat: 3.582 G_VGG: 1.599 D_real: 0.078 D_fake: 0.010 \n",
            "(epoch: 49, iters: 1700, time: 0.564) G_GAN: 2.086 G_GAN_Feat: 3.379 G_VGG: 1.429 D_real: 0.009 D_fake: 0.005 \n",
            "(epoch: 49, iters: 1800, time: 0.566) G_GAN: 1.992 G_GAN_Feat: 3.611 G_VGG: 1.487 D_real: 0.005 D_fake: 0.006 \n",
            "(epoch: 49, iters: 1900, time: 0.566) G_GAN: 1.798 G_GAN_Feat: 3.674 G_VGG: 1.524 D_real: 0.057 D_fake: 0.044 \n",
            "(epoch: 49, iters: 2000, time: 0.565) G_GAN: 1.811 G_GAN_Feat: 3.315 G_VGG: 1.874 D_real: 0.008 D_fake: 0.020 \n",
            "(epoch: 49, iters: 2100, time: 0.566) G_GAN: 1.975 G_GAN_Feat: 3.808 G_VGG: 1.616 D_real: 0.036 D_fake: 0.008 \n",
            "(epoch: 49, iters: 2200, time: 0.565) G_GAN: 1.884 G_GAN_Feat: 3.858 G_VGG: 1.959 D_real: 0.037 D_fake: 0.013 \n",
            "(epoch: 49, iters: 2300, time: 0.566) G_GAN: 1.794 G_GAN_Feat: 4.037 G_VGG: 1.581 D_real: 0.020 D_fake: 0.013 \n",
            "(epoch: 49, iters: 2400, time: 0.565) G_GAN: 1.819 G_GAN_Feat: 4.098 G_VGG: 1.300 D_real: 0.006 D_fake: 0.057 \n",
            "saving the latest model (epoch 49, total_steps 204000)\n",
            "(epoch: 49, iters: 2500, time: 0.568) G_GAN: 1.740 G_GAN_Feat: 3.360 G_VGG: 1.614 D_real: 0.048 D_fake: 0.191 \n",
            "(epoch: 49, iters: 2600, time: 0.564) G_GAN: 1.680 G_GAN_Feat: 3.281 G_VGG: 1.246 D_real: 0.017 D_fake: 0.027 \n",
            "(epoch: 49, iters: 2700, time: 0.565) G_GAN: 1.756 G_GAN_Feat: 3.469 G_VGG: 1.539 D_real: 0.018 D_fake: 0.020 \n",
            "(epoch: 49, iters: 2800, time: 0.566) G_GAN: 2.503 G_GAN_Feat: 3.425 G_VGG: 1.447 D_real: 0.073 D_fake: 0.033 \n",
            "(epoch: 49, iters: 2900, time: 0.566) G_GAN: 1.974 G_GAN_Feat: 2.911 G_VGG: 1.213 D_real: 0.017 D_fake: 0.020 \n",
            "(epoch: 49, iters: 3000, time: 0.566) G_GAN: 1.776 G_GAN_Feat: 3.297 G_VGG: 1.426 D_real: 0.010 D_fake: 0.016 \n",
            "(epoch: 49, iters: 3100, time: 0.566) G_GAN: 1.826 G_GAN_Feat: 3.100 G_VGG: 1.415 D_real: 0.005 D_fake: 0.047 \n",
            "(epoch: 49, iters: 3200, time: 0.565) G_GAN: 1.956 G_GAN_Feat: 3.689 G_VGG: 1.481 D_real: 0.005 D_fake: 0.004 \n",
            "(epoch: 49, iters: 3300, time: 0.566) G_GAN: 2.276 G_GAN_Feat: 3.480 G_VGG: 1.751 D_real: 0.048 D_fake: 0.023 \n",
            "(epoch: 49, iters: 3400, time: 0.564) G_GAN: 2.074 G_GAN_Feat: 3.562 G_VGG: 1.312 D_real: 0.019 D_fake: 0.004 \n",
            "saving the latest model (epoch 49, total_steps 205000)\n",
            "(epoch: 49, iters: 3500, time: 0.567) G_GAN: 1.156 G_GAN_Feat: 3.115 G_VGG: 1.498 D_real: 0.049 D_fake: 0.416 \n",
            "(epoch: 49, iters: 3600, time: 0.564) G_GAN: 1.985 G_GAN_Feat: 3.362 G_VGG: 1.824 D_real: 0.013 D_fake: 0.018 \n",
            "(epoch: 49, iters: 3700, time: 0.565) G_GAN: 1.688 G_GAN_Feat: 3.303 G_VGG: 1.544 D_real: 0.023 D_fake: 0.059 \n",
            "(epoch: 49, iters: 3800, time: 0.566) G_GAN: 1.933 G_GAN_Feat: 3.989 G_VGG: 1.512 D_real: 0.010 D_fake: 0.005 \n",
            "(epoch: 49, iters: 3900, time: 0.566) G_GAN: 1.754 G_GAN_Feat: 3.375 G_VGG: 1.475 D_real: 0.010 D_fake: 0.177 \n",
            "(epoch: 49, iters: 4000, time: 0.565) G_GAN: 2.057 G_GAN_Feat: 3.394 G_VGG: 1.403 D_real: 0.006 D_fake: 0.006 \n",
            "(epoch: 49, iters: 4100, time: 0.565) G_GAN: 1.698 G_GAN_Feat: 3.527 G_VGG: 1.683 D_real: 0.024 D_fake: 0.047 \n",
            "(epoch: 49, iters: 4200, time: 0.565) G_GAN: 1.883 G_GAN_Feat: 3.699 G_VGG: 1.771 D_real: 0.004 D_fake: 0.012 \n",
            "End of epoch 49 / 200 \t Time Taken: 2392 sec\n",
            "(epoch: 50, iters: 100, time: 0.564) G_GAN: 2.049 G_GAN_Feat: 3.784 G_VGG: 1.324 D_real: 0.005 D_fake: 0.005 \n",
            "(epoch: 50, iters: 200, time: 0.566) G_GAN: 1.921 G_GAN_Feat: 3.931 G_VGG: 1.659 D_real: 0.005 D_fake: 0.004 \n",
            "saving the latest model (epoch 50, total_steps 206000)\n",
            "(epoch: 50, iters: 300, time: 0.566) G_GAN: 1.957 G_GAN_Feat: 3.716 G_VGG: 1.389 D_real: 0.021 D_fake: 0.006 \n",
            "(epoch: 50, iters: 400, time: 0.566) G_GAN: 2.002 G_GAN_Feat: 3.126 G_VGG: 1.714 D_real: 0.043 D_fake: 0.005 \n",
            "(epoch: 50, iters: 500, time: 0.564) G_GAN: 1.918 G_GAN_Feat: 3.106 G_VGG: 1.140 D_real: 0.004 D_fake: 0.040 \n",
            "(epoch: 50, iters: 600, time: 0.565) G_GAN: 2.015 G_GAN_Feat: 3.601 G_VGG: 1.606 D_real: 0.012 D_fake: 0.003 \n",
            "(epoch: 50, iters: 700, time: 0.565) G_GAN: 1.957 G_GAN_Feat: 3.905 G_VGG: 1.547 D_real: 0.007 D_fake: 0.006 \n",
            "(epoch: 50, iters: 800, time: 0.566) G_GAN: 1.624 G_GAN_Feat: 3.057 G_VGG: 1.376 D_real: 0.020 D_fake: 0.213 \n",
            "(epoch: 50, iters: 900, time: 0.566) G_GAN: 1.866 G_GAN_Feat: 4.734 G_VGG: 1.771 D_real: 0.032 D_fake: 0.020 \n",
            "(epoch: 50, iters: 1000, time: 0.565) G_GAN: 1.844 G_GAN_Feat: 3.334 G_VGG: 1.391 D_real: 0.006 D_fake: 0.025 \n",
            "(epoch: 50, iters: 1100, time: 0.567) G_GAN: 2.057 G_GAN_Feat: 3.410 G_VGG: 1.095 D_real: 0.008 D_fake: 0.004 \n",
            "(epoch: 50, iters: 1200, time: 0.565) G_GAN: 1.839 G_GAN_Feat: 3.008 G_VGG: 1.569 D_real: 0.006 D_fake: 0.041 \n",
            "saving the latest model (epoch 50, total_steps 207000)\n",
            "(epoch: 50, iters: 1300, time: 0.568) G_GAN: 1.939 G_GAN_Feat: 3.100 G_VGG: 1.365 D_real: 0.006 D_fake: 0.005 \n",
            "(epoch: 50, iters: 1400, time: 0.564) G_GAN: 2.015 G_GAN_Feat: 4.059 G_VGG: 1.142 D_real: 0.014 D_fake: 0.003 \n",
            "(epoch: 50, iters: 1500, time: 0.566) G_GAN: 2.126 G_GAN_Feat: 3.532 G_VGG: 1.658 D_real: 0.011 D_fake: 0.009 \n",
            "(epoch: 50, iters: 1600, time: 0.565) G_GAN: 2.081 G_GAN_Feat: 3.654 G_VGG: 1.488 D_real: 0.008 D_fake: 0.006 \n",
            "(epoch: 50, iters: 1700, time: 0.565) G_GAN: 1.960 G_GAN_Feat: 3.285 G_VGG: 1.624 D_real: 0.007 D_fake: 0.016 \n",
            "(epoch: 50, iters: 1800, time: 0.565) G_GAN: 1.906 G_GAN_Feat: 3.920 G_VGG: 1.575 D_real: 0.191 D_fake: 0.014 \n",
            "(epoch: 50, iters: 1900, time: 0.566) G_GAN: 1.387 G_GAN_Feat: 3.364 G_VGG: 1.291 D_real: 0.106 D_fake: 0.379 \n",
            "(epoch: 50, iters: 2000, time: 0.565) G_GAN: 1.830 G_GAN_Feat: 3.077 G_VGG: 1.374 D_real: 0.024 D_fake: 0.051 \n",
            "(epoch: 50, iters: 2100, time: 0.566) G_GAN: 1.989 G_GAN_Feat: 3.553 G_VGG: 1.364 D_real: 0.009 D_fake: 0.008 \n",
            "(epoch: 50, iters: 2200, time: 0.565) G_GAN: 1.800 G_GAN_Feat: 3.455 G_VGG: 1.592 D_real: 0.018 D_fake: 0.018 \n",
            "saving the latest model (epoch 50, total_steps 208000)\n",
            "(epoch: 50, iters: 2300, time: 0.567) G_GAN: 1.873 G_GAN_Feat: 3.513 G_VGG: 1.597 D_real: 0.078 D_fake: 0.067 \n",
            "(epoch: 50, iters: 2400, time: 0.567) G_GAN: 1.735 G_GAN_Feat: 3.481 G_VGG: 1.663 D_real: 0.022 D_fake: 0.074 \n",
            "(epoch: 50, iters: 2500, time: 0.566) G_GAN: 2.000 G_GAN_Feat: 3.215 G_VGG: 1.355 D_real: 0.007 D_fake: 0.009 \n",
            "(epoch: 50, iters: 2600, time: 0.566) G_GAN: 2.052 G_GAN_Feat: 3.496 G_VGG: 1.486 D_real: 0.005 D_fake: 0.004 \n",
            "(epoch: 50, iters: 2700, time: 0.565) G_GAN: 1.702 G_GAN_Feat: 3.103 G_VGG: 1.577 D_real: 0.009 D_fake: 0.044 \n",
            "(epoch: 50, iters: 2800, time: 0.566) G_GAN: 1.800 G_GAN_Feat: 3.415 G_VGG: 1.425 D_real: 0.004 D_fake: 0.031 \n",
            "(epoch: 50, iters: 2900, time: 0.567) G_GAN: 1.913 G_GAN_Feat: 3.896 G_VGG: 1.379 D_real: 0.034 D_fake: 0.006 \n",
            "(epoch: 50, iters: 3000, time: 0.564) G_GAN: 2.232 G_GAN_Feat: 3.793 G_VGG: 1.694 D_real: 0.204 D_fake: 0.019 \n",
            "(epoch: 50, iters: 3100, time: 0.567) G_GAN: 1.861 G_GAN_Feat: 3.369 G_VGG: 1.588 D_real: 0.035 D_fake: 0.024 \n",
            "(epoch: 50, iters: 3200, time: 0.564) G_GAN: 1.788 G_GAN_Feat: 2.864 G_VGG: 1.397 D_real: 0.071 D_fake: 0.026 \n",
            "saving the latest model (epoch 50, total_steps 209000)\n",
            "(epoch: 50, iters: 3300, time: 0.567) G_GAN: 1.965 G_GAN_Feat: 3.575 G_VGG: 1.729 D_real: 0.115 D_fake: 0.076 \n",
            "(epoch: 50, iters: 3400, time: 0.565) G_GAN: 1.890 G_GAN_Feat: 4.545 G_VGG: 1.407 D_real: 0.021 D_fake: 0.006 \n",
            "(epoch: 50, iters: 3500, time: 0.566) G_GAN: 2.216 G_GAN_Feat: 3.566 G_VGG: 1.388 D_real: 0.019 D_fake: 0.016 \n",
            "(epoch: 50, iters: 3600, time: 0.566) G_GAN: 1.994 G_GAN_Feat: 3.989 G_VGG: 1.453 D_real: 0.016 D_fake: 0.005 \n",
            "(epoch: 50, iters: 3700, time: 0.565) G_GAN: 1.921 G_GAN_Feat: 3.533 G_VGG: 1.643 D_real: 0.006 D_fake: 0.006 \n",
            "(epoch: 50, iters: 3800, time: 0.565) G_GAN: 2.163 G_GAN_Feat: 4.375 G_VGG: 1.676 D_real: 0.012 D_fake: 0.006 \n",
            "(epoch: 50, iters: 3900, time: 0.564) G_GAN: 1.408 G_GAN_Feat: 3.425 G_VGG: 1.537 D_real: 0.030 D_fake: 0.346 \n",
            "(epoch: 50, iters: 4000, time: 0.565) G_GAN: 1.757 G_GAN_Feat: 2.877 G_VGG: 1.473 D_real: 0.010 D_fake: 0.045 \n",
            "(epoch: 50, iters: 4100, time: 0.566) G_GAN: 2.160 G_GAN_Feat: 5.176 G_VGG: 1.871 D_real: 0.012 D_fake: 0.010 \n",
            "(epoch: 50, iters: 4200, time: 0.566) G_GAN: 2.068 G_GAN_Feat: 3.600 G_VGG: 1.203 D_real: 0.064 D_fake: 0.047 \n",
            "saving the latest model (epoch 50, total_steps 210000)\n",
            "End of epoch 50 / 200 \t Time Taken: 2393 sec\n",
            "saving the model at the end of epoch 50, iters 210000\n",
            "(epoch: 51, iters: 100, time: 0.571) G_GAN: 2.017 G_GAN_Feat: 3.511 G_VGG: 1.727 D_real: 0.013 D_fake: 0.008 \n",
            "(epoch: 51, iters: 200, time: 0.563) G_GAN: 1.909 G_GAN_Feat: 3.503 G_VGG: 1.196 D_real: 0.005 D_fake: 0.022 \n",
            "(epoch: 51, iters: 300, time: 0.565) G_GAN: 2.011 G_GAN_Feat: 3.585 G_VGG: 1.418 D_real: 0.016 D_fake: 0.007 \n",
            "(epoch: 51, iters: 400, time: 0.567) G_GAN: 2.084 G_GAN_Feat: 3.762 G_VGG: 1.568 D_real: 0.011 D_fake: 0.007 \n",
            "(epoch: 51, iters: 500, time: 0.566) G_GAN: 1.820 G_GAN_Feat: 3.397 G_VGG: 1.562 D_real: 0.020 D_fake: 0.021 \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/train.py\", line 70, in <module>\n",
            "    losses, generated = model(Variable(data['label']), Variable(data['inst']), \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n",
            "    return self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/pix2pixHD_model.py\", line 170, in forward\n",
            "    pred_real = self.discriminate(input_label, real_image)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/pix2pixHD_model.py\", line 150, in discriminate\n",
            "    return self.netD.forward(input_concat)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/networks.py\", line 328, in forward\n",
            "    result.append(self.singleD_forward(model, input_downsampled))\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD/models/networks.py\", line 314, in singleD_forward\n",
            "    result.append(model[i](result[-1]))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py\", line 74, in forward\n",
            "    return self._apply_instance_norm(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py\", line 34, in _apply_instance_norm\n",
            "    return F.instance_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2495, in instance_norm\n",
            "    return torch.instance_norm(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia_medium --dataroot ./datasets/nebbia_medium --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "KHhH8FILC0Od",
        "outputId": "3e24808e-2079-4187-e544-cc59b988786b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia_medium\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_medium\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 51 at iteration 3000\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c5cac3211b9c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#training on low resolution only G1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python train.py --continue_train --label_nc 0 --no_instance --name nebbia_medium --dataroot ./datasets/nebbia_medium --resize_or_crop crop --fineSize 512 --batchSize 4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia_high --dataroot ./datasets/nebbia_high --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "QnrdU2E9C0hN",
        "outputId": "f442f15d-aa97-4bb7-b406-89a392c75029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia_high\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_high\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 51 at iteration 2000\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-62a011cc3f7e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#training on low resolution only G1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python train.py --continue_train --label_nc 0 --no_instance --name nebbia_high --dataroot ./datasets/nebbia_high --resize_or_crop crop --fineSize 512 --batchSize 4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2Pix test\n"
      ],
      "metadata": {
        "id": "qEuQqQPxGISq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General model epoch 50\n"
      ],
      "metadata": {
        "id": "79Es7B0cdWS-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_m4nxHHLAtxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c3e541-ceb1-4179-d34d-36783f2af340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./datasets/indoor\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./datasets/indoor/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/models/pix2pixHD_model.py:128: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input_label = Variable(input_label, volatile=infer)\n",
            "process image... ['./datasets/indoor/test_A/1400_1.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_10.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_2.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_3.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_4.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_5.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_6.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_7.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_8.png']\n",
            "process image... ['./datasets/indoor/test_A/1400_9.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_1.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_10.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_2.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_3.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_4.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_5.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_6.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_7.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_8.png']\n",
            "process image... ['./datasets/indoor/test_A/1401_9.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_1.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_10.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_2.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_3.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_4.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_5.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_6.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_7.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_8.png']\n",
            "process image... ['./datasets/indoor/test_A/1402_9.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_1.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_10.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_2.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_3.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_4.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_5.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_6.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_7.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_8.png']\n",
            "process image... ['./datasets/indoor/test_A/1403_9.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_1.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_10.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_2.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_3.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_4.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_5.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_6.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_7.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_8.png']\n",
            "process image... ['./datasets/indoor/test_A/1404_9.png']\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./datasets/outdoor\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./datasets/outdoor/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/models/pix2pixHD_model.py:128: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input_label = Variable(input_label, volatile=infer)\n",
            "process image... ['./datasets/outdoor/test_A/0001_0.8_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0002_0.8_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0003_0.8_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0004_0.9_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0006_0.85_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0007_0.9_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0009_0.8_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0010_0.95_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0011_0.95_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0014_0.8_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0016_0.8_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0017_0.9_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0018_0.9_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0019_0.8_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0021_0.8_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0022_0.8_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0023_0.85_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0024_0.85_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0025_0.8_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0026_0.95_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0029_0.85_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0030_0.95_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0033_0.85_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0034_0.9_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0036_1_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0039_0.9_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0040_1_0.08.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0042_0.8_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0045_0.8_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0046_0.9_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0047_0.9_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0048_0.9_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0049_0.85_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0051_0.8_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0051_0.95_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0052_0.9_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0053_0.9_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0054_0.85_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0055_0.85_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0056_0.8_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0057_0.8_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0058_0.8_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0059_0.9_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0060_0.9_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0061_0.8_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0062_0.9_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0063_0.8_0.16.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0064_0.8_0.2.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0065_0.9_0.12.jpg']\n",
            "process image... ['./datasets/outdoor/test_A/0066_1_0.08.jpg']\n"
          ]
        }
      ],
      "source": [
        "#se non  stato eseguito il download\n",
        "if os.getcwd() != \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD\":\n",
        "  os.chdir(\"pix2pixHD\")\n",
        "\n",
        "!python test.py --dataroot ./datasets/indoor --name nebbia --resize_or_crop none --label_nc 0 --no_instance --which_epoch 50 --results_dir ./datasets/indoor/synth\n",
        "!python test.py --dataroot ./datasets/outdoor --name nebbia --resize_or_crop none --label_nc 0 --no_instance --which_epoch 50 --results_dir ./datasets/outdoor/synth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisci i percorsi delle cartelle contenenti le immagini\n",
        "indoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/indoor/synth/nebbia/test_50/images\"  # Cartella con le immagini generate\n",
        "indoor_original_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/indoor/test_A\"        # Cartella con le immagini originali\n",
        "outdoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/outdoor/synth/nebbia/test_50/images\"  # Cartella con le immagini generate\n",
        "outdoor_original_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/outdoor/test_A\"       # Cartella con le immagini originali\n",
        "result_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/results\"\n",
        "\n",
        "test_indoor_outdoor(indoor_synth_folder, indoor_original_folder, outdoor_synth_folder, outdoor_original_folder,os.path.join(result_folder,\"50\"))"
      ],
      "metadata": {
        "id": "o-Uvp5SC9u4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "9ce22cab-a6d7-4709-b772-2b68be132baf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-51ad101db7d5>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresult_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/results\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_indoor_outdoor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindoor_synth_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindoor_original_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdoor_synth_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdoor_original_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"50\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-a01df93258a9>\u001b[0m in \u001b[0;36mtest_indoor_outdoor\u001b[0;34m(indoor_synth_folder, indoor_original_folder, outdoor_synth_folder, outdoor_original_folder, type)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_indoor_outdoor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindoor_synth_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindoor_original_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdoor_synth_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdoor_original_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindoor_synth_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindoor_original_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_indoor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m   \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdoor_synth_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdoor_original_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_outdoor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-a01df93258a9>\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(synth_folder, gt_folder, type)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Ridimensiona l'immagine sintetizzata alla stessa dimensione dell'immagine originale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msynth_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynth_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgt_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Calcola le metriche con skimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Model full train epoch 110"
      ],
      "metadata": {
        "id": "6hRFAx1AdjyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --dataroot ./datasets/outdoor --name nebbia --resize_or_crop none --label_nc 0 --no_instance --results_dir ./datasets/outdoor/synth"
      ],
      "metadata": {
        "id": "tjQklq0JevLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisci i percorsi delle cartelle contenenti le immagini\n",
        "indoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/indoor/synth/nebbia/test_latest/images\"  # Cartella con le immagini generate\n",
        "outdoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/outdoor/synth/nebbia/test_latest/images\"  # Cartella con le immagini generate\n",
        "\n",
        "test_indoor_outdoor(indoor_synth_folder, indoor_original_folder, outdoor_synth_folder, outdoor_original_folder,os.path.join(result_folder,\"latest\"))"
      ],
      "metadata": {
        "id": "ETiBNR0-niVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier + 3GAN epoch 50"
      ],
      "metadata": {
        "id": "KIQ7RJpK-vH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier"
      ],
      "metadata": {
        "id": "EA3lpWp4pCPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Carica il tuo modello di classificazione\n",
        "classification_model = load_model(\"./classifier/dataset/models/InceptionV3/InceptionV3_128.h5\")\n",
        "#classification_model = load_model('./classifier/dataset/models/VGG16/VGG16_model.h5')\n",
        "\n",
        "# Definisci le etichette delle classi\n",
        "class_labels = [\"no_nebbia\", \"nebbia_low\", \"nebbia_medium\", \"nebbia_high\"]\n",
        "\n",
        "# Percorso delle immagini di input\n",
        "base_path = \"./pix2pixHD/datasets/\"\n",
        "context = [\"indoor\",\"outdoor\"]\n",
        "\n",
        "# Percorso della cartella dei risultati\n",
        "output_folder = \"./pix2pixHD/results/\"\n",
        "for scene in context:\n",
        "  input_folder = os.path.join(base_path, os.path.join(scene, \"test_A\"))\n",
        "  # Ciclo attraverso tutte le immagini nella cartella di input\n",
        "  for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
        "      # Carica l'immagine di input\n",
        "      input_image = cv2.imread(os.path.join(input_folder, filename))\n",
        "\n",
        "      # Ridimensiona l'immagine\n",
        "      target_width = 512\n",
        "      target_height = 512\n",
        "      resized_image = Image.fromarray(input_image).resize((target_width, target_height))\n",
        "      resized_image_array = np.array(resized_image)\n",
        "      resized_image_array = np.expand_dims(resized_image_array, axis=0)\n",
        "\n",
        "      # Esegui la classificazione dell'immagine\n",
        "      predicted_probabilities = classification_model.predict(resized_image_array)\n",
        "      predicted_class = np.argmax(predicted_probabilities)\n",
        "      image_class_label = class_labels[predicted_class]\n",
        "\n",
        "      # Crea la cartella di output se non esiste\n",
        "      output_class_folder = os.path.join(output_folder, os.path.join(\"nebbia3GAN\",os.path.join(scene,os.path.join(image_class_label,\"test_A\"))))\n",
        "      os.makedirs(output_class_folder, exist_ok=True)\n",
        "\n",
        "      # Salva l'immagine classificata nella cartella di output\n",
        "      output_path = os.path.join(output_class_folder, filename)\n",
        "      cv2.imwrite(output_path, input_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "flcY45R3-Xkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9387f5d-8819-4387-a7c9-95607d2374ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 236ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Images"
      ],
      "metadata": {
        "id": "hSl4-OJnpIoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "class_labels = [\"nebbia_low\", \"nebbia_medium\", \"nebbia_high\"]\n",
        "context = [\"indoor\",\"outdoor\"]\n",
        "base_path = \"./pix2pixHD/results/nebbia3GAN\"\n",
        "checkpoints_dir = \"./pix2pixHD/checkpoints\"\n",
        "\n",
        "for scene in context:\n",
        "  for label in class_labels:\n",
        "    dataset_path = os.path.join(base_path, os.path.join(scene, label))\n",
        "    result_path = os.path.join(base_path, os.path.join(scene, os.path.join(label, \"synth\")))\n",
        "    # Definisci il comando da eseguire\n",
        "    command = [\n",
        "        'python', 'pix2pixHD/test.py',\n",
        "        '--dataroot', dataset_path,\n",
        "        '--checkpoints_dir', checkpoints_dir,\n",
        "        '--name', label,\n",
        "        '--resize_or_crop', 'none',\n",
        "        '--label_nc', '0',\n",
        "        '--no_instance',\n",
        "        '--which_epoch', '50',\n",
        "        '--results_dir', result_path\n",
        "    ]\n",
        "\n",
        "    # Esegui il comando utilizzando subprocess\n",
        "    completed_process = subprocess.run(command, stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "    # Ottieni l'output dall'oggetto CompletedProcess\n",
        "    output = completed_process.stdout\n",
        "\n",
        "    # Stampa l'output\n",
        "    print(label)\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "87e6vnloNtFe",
        "outputId": "8850861c-4541-445e-f61a-870d75e1bb06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nebbia_low\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./pix2pixHD/checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_low\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1401_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1401_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1403_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1403_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1410_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1410_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1411_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1411_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1411_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1411_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1411_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1411_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1411_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1413_3.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1413_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1413_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_2.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_3.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1414_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1415_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1415_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1415_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1415_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1415_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1415_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1415_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1416_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1417_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1420_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1420_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1420_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1420_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1420_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1423_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1423_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1423_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1423_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1424_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1424_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1424_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1430_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1430_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1430_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_low/test_A/1430_9.png']\n",
            "\n",
            "nebbia_medium\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./pix2pixHD/checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_medium\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1413_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1413_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1413_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1413_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1413_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1420_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1434_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1434_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1435_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1435_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1435_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1435_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1440_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_medium/test_A/1440_9.png']\n",
            "\n",
            "nebbia_high\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./pix2pixHD/checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_high\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_1.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_2.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_3.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1400_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_1.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_2.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_3.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1401_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_1.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_2.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_3.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1402_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_1.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_2.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_3.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1403_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_1.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_2.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_3.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_4.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_5.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_6.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_7.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_8.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1404_9.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1405_1.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1405_10.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1405_2.png']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/indoor/nebbia_high/test_A/1405_3.png']\n",
            "\n",
            "nebbia_low\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./pix2pixHD/checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_low\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0003_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0007_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0011_0.95_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0023_0.85_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0045_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0046_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0048_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0056_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0060_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0061_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0062_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0066_1_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0072_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0083_0.85_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0105_0.95_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0112_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0116_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0117_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0120_0.85_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0121_0.85_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0126_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0127_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0132_1_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0133_1_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0143_1_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0148_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0153_0.8_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0160_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0161_0.85_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0162_0.95_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0166_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0167_0.8_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0175_0.85_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0185_1_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0189_0.85_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0194_1_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0212_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0225_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0230_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0238_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0239_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0243_0.8_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0267_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0269_0.95_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0270_0.85_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0271_0.85_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0273_0.85_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0280_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0281_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_low/test_A/0287_0.8_0.2.jpg']\n",
            "\n",
            "nebbia_medium\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./pix2pixHD/checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_medium\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_medium\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_medium/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_medium/test_A/0125_0.8_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_medium/test_A/0226_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_medium/test_A/0228_1_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_medium/test_A/0335_1_0.2.jpg']\n",
            "\n",
            "nebbia_high\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./pix2pixHD/checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "how_many: 50\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: nebbia_high\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_instance: True\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: none\n",
            "results_dir: ./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/synth\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: 50\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0001_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0002_0.8_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0004_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0006_0.85_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0009_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0010_0.95_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0014_0.8_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0016_0.8_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0017_0.9_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0018_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0019_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0021_0.8_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0022_0.8_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0024_0.85_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0025_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0026_0.95_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0029_0.85_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0030_0.95_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0033_0.85_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0034_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0036_1_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0039_0.9_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0040_1_0.08.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0042_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0047_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0049_0.85_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0051_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0051_0.95_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0052_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0053_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0054_0.85_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0055_0.85_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0057_0.8_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0058_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0059_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0063_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0064_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0065_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0068_1_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0069_0.9_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0070_0.9_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0071_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0073_0.8_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0074_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0075_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0076_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0076_1_0.16.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0077_0.8_0.2.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0079_0.8_0.12.jpg']\n",
            "process image... ['./pix2pixHD/results/nebbia3GAN/outdoor/nebbia_high/test_A/0081_0.8_0.2.jpg']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate images"
      ],
      "metadata": {
        "id": "iwiE3e5WpQpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "class_labels = [\"nebbia_low\", \"nebbia_medium\", \"nebbia_high\"]\n",
        "context = [\"indoor\",\"outdoor\"]\n",
        "base_path = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/results/nebbia3GAN/\"\n",
        "for scene in context:\n",
        "  for label in class_labels:\n",
        "    source = os.path.join(base_path, os.path.join(scene, os.path.join(label, os.path.join(\"synth\", os.path.join(label, os.path.join(\"test_50\", \"images\"))))))\n",
        "    dest = os.path.join(base_path, os.path.join(scene, \"synth\"))\n",
        "\n",
        "    # Ottieni la lista dei file nella cartella di origine\n",
        "    elenco_file = os.listdir(source)\n",
        "\n",
        "    if not os.path.exists(dest):\n",
        "      os.makedirs(dest)\n",
        "\n",
        "    # Loop attraverso tutti i file nella cartella di origine e spostali nella cartella di destinazione\n",
        "    for file in elenco_file:\n",
        "        percorso_file_origine = os.path.join(source, file)\n",
        "        percorso_file_destinazione = os.path.join(dest, file)\n",
        "\n",
        "        # Sposta il file\n",
        "        shutil.move(percorso_file_origine, percorso_file_destinazione)\n",
        "\n",
        "print(\"Spostamento completato.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqQ_Qq0SpbzN",
        "outputId": "e36e887c-7ecd-4063-d2b9-3dcb03b141b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spostamento completato.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisci i percorsi delle cartelle contenenti le immagini\n",
        "indoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/results/nebbia3GAN/indoor/synth\"  # Cartella con le immagini generate\n",
        "outdoor_synth_folder = \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/results/nebbia3GAN/outdoor/synth\"  # Cartella con le immagini generate\n",
        "\n",
        "test_indoor_outdoor(indoor_synth_folder, indoor_original_folder, outdoor_synth_folder, outdoor_original_folder,os.path.join(result_folder,\"3GAN\"))"
      ],
      "metadata": {
        "id": "R4lopGzjnv9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}