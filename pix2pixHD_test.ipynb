{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiadido95/Haze-Fog-suppression/blob/main/pix2pixHD_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "N-fvIhrKELDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1pF_YMYXAtxV",
        "outputId": "14bf2746-6a60-4ce8-a004-61babed4674e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dominate\n",
            "  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: dominate\n",
            "Successfully installed dominate-2.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dominate gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import gdown"
      ],
      "metadata": {
        "id": "DW2OEAxLB1ni"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qMC8BzYnB-Mq",
        "outputId": "33d91d8f-8107-4d1f-9de9-1fc088bbd98c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"drive/MyDrive/Haze-Fog-suppression\"\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "0vFtFOdwCHWs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Apex for Automatic Mixed Precision to speed up training\n"
      ],
      "metadata": {
        "id": "8GO4-qJQlpsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key...\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "cLaTD67wlxW_",
        "outputId": "7321fd9d-5eb9-400f-bb79-a4edf163e797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Processing /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info\n",
            "  writing /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-k24uqvr5/apex.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-k24uqvr5/apex-0.1.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/dist-packages (from apex==0.1) (23.1)\n",
            "Building wheels for collected packages: apex\n",
            "  Running command Building wheel for apex (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "\n",
            "  Compiling cuda extensions with\n",
            "  nvcc: NVIDIA (R) Cuda compiler driver\n",
            "  Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "  Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "  Cuda compilation tools, release 11.8, V11.8.89\n",
            "  Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "  from /usr/local/cuda/bin\n",
            "\n",
            "  running bdist_wheel\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-cpython-310\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/__init__.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib.linux-x86_64-cpython-310/apex\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib.linux-x86_64-cpython-310/apex/RNN\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib.linux-x86_64-cpython-310/apex/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/fp16_utils\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-cpython-310/apex/fused_dense\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib.linux-x86_64-cpython-310/apex/mlp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/normalization\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-cpython-310/apex/parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/_ucc_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-cpython-310/apex/amp/lists\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  copying apex/contrib/test/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/_transducer_ref.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/test_fmha.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/_data\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/amp\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/functional\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/layers\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/testing\n",
            "  creating build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'apex_C' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'amp_C' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'syncbn' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-cpython-310/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/welford.cu -o build/temp.linux-x86_64-cpython-310/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/syncbn.o build/temp.linux-x86_64-cpython-310/csrc/welford.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_layer_norm_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp.cpp -o build/temp.linux-x86_64-cpython-310/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>):\n",
            "  csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     57 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:64:76: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "        |                                                              ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:65:85: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "        |                                                                       ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/mlp.cpp:67:58: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "        |                                            ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:69:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "     72 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = mlp_fp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>):\n",
            "  csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    115 |   for (int i = 0; i < num_layers; i++) {\n",
            "        |                   ~~^~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    120 |   for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                   ~~^~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:121:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "        |                                                    ~~~~~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:124:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |                                       ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    228 |     const auto& the_type = TYPE;                                            \\\n",
            "        |                            ^~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:231:47: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "    258 |   AT_DISPATCH_SWITCH(                                        \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n",
            "    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "        |                       ^~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp: In lambda function:\n",
            "  csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: int and long unsigned int [-Wsign-compare]\n",
            "    126 |     for (int i = 0; i < num_layers; i++) {\n",
            "        |                     ~~^~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: int and std::vector<at::Tensor>::size_type {aka long unsigned int} [-Wsign-compare]\n",
            "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
            "        |                     ~~^~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:138:98: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/mlp.cpp:1:\n",
            "  csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "    140 |     auto result = mlp_bp<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/mlp.o build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_dense_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  csrc/fused_dense.cpp: In function at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:30:62: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     30 |   auto out = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                    ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:33:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     33 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:64:68: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     64 |   auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "        |                                                          ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:68:53: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     68 |   auto d_bias = at::empty({out_features}, input.type());\n",
            "        |                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:70:65: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     70 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:73:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     73 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "        |               ^~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:106:69: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:107:69: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                           ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:108:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    108 |   auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "        |                                                        ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:111:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    111 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "  csrc/fused_dense.cpp:149:72: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "        |                                                              ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:150:73: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "        |                                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:151:57: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    151 |   auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "        |                                               ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:152:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    152 |   auto d_bias2 = at::empty({out_features}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:153:65: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    153 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "        |                                                       ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:154:71: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "        |                                                             ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  csrc/fused_dense.cpp:157:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "    157 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "        |                                            ~~~~~~~~~~^~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
            "    222 |   DeprecatedTypeProperties & type() const {\n",
            "        |                              ^~~~\n",
            "  In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                   from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:4,\n",
            "                   from csrc/fused_dense.cpp:1:\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES\n",
            "    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp: In lambda function:\n",
            "  csrc/fused_dense.cpp:163:10: warning: unused variable result [-Wunused-variable]\n",
            "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "        |          ^~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "    234 |       __VA_ARGS__                                                           \\\n",
            "        |       ^~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
            "        |   ^~~~~~~~~~~~~~~~\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND2\n",
            "    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
            "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND2\n",
            "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
            "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/csrc/megatron\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'generic_scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_masked_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'scaled_softmax_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/scaled_softmax_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fused_weight_gradient_mlp_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/cuda/bin/nvcc -I/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\n",
            "\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n",
            "            detected during:\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  (61): here\n",
            "              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib.linux-x86_64-cpython-310/apex_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/amp_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/syncbn.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-310/fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing requirements to apex.egg-info/requires.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-d3ttvz1h/.tmp-eld9dset/apex-0.1-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'amp_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex_C.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_dense_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fused_weight_gradient_mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'generic_scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'mlp_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'syncbn.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/batch_norm.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/group_norm/group_norm.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/channel_swap.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/test/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/test_bottleneck_module.py'\n",
            "  adding 'apex/contrib/test/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/test/clip_grad/test_clip_grad.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py'\n",
            "  adding 'apex/contrib/test/fmha/__init__.py'\n",
            "  adding 'apex/contrib/test/fmha/test_fmha.py'\n",
            "  adding 'apex/contrib/test/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/test/focal_loss/test_focal_loss.py'\n",
            "  adding 'apex/contrib/test/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/group_norm/test_group_norm.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/test_index_mul_2d.py'\n",
            "  adding 'apex/contrib/test/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/layer_norm/test_fast_layer_norm.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_mha_fused_softmax.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_dist_adam.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/test/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py'\n",
            "  adding 'apex/contrib/test/transducer/__init__.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_joint.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_loss.py'\n",
            "  adding 'apex/contrib/test/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/test/xentropy/test_label_smoothing.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/_transducer_ref.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/_ucc_util.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp310-cp310-linux_x86_64.whl size=32236627 sha256=8d0f5d57c52dd93eda408da02959a0dc706e869e8b8c56c93333c8ac6452e593\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ep_u209/wheels/af/00/73/0e6dbf1a46334a3c4c0360bcdaf1085990eaf534b2bb7efefb\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD download\n"
      ],
      "metadata": {
        "id": "6eqPZHOXEAfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YoX7TkTS9MG",
        "outputId": "09120a1d-5280-4fed-f4ba-e3f727363778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pix2pixHD'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 55.68 MiB | 20.18 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n",
            "Updating files: 100% (115/115), done.\n",
            "/content/drive/MyDrive/Haze-suppression/pix2pixHD\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/pix2pixHD.git\n",
        "os.chdir(\"pix2pixHD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKuREuwyAtxd"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"./checkpoints/label2city_1024p/\")\n",
        "os.chdir(\"./checkpoints/label2city_1024p/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download'\n",
        "output = 'latest_net_G.pth'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "5VRByC-oFDWv",
        "outputId": "76edc983-ec8a-4868-c3e7-80a9d0229ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1h9SykUnuZul7J3Nbms2QGH1wa85nbN2-&export=download\n",
            "To: /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/checkpoints/label2city_1024p/latest_net_G.pth\n",
            "100%|| 732M/732M [00:11<00:00, 62.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'latest_net_G.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"../..\")"
      ],
      "metadata": {
        "id": "zpELF8NXHBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixHD Train\n"
      ],
      "metadata": {
        "id": "CFdeWg-Sqj6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pix2pixHD/"
      ],
      "metadata": {
        "id": "RxD4lD7grQjI",
        "outputId": "18bb4adb-29cb-4df9-f230-20693927a3f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1MeMU7hWD5WVMk25vU3ch5Ii3csnSydwf/Haze-Fog-suppression/pix2pixHD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1000\"\n",
        "\n",
        "#training on low resolution only G1\n",
        "!python train.py --continue_train --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 512 --batchSize 4"
      ],
      "metadata": {
        "id": "cBS7uIxAqn6W",
        "outputId": "3e23a2aa-1907-4f47-b377-933e3da98e48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 4\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/nebbia\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 0\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: nebbia\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: True\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: crop\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 11 at iteration 4000\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 4200\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:06<00:00, 86.5MB/s]\n",
            "create web directory ./checkpoints/nebbia/web...\n",
            "(epoch: 11, iters: 4100, time: 2.155) G_GAN: 1.458 G_GAN_Feat: 2.815 G_VGG: 2.057 D_real: 0.033 D_fake: 0.161 \n",
            "(epoch: 11, iters: 4200, time: 0.552) G_GAN: 1.815 G_GAN_Feat: 4.489 G_VGG: 3.044 D_real: 0.063 D_fake: 0.022 \n",
            "End of epoch 11 / 200 \t Time Taken: 278 sec\n",
            "(epoch: 12, iters: 100, time: 0.573) G_GAN: 1.800 G_GAN_Feat: 3.004 G_VGG: 2.790 D_real: 0.322 D_fake: 0.042 \n",
            "(epoch: 12, iters: 200, time: 0.578) G_GAN: 1.703 G_GAN_Feat: 3.194 G_VGG: 2.906 D_real: 0.047 D_fake: 0.050 \n",
            "(epoch: 12, iters: 300, time: 0.573) G_GAN: 1.411 G_GAN_Feat: 3.387 G_VGG: 3.101 D_real: 0.031 D_fake: 0.124 \n",
            "(epoch: 12, iters: 400, time: 0.576) G_GAN: 1.705 G_GAN_Feat: 3.286 G_VGG: 3.476 D_real: 0.077 D_fake: 0.047 \n",
            "(epoch: 12, iters: 500, time: 0.576) G_GAN: 1.419 G_GAN_Feat: 3.057 G_VGG: 2.443 D_real: 0.025 D_fake: 0.133 \n",
            "(epoch: 12, iters: 600, time: 0.576) G_GAN: 1.771 G_GAN_Feat: 3.598 G_VGG: 2.403 D_real: 0.049 D_fake: 0.051 \n",
            "(epoch: 12, iters: 700, time: 0.575) G_GAN: 1.718 G_GAN_Feat: 3.003 G_VGG: 2.666 D_real: 0.030 D_fake: 0.091 \n",
            "(epoch: 12, iters: 800, time: 0.574) G_GAN: 1.917 G_GAN_Feat: 3.251 G_VGG: 2.771 D_real: 0.186 D_fake: 0.010 \n",
            "saving the latest model (epoch 12, total_steps 47000)\n",
            "(epoch: 12, iters: 900, time: 0.576) G_GAN: 1.740 G_GAN_Feat: 3.327 G_VGG: 2.355 D_real: 0.032 D_fake: 0.076 \n",
            "(epoch: 12, iters: 1000, time: 0.576) G_GAN: 2.246 G_GAN_Feat: 3.639 G_VGG: 3.100 D_real: 0.038 D_fake: 0.045 \n",
            "(epoch: 12, iters: 1100, time: 0.573) G_GAN: 2.178 G_GAN_Feat: 3.159 G_VGG: 2.416 D_real: 0.100 D_fake: 0.022 \n",
            "(epoch: 12, iters: 1200, time: 0.575) G_GAN: 1.552 G_GAN_Feat: 3.118 G_VGG: 2.499 D_real: 0.039 D_fake: 0.081 \n",
            "(epoch: 12, iters: 1300, time: 0.573) G_GAN: 1.710 G_GAN_Feat: 3.014 G_VGG: 2.425 D_real: 0.022 D_fake: 0.094 \n",
            "(epoch: 12, iters: 1400, time: 0.576) G_GAN: 2.132 G_GAN_Feat: 3.467 G_VGG: 3.571 D_real: 0.038 D_fake: 0.020 \n",
            "(epoch: 12, iters: 1500, time: 0.573) G_GAN: 1.800 G_GAN_Feat: 2.921 G_VGG: 2.767 D_real: 0.316 D_fake: 0.030 \n",
            "(epoch: 12, iters: 1600, time: 0.575) G_GAN: 2.009 G_GAN_Feat: 3.898 G_VGG: 2.976 D_real: 0.039 D_fake: 0.044 \n",
            "(epoch: 12, iters: 1700, time: 0.573) G_GAN: 1.702 G_GAN_Feat: 3.443 G_VGG: 3.722 D_real: 0.034 D_fake: 0.054 \n",
            "(epoch: 12, iters: 1800, time: 0.572) G_GAN: 1.773 G_GAN_Feat: 3.319 G_VGG: 2.400 D_real: 0.015 D_fake: 0.035 \n",
            "saving the latest model (epoch 12, total_steps 48000)\n",
            "(epoch: 12, iters: 1900, time: 0.581) G_GAN: 1.884 G_GAN_Feat: 2.986 G_VGG: 2.733 D_real: 0.287 D_fake: 0.092 \n",
            "(epoch: 12, iters: 2000, time: 0.573) G_GAN: 1.791 G_GAN_Feat: 3.325 G_VGG: 3.085 D_real: 0.030 D_fake: 0.072 \n",
            "(epoch: 12, iters: 2100, time: 0.575) G_GAN: 1.487 G_GAN_Feat: 3.049 G_VGG: 2.257 D_real: 0.020 D_fake: 0.131 \n",
            "(epoch: 12, iters: 2200, time: 0.574) G_GAN: 2.093 G_GAN_Feat: 3.100 G_VGG: 2.526 D_real: 0.165 D_fake: 0.020 \n",
            "(epoch: 12, iters: 2300, time: 0.575) G_GAN: 1.523 G_GAN_Feat: 3.370 G_VGG: 2.730 D_real: 0.221 D_fake: 0.094 \n",
            "(epoch: 12, iters: 2400, time: 0.574) G_GAN: 1.469 G_GAN_Feat: 3.817 G_VGG: 2.886 D_real: 0.031 D_fake: 0.109 \n",
            "(epoch: 12, iters: 2500, time: 0.576) G_GAN: 1.751 G_GAN_Feat: 2.900 G_VGG: 2.384 D_real: 0.022 D_fake: 0.078 \n",
            "(epoch: 12, iters: 2600, time: 0.575) G_GAN: 2.072 G_GAN_Feat: 3.040 G_VGG: 2.871 D_real: 0.149 D_fake: 0.080 \n",
            "(epoch: 12, iters: 2700, time: 0.574) G_GAN: 1.338 G_GAN_Feat: 2.968 G_VGG: 2.778 D_real: 0.016 D_fake: 0.179 \n",
            "(epoch: 12, iters: 2800, time: 0.575) G_GAN: 1.480 G_GAN_Feat: 2.859 G_VGG: 2.861 D_real: 0.183 D_fake: 0.183 \n",
            "saving the latest model (epoch 12, total_steps 49000)\n",
            "(epoch: 12, iters: 2900, time: 0.576) G_GAN: 1.725 G_GAN_Feat: 3.139 G_VGG: 3.012 D_real: 0.033 D_fake: 0.099 \n",
            "(epoch: 12, iters: 3000, time: 0.575) G_GAN: 1.643 G_GAN_Feat: 3.136 G_VGG: 3.227 D_real: 0.035 D_fake: 0.062 \n",
            "(epoch: 12, iters: 3100, time: 0.573) G_GAN: 1.625 G_GAN_Feat: 3.520 G_VGG: 3.049 D_real: 0.039 D_fake: 0.045 \n",
            "(epoch: 12, iters: 3200, time: 0.577) G_GAN: 1.499 G_GAN_Feat: 3.656 G_VGG: 2.658 D_real: 0.051 D_fake: 0.161 \n",
            "(epoch: 12, iters: 3300, time: 0.575) G_GAN: 1.964 G_GAN_Feat: 3.028 G_VGG: 2.010 D_real: 0.095 D_fake: 0.044 \n",
            "(epoch: 12, iters: 3400, time: 0.573) G_GAN: 1.991 G_GAN_Feat: 3.266 G_VGG: 2.909 D_real: 0.080 D_fake: 0.030 \n",
            "(epoch: 12, iters: 3500, time: 0.577) G_GAN: 1.560 G_GAN_Feat: 3.254 G_VGG: 3.422 D_real: 0.043 D_fake: 0.205 \n",
            "(epoch: 12, iters: 3600, time: 0.573) G_GAN: 0.966 G_GAN_Feat: 2.577 G_VGG: 2.085 D_real: 0.077 D_fake: 0.457 \n",
            "(epoch: 12, iters: 3700, time: 0.575) G_GAN: 1.801 G_GAN_Feat: 3.117 G_VGG: 2.565 D_real: 0.091 D_fake: 0.057 \n",
            "(epoch: 12, iters: 3800, time: 0.573) G_GAN: 1.942 G_GAN_Feat: 3.784 G_VGG: 3.401 D_real: 0.053 D_fake: 0.038 \n",
            "saving the latest model (epoch 12, total_steps 50000)\n",
            "(epoch: 12, iters: 3900, time: 0.579) G_GAN: 1.182 G_GAN_Feat: 2.775 G_VGG: 2.637 D_real: 0.022 D_fake: 0.375 \n",
            "(epoch: 12, iters: 4000, time: 0.573) G_GAN: 1.704 G_GAN_Feat: 3.022 G_VGG: 2.937 D_real: 0.029 D_fake: 0.042 \n",
            "(epoch: 12, iters: 4100, time: 0.576) G_GAN: 1.806 G_GAN_Feat: 3.240 G_VGG: 2.715 D_real: 0.148 D_fake: 0.064 \n",
            "(epoch: 12, iters: 4200, time: 0.574) G_GAN: 1.832 G_GAN_Feat: 2.660 G_VGG: 2.446 D_real: 0.071 D_fake: 0.022 \n",
            "End of epoch 12 / 200 \t Time Taken: 2440 sec\n",
            "(epoch: 13, iters: 100, time: 0.574) G_GAN: 2.014 G_GAN_Feat: 3.453 G_VGG: 3.386 D_real: 0.324 D_fake: 0.068 \n",
            "(epoch: 13, iters: 200, time: 0.576) G_GAN: 1.507 G_GAN_Feat: 3.687 G_VGG: 3.525 D_real: 0.017 D_fake: 0.129 \n",
            "(epoch: 13, iters: 300, time: 0.576) G_GAN: 1.805 G_GAN_Feat: 3.658 G_VGG: 2.694 D_real: 0.020 D_fake: 0.044 \n",
            "(epoch: 13, iters: 400, time: 0.574) G_GAN: 1.737 G_GAN_Feat: 3.308 G_VGG: 2.182 D_real: 0.441 D_fake: 0.043 \n",
            "(epoch: 13, iters: 500, time: 0.574) G_GAN: 1.503 G_GAN_Feat: 3.275 G_VGG: 2.315 D_real: 0.025 D_fake: 0.061 \n",
            "(epoch: 13, iters: 600, time: 0.576) G_GAN: 1.478 G_GAN_Feat: 3.412 G_VGG: 2.673 D_real: 0.073 D_fake: 0.101 \n",
            "saving the latest model (epoch 13, total_steps 51000)\n",
            "(epoch: 13, iters: 700, time: 0.576) G_GAN: 2.372 G_GAN_Feat: 3.919 G_VGG: 3.339 D_real: 0.100 D_fake: 0.028 \n",
            "(epoch: 13, iters: 800, time: 0.573) G_GAN: 2.106 G_GAN_Feat: 3.771 G_VGG: 3.629 D_real: 0.264 D_fake: 0.020 \n",
            "(epoch: 13, iters: 900, time: 0.576) G_GAN: 1.239 G_GAN_Feat: 2.635 G_VGG: 2.230 D_real: 0.051 D_fake: 0.367 \n",
            "(epoch: 13, iters: 1000, time: 0.577) G_GAN: 2.064 G_GAN_Feat: 2.834 G_VGG: 2.605 D_real: 0.133 D_fake: 0.083 \n",
            "(epoch: 13, iters: 1100, time: 0.575) G_GAN: 1.186 G_GAN_Feat: 2.749 G_VGG: 2.792 D_real: 0.446 D_fake: 0.127 \n",
            "(epoch: 13, iters: 1200, time: 0.574) G_GAN: 2.018 G_GAN_Feat: 3.353 G_VGG: 3.485 D_real: 0.187 D_fake: 0.015 \n",
            "(epoch: 13, iters: 1300, time: 0.575) G_GAN: 2.010 G_GAN_Feat: 3.410 G_VGG: 2.975 D_real: 0.055 D_fake: 0.020 \n",
            "(epoch: 13, iters: 1400, time: 0.576) G_GAN: 2.199 G_GAN_Feat: 3.619 G_VGG: 3.100 D_real: 0.160 D_fake: 0.024 \n",
            "(epoch: 13, iters: 1500, time: 0.576) G_GAN: 1.654 G_GAN_Feat: 3.782 G_VGG: 3.564 D_real: 0.015 D_fake: 0.068 \n",
            "(epoch: 13, iters: 1600, time: 0.576) G_GAN: 1.074 G_GAN_Feat: 2.235 G_VGG: 1.688 D_real: 0.057 D_fake: 0.381 \n",
            "saving the latest model (epoch 13, total_steps 52000)\n",
            "(epoch: 13, iters: 1700, time: 0.577) G_GAN: 1.732 G_GAN_Feat: 3.094 G_VGG: 2.469 D_real: 0.017 D_fake: 0.064 \n",
            "(epoch: 13, iters: 1800, time: 0.576) G_GAN: 1.870 G_GAN_Feat: 3.336 G_VGG: 2.890 D_real: 0.009 D_fake: 0.057 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training on high resolution G1 and G2\n",
        "#!python train.py --netG local --load_pretrain checkpoints/nebbia/ --label_nc 0 --no_instance --name nebbia --dataroot ./datasets/nebbia --resize_or_crop crop --fineSize 1024"
      ],
      "metadata": {
        "id": "3XhQJ5s95ZUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2Pix test"
      ],
      "metadata": {
        "id": "qEuQqQPxGISq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m4nxHHLAtxf",
        "outputId": "d8bb40ca-fe7a-4438-f504-f730e3213a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter.txt  latest_net_D.pth  latest_net_G.pth  loss_log.txt  opt.txt  web\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/test.py\", line 12, in <module>\n",
            "    opt = TestOptions().parse(save=False)\n",
            "  File \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/options/base_options.py\", line 80, in parse\n",
            "    torch.cuda.set_device(self.opt.gpu_ids[0])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 350, in set_device\n",
            "    torch._C._cuda_setDevice(device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        }
      ],
      "source": [
        "#se non  stato eseguito il download\n",
        "if os.getcwd() != \"/content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD\":\n",
        "  os.chdir(\"pix2pixHD\")\n",
        "\n",
        "#!./scripts/test_1024p.sh\n",
        "!python test.py --name nebbia --netG local --ngf 32 --resize_or_crop none --ntest 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/Haze-Fog-suppression/dataset/SOTS/outdoor/gt/* /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/nebbia/test_img/"
      ],
      "metadata": {
        "id": "o-Uvp5SC9u4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/Haze-Fog-suppression/dataset/SOTS/outdoor/hazy/* /content/drive/MyDrive/Haze-Fog-suppression/pix2pixHD/datasets/nebbia/test_label/"
      ],
      "metadata": {
        "id": "ylzeExck-S7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}